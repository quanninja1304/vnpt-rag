{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07104997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b88cc2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'crawl_output/delta_chunks_to_index.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m huhu = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcrawl_output/delta_chunks_to_index.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LapTop\\anaconda3\\envs\\vnpt\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LapTop\\anaconda3\\envs\\vnpt\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LapTop\\anaconda3\\envs\\vnpt\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LapTop\\anaconda3\\envs\\vnpt\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'crawl_output/delta_chunks_to_index.parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "huhu = pd.read_parquet(path=\"crawl_output/delta_chunks_to_index.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722c1b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114797, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huhu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a62ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hihi = pd.read_parquet(path=\"output/1_manual_law_strict.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088381aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2084, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hihi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efb13d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>doc_category</th>\n",
       "      <th>doc_url</th>\n",
       "      <th>vector_text</th>\n",
       "      <th>display_text</th>\n",
       "      <th>crawled_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>law_luat_hon_nhan_va_gia_dinh.txt_0</td>\n",
       "      <td>Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014</td>\n",
       "      <td>Ph√°p lu·∫≠t</td>\n",
       "      <td>local/luat_hon_nhan_va_gia_dinh.txt</td>\n",
       "      <td>VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...</td>\n",
       "      <td>VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...</td>\n",
       "      <td>2025-12-11T10:46:46.879494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>law_luat_hon_nhan_va_gia_dinh.txt_1</td>\n",
       "      <td>Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014</td>\n",
       "      <td>Ph√°p lu·∫≠t</td>\n",
       "      <td>local/luat_hon_nhan_va_gia_dinh.txt</td>\n",
       "      <td>VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...</td>\n",
       "      <td>VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...</td>\n",
       "      <td>2025-12-11T10:46:46.879494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>law_luat_hon_nhan_va_gia_dinh.txt_2</td>\n",
       "      <td>Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014</td>\n",
       "      <td>Ph√°p lu·∫≠t</td>\n",
       "      <td>local/luat_hon_nhan_va_gia_dinh.txt</td>\n",
       "      <td>VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...</td>\n",
       "      <td>VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...</td>\n",
       "      <td>2025-12-11T10:46:46.879494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>law_luat_hon_nhan_va_gia_dinh.txt_3</td>\n",
       "      <td>Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014</td>\n",
       "      <td>Ph√°p lu·∫≠t</td>\n",
       "      <td>local/luat_hon_nhan_va_gia_dinh.txt</td>\n",
       "      <td>VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...</td>\n",
       "      <td>VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...</td>\n",
       "      <td>2025-12-11T10:46:46.879494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>law_luat_hon_nhan_va_gia_dinh.txt_4</td>\n",
       "      <td>Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014</td>\n",
       "      <td>Ph√°p lu·∫≠t</td>\n",
       "      <td>local/luat_hon_nhan_va_gia_dinh.txt</td>\n",
       "      <td>VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...</td>\n",
       "      <td>VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...</td>\n",
       "      <td>2025-12-11T10:46:46.879494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              chunk_id                       doc_title  \\\n",
       "0  law_luat_hon_nhan_va_gia_dinh.txt_0  Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014   \n",
       "1  law_luat_hon_nhan_va_gia_dinh.txt_1  Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014   \n",
       "2  law_luat_hon_nhan_va_gia_dinh.txt_2  Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014   \n",
       "3  law_luat_hon_nhan_va_gia_dinh.txt_3  Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014   \n",
       "4  law_luat_hon_nhan_va_gia_dinh.txt_4  Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014   \n",
       "\n",
       "  doc_category                              doc_url  \\\n",
       "0    Ph√°p lu·∫≠t  local/luat_hon_nhan_va_gia_dinh.txt   \n",
       "1    Ph√°p lu·∫≠t  local/luat_hon_nhan_va_gia_dinh.txt   \n",
       "2    Ph√°p lu·∫≠t  local/luat_hon_nhan_va_gia_dinh.txt   \n",
       "3    Ph√°p lu·∫≠t  local/luat_hon_nhan_va_gia_dinh.txt   \n",
       "4    Ph√°p lu·∫≠t  local/luat_hon_nhan_va_gia_dinh.txt   \n",
       "\n",
       "                                         vector_text  \\\n",
       "0  VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...   \n",
       "1  VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...   \n",
       "2  VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...   \n",
       "3  VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...   \n",
       "4  VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...   \n",
       "\n",
       "                                        display_text  \\\n",
       "0  VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...   \n",
       "1  VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...   \n",
       "2  VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...   \n",
       "3  VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...   \n",
       "4  VƒÉn b·∫£n: Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014\\nCh∆∞∆°n...   \n",
       "\n",
       "                   crawled_at  \n",
       "0  2025-12-11T10:46:46.879494  \n",
       "1  2025-12-11T10:46:46.879494  \n",
       "2  2025-12-11T10:46:46.879494  \n",
       "3  2025-12-11T10:46:46.879494  \n",
       "4  2025-12-11T10:46:46.879494  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hihi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1bb87d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>doc_category</th>\n",
       "      <th>vector_text</th>\n",
       "      <th>display_text</th>\n",
       "      <th>doc_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam_0</td>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam</td>\n",
       "      <td>Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam</td>\n",
       "      <td>Lƒ©nh v·ª±c: Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam. Ch·ªß ƒë·ªÅ: A...</td>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam l√† thu·∫≠t ng·ªØ ch·ªâ nh·ªØ...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam_1</td>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam</td>\n",
       "      <td>Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam</td>\n",
       "      <td>Lƒ©nh v·ª±c: Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam. Ch·ªß ƒë·ªÅ: A...</td>\n",
       "      <td>ƒêinh Ti√™n Ho√†ng, t·ª©c ƒêinh B·ªô Lƒ©nh: ng∆∞·ªùi ƒë√°nh ...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam_2</td>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam</td>\n",
       "      <td>Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam</td>\n",
       "      <td>Lƒ©nh v·ª±c: Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam. Ch·ªß ƒë·ªÅ: A...</td>\n",
       "      <td>Ti√™u chu·∫©n\\n\\n14 v·ªã Anh h√πng d√¢n t·ªôc Vi·ªát Nam ...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An D∆∞∆°ng V∆∞∆°ng_0</td>\n",
       "      <td>An D∆∞∆°ng V∆∞∆°ng</td>\n",
       "      <td>Tri·ªÅu ƒë·∫°i Vi·ªát Nam</td>\n",
       "      <td>Lƒ©nh v·ª±c: Tri·ªÅu ƒë·∫°i Vi·ªát Nam. Ch·ªß ƒë·ªÅ: An D∆∞∆°ng...</td>\n",
       "      <td>An D∆∞∆°ng V∆∞∆°ng (ch·ªØ H√°n: ÂÆâÈôΩÁéã), t√™n th·∫≠t l√† Th·ª•...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/An_D%C6%B0%C6%A1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nh√† n∆∞·ªõc Vi·ªát Nam_0</td>\n",
       "      <td>Nh√† n∆∞·ªõc Vi·ªát Nam</td>\n",
       "      <td>Nh√† n∆∞·ªõc Vi·ªát Nam</td>\n",
       "      <td>Lƒ©nh v·ª±c: Nh√† n∆∞·ªõc Vi·ªát Nam. Ch·ªß ƒë·ªÅ: Nh√† n∆∞·ªõc ...</td>\n",
       "      <td>Nh√† n∆∞·ªõc C·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam l√†...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Nh%C3%A0_n%C6%B0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      chunk_id                  doc_title  \\\n",
       "0  Anh h√πng d√¢n t·ªôc Vi·ªát Nam_0  Anh h√πng d√¢n t·ªôc Vi·ªát Nam   \n",
       "1  Anh h√πng d√¢n t·ªôc Vi·ªát Nam_1  Anh h√πng d√¢n t·ªôc Vi·ªát Nam   \n",
       "2  Anh h√πng d√¢n t·ªôc Vi·ªát Nam_2  Anh h√πng d√¢n t·ªôc Vi·ªát Nam   \n",
       "3             An D∆∞∆°ng V∆∞∆°ng_0             An D∆∞∆°ng V∆∞∆°ng   \n",
       "4          Nh√† n∆∞·ªõc Vi·ªát Nam_0          Nh√† n∆∞·ªõc Vi·ªát Nam   \n",
       "\n",
       "                doc_category  \\\n",
       "0  Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam   \n",
       "1  Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam   \n",
       "2  Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam   \n",
       "3         Tri·ªÅu ƒë·∫°i Vi·ªát Nam   \n",
       "4          Nh√† n∆∞·ªõc Vi·ªát Nam   \n",
       "\n",
       "                                         vector_text  \\\n",
       "0  Lƒ©nh v·ª±c: Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam. Ch·ªß ƒë·ªÅ: A...   \n",
       "1  Lƒ©nh v·ª±c: Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam. Ch·ªß ƒë·ªÅ: A...   \n",
       "2  Lƒ©nh v·ª±c: Nh√¢n v·∫≠t l·ªãch s·ª≠ Vi·ªát Nam. Ch·ªß ƒë·ªÅ: A...   \n",
       "3  Lƒ©nh v·ª±c: Tri·ªÅu ƒë·∫°i Vi·ªát Nam. Ch·ªß ƒë·ªÅ: An D∆∞∆°ng...   \n",
       "4  Lƒ©nh v·ª±c: Nh√† n∆∞·ªõc Vi·ªát Nam. Ch·ªß ƒë·ªÅ: Nh√† n∆∞·ªõc ...   \n",
       "\n",
       "                                        display_text  \\\n",
       "0  Anh h√πng d√¢n t·ªôc Vi·ªát Nam l√† thu·∫≠t ng·ªØ ch·ªâ nh·ªØ...   \n",
       "1  ƒêinh Ti√™n Ho√†ng, t·ª©c ƒêinh B·ªô Lƒ©nh: ng∆∞·ªùi ƒë√°nh ...   \n",
       "2  Ti√™u chu·∫©n\\n\\n14 v·ªã Anh h√πng d√¢n t·ªôc Vi·ªát Nam ...   \n",
       "3  An D∆∞∆°ng V∆∞∆°ng (ch·ªØ H√°n: ÂÆâÈôΩÁéã), t√™n th·∫≠t l√† Th·ª•...   \n",
       "4  Nh√† n∆∞·ªõc C·ªông h√≤a x√£ h·ªôi ch·ªß nghƒ©a Vi·ªát Nam l√†...   \n",
       "\n",
       "                                             doc_url  \n",
       "0  https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...  \n",
       "1  https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...  \n",
       "2  https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...  \n",
       "3  https://vi.wikipedia.org/wiki/An_D%C6%B0%C6%A1...  \n",
       "4  https://vi.wikipedia.org/wiki/Nh%C3%A0_n%C6%B0...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huhu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e56067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üï∑Ô∏è STARTING TARGETED CRAWL (V2 - SSL BYPASS)...\n",
      "\n",
      "üöÄ Campaign: Tu_Lieu_Dang\n",
      "  üîç Inspecting: https://tulieuvankien.dangcongsan.vn/ho-chi-minh-toan-tap\n",
      "    ‚úÖ Connected (Size: 6021 bytes)\n",
      "    ‚ÑπÔ∏è T√¨m th·∫•y 1 th·∫ª <a> trong trang.\n",
      "      üëÄ Sample link: https://tulieuvankien.dangcongsan.vn/\n",
      "    => L·ªçc ƒë∆∞·ª£c 0 link ph√π h·ª£p pattern.\n",
      "  üëâ Th·ª≠ trang 2: https://tulieuvankien.dangcongsan.vn/ho-chi-minh-toan-tap?page=2\n",
      "  üîç Inspecting: https://tulieuvankien.dangcongsan.vn/ho-chi-minh-toan-tap?page=2\n",
      "    ‚úÖ Connected (Size: 6021 bytes)\n",
      "    ‚ÑπÔ∏è T√¨m th·∫•y 1 th·∫ª <a> trong trang.\n",
      "      üëÄ Sample link: https://tulieuvankien.dangcongsan.vn/\n",
      "    => L·ªçc ƒë∆∞·ª£c 0 link ph√π h·ª£p pattern.\n",
      "  üîç Inspecting: https://tulieuvankien.dangcongsan.vn/van-kien-tu-lieu-ve-dang\n",
      "    ‚úÖ Connected (Size: 6021 bytes)\n",
      "    ‚ÑπÔ∏è T√¨m th·∫•y 1 th·∫ª <a> trong trang.\n",
      "      üëÄ Sample link: https://tulieuvankien.dangcongsan.vn/\n",
      "    => L·ªçc ƒë∆∞·ª£c 0 link ph√π h·ª£p pattern.\n",
      "  üëâ Th·ª≠ trang 2: https://tulieuvankien.dangcongsan.vn/van-kien-tu-lieu-ve-dang?page=2\n",
      "  üîç Inspecting: https://tulieuvankien.dangcongsan.vn/van-kien-tu-lieu-ve-dang?page=2\n",
      "    ‚úÖ Connected (Size: 6021 bytes)\n",
      "    ‚ÑπÔ∏è T√¨m th·∫•y 1 th·∫ª <a> trong trang.\n",
      "      üëÄ Sample link: https://tulieuvankien.dangcongsan.vn/\n",
      "    => L·ªçc ƒë∆∞·ª£c 0 link ph√π h·ª£p pattern.\n",
      "  => T·ªïng link thu ƒë∆∞·ª£c cho Tu_Lieu_Dang: 0\n",
      "\n",
      "üöÄ Campaign: Di_San_Van_Hoa\n",
      "  üîç Inspecting: http://dsvh.gov.vn/di-tich-quoc-gia-dac-biet-1756\n",
      "    ‚úÖ Connected (Size: 66596 bytes)\n",
      "    ‚ÑπÔ∏è T√¨m th·∫•y 160 th·∫ª <a> trong trang.\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/gioi-thieu-15\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/thong-tin-chung-1727\n",
      "    => L·ªçc ƒë∆∞·ª£c 63 link ph√π h·ª£p pattern.\n",
      "  üëâ Th·ª≠ trang 2: http://dsvh.gov.vn/di-tich-quoc-gia-dac-biet-1756?page=2\n",
      "  üîç Inspecting: http://dsvh.gov.vn/di-tich-quoc-gia-dac-biet-1756?page=2\n",
      "    ‚úÖ Connected (Size: 66596 bytes)\n",
      "    ‚ÑπÔ∏è T√¨m th·∫•y 160 th·∫ª <a> trong trang.\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/gioi-thieu-15\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/thong-tin-chung-1727\n",
      "    => L·ªçc ƒë∆∞·ª£c 56 link ph√π h·ª£p pattern.\n",
      "  üîç Inspecting: http://dsvh.gov.vn/di-tich-quoc-gia-1757\n",
      "    ‚úÖ Connected (Size: 66596 bytes)\n",
      "    ‚ÑπÔ∏è T√¨m th·∫•y 160 th·∫ª <a> trong trang.\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/gioi-thieu-15\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/thong-tin-chung-1727\n",
      "    => L·ªçc ƒë∆∞·ª£c 85 link ph√π h·ª£p pattern.\n",
      "  üëâ Th·ª≠ trang 2: http://dsvh.gov.vn/di-tich-quoc-gia-1757?page=2\n",
      "  üîç Inspecting: http://dsvh.gov.vn/di-tich-quoc-gia-1757?page=2\n",
      "    ‚úÖ Connected (Size: 66596 bytes)\n",
      "    ‚ÑπÔ∏è T√¨m th·∫•y 160 th·∫ª <a> trong trang.\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/gioi-thieu-15\n",
      "      üëÄ Sample link: http://dsvh.gov.vn/thong-tin-chung-1727\n",
      "    => L·ªçc ƒë∆∞·ª£c 63 link ph√π h·ª£p pattern.\n",
      "  => T·ªïng link thu ƒë∆∞·ª£c cho Di_San_Van_Hoa: 65\n",
      "  Downloading content (65 b√†i)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [12:08<00:00, 11.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ DONE! ƒê√£ l∆∞u 1 b√†i vi·∫øt ch·∫•t l∆∞·ª£ng v√†o targeted_knowledge.parquet\n",
      "\n",
      "üëÄ V√≠ d·ª• d·ªØ li·ªáu:\n",
      "  doc_title                                            doc_url\n",
      "0            http://dsvh.gov.vn/mo-cu-pho-bang-nguyen-sinh-...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import urllib3\n",
    "\n",
    "# [QUAN TR·ªåNG] T·∫Øt c·∫£nh b√°o b·∫£o m·∫≠t (SSL Warning) ƒë·ªÉ crawl web ch√≠nh ph·ªß c≈©\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "OUTPUT_FILE = \"targeted_knowledge.parquet\"\n",
    "MAX_PAGES = 5\n",
    "\n",
    "CAMPAIGNS = [\n",
    "    {\n",
    "        \"name\": \"Tu_Lieu_Dang\",\n",
    "        \"base_url\": \"https://tulieuvankien.dangcongsan.vn\",\n",
    "        \"seed_urls\": [\n",
    "            \"https://tulieuvankien.dangcongsan.vn/ho-chi-minh-toan-tap\",\n",
    "            \"https://tulieuvankien.dangcongsan.vn/van-kien-tu-lieu-ve-dang\"\n",
    "        ],\n",
    "        # Regex linh ho·∫°t h∆°n: Ch·∫•p nh·∫≠n m·ªçi link con ch·ª©a t·ª´ kh√≥a\n",
    "        \"link_pattern\": r\"(ho-chi-minh-toan-tap|van-kien-tu-lieu-ve-dang)/\" \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Di_San_Van_Hoa\",\n",
    "        \"base_url\": \"http://dsvh.gov.vn\", # ƒê·ªïi sang trang C·ª•c Di s·∫£n vƒÉn h√≥a (chuy√™n s√¢u h∆°n)\n",
    "        \"seed_urls\": [\n",
    "            \"http://dsvh.gov.vn/di-tich-quoc-gia-dac-biet-1756\",\n",
    "            \"http://dsvh.gov.vn/di-tich-quoc-gia-1757\"\n",
    "        ],\n",
    "        \"link_pattern\": r\"dsvh.gov.vn/.*\" \n",
    "    }\n",
    "]\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"G·ª≠i request v·ªõi ch·∫ø ƒë·ªô 'gi·∫£ danh' tr√¨nh duy·ªát th·∫≠t\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "            'Referer': 'https://google.com'\n",
    "        }\n",
    "        # [QUAN TR·ªåNG] verify=False ƒë·ªÉ b·ªè qua l·ªói SSL\n",
    "        resp = requests.get(url, headers=headers, timeout=20, verify=False)\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            # In ra ƒë·ªô d√†i n·ªôi dung ƒë·ªÉ debug\n",
    "            print(f\"    ‚úÖ Connected (Size: {len(resp.content)} bytes)\")\n",
    "            return BeautifulSoup(resp.content, 'html.parser')\n",
    "        else:\n",
    "            print(f\"    ‚ùå Status Code: {resp.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ö†Ô∏è Connect Error: {url} - {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_links_from_list(campaign, url):\n",
    "    links = set()\n",
    "    print(f\"  üîç Inspecting: {url}\")\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    if not soup: \n",
    "        print(\"    ‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c HTML.\")\n",
    "        return []\n",
    "\n",
    "    found_count = 0\n",
    "    match_count = 0\n",
    "    \n",
    "    # T√¨m t·∫•t c·∫£ th·∫ª a\n",
    "    all_tags = soup.find_all('a', href=True)\n",
    "    print(f\"    ‚ÑπÔ∏è T√¨m th·∫•y {len(all_tags)} th·∫ª <a> trong trang.\")\n",
    "\n",
    "    for a in all_tags:\n",
    "        href = a['href']\n",
    "        full_url = urljoin(campaign['base_url'], href)\n",
    "        \n",
    "        # [DEBUG] In th·ª≠ 3 link ƒë·∫ßu ti√™n ƒë·ªÉ xem format\n",
    "        if found_count < 3:\n",
    "            print(f\"      üëÄ Sample link: {full_url}\")\n",
    "        found_count += 1\n",
    "\n",
    "        # L·ªçc link\n",
    "        if re.search(campaign['link_pattern'], full_url):\n",
    "            if any(x in full_url for x in ['print', 'javascript', '#', 'download', 'jpg', 'png']):\n",
    "                continue\n",
    "            \n",
    "            # Ch·ªâ l·∫•y link chi ti·∫øt (th∆∞·ªùng d√†i h∆°n link danh m·ª•c)\n",
    "            if len(full_url) > len(url) + 5: \n",
    "                links.add(full_url)\n",
    "                match_count += 1\n",
    "    \n",
    "    print(f\"    => L·ªçc ƒë∆∞·ª£c {match_count} link ph√π h·ª£p pattern.\")\n",
    "    return list(links)\n",
    "\n",
    "def crawl_article(url, category):\n",
    "    try:\n",
    "        # Trafilatura c≈©ng c·∫ßn config ƒë·ªÉ b·ªè qua SSL\n",
    "        config = trafilatura.settings.use_config()\n",
    "        config.set(\"DEFAULT\", \"USER_AGENT\", \"Mozilla/5.0\")\n",
    "        \n",
    "        # Download th·ªß c√¥ng b·∫±ng requests ƒë·ªÉ bypass SSL, sau ƒë√≥ ƒë∆∞a v√†o trafilatura\n",
    "        resp = requests.get(url, verify=False, timeout=10)\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            data = trafilatura.extract(resp.text, output_format=\"json\", include_comments=False)\n",
    "            if data:\n",
    "                import json\n",
    "                j = json.loads(data)\n",
    "                title = j.get('title', '')\n",
    "                text = j.get('text', '')\n",
    "                \n",
    "                # Validation: B·ªè qua b√†i qu√° ng·∫Øn (l·ªói parse)\n",
    "                if len(text) < 100: return None\n",
    "                \n",
    "                return {\n",
    "                    \"doc_title\": title,\n",
    "                    \"doc_url\": url,\n",
    "                    \"doc_category\": category,\n",
    "                    \"original_text\": text,\n",
    "                    \"vector_text\": f\"Ngu·ªìn: {category}\\nTi√™u ƒë·ªÅ: {title}\\nN·ªôi dung:\\n{text}\",\n",
    "                    \"display_text\": f\"Ngu·ªìn: {category}\\nTi√™u ƒë·ªÅ: {title}\\nN·ªôi dung:\\n{text}\",\n",
    "                    \"chunk_id\": f\"crawl_{int(time.time())}_{random.randint(1000,9999)}\"\n",
    "                }\n",
    "    except: pass\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    print(\"üï∑Ô∏è STARTING TARGETED CRAWL (V2 - SSL BYPASS)...\")\n",
    "    all_data = []\n",
    "    \n",
    "    for camp in CAMPAIGNS:\n",
    "        print(f\"\\nüöÄ Campaign: {camp['name']}\")\n",
    "        campaign_links = set()\n",
    "        \n",
    "        # 1. Qu√©t danh s√°ch\n",
    "        for seed in camp['seed_urls']:\n",
    "            links = extract_links_from_list(camp, seed)\n",
    "            campaign_links.update(links)\n",
    "            \n",
    "            # Pagination c∆° b·∫£n (Trang 2)\n",
    "            # Th∆∞·ªùng web VN d√πng ?page=2 ho·∫∑c /p2\n",
    "            next_page = f\"{seed}?page=2\"\n",
    "            print(f\"  üëâ Th·ª≠ trang 2: {next_page}\")\n",
    "            links_p2 = extract_links_from_list(camp, next_page)\n",
    "            campaign_links.update(links_p2)\n",
    "\n",
    "        unique_links = list(campaign_links)\n",
    "        print(f\"  => T·ªïng link thu ƒë∆∞·ª£c cho {camp['name']}: {len(unique_links)}\")\n",
    "        \n",
    "        # 2. Crawl chi ti·∫øt\n",
    "        if unique_links:\n",
    "            print(f\"  Downloading content ({len(unique_links)} b√†i)...\")\n",
    "            for url in tqdm(unique_links):\n",
    "                row = crawl_article(url, camp['name'])\n",
    "                if row:\n",
    "                    all_data.append(row)\n",
    "                time.sleep(random.uniform(0.5, 1.0))\n",
    "\n",
    "    # 3. L∆∞u\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        # Lo·∫°i b·ªè tr√πng l·∫∑p n·ªôi dung\n",
    "        df.drop_duplicates(subset=['doc_title'], inplace=True)\n",
    "        \n",
    "        df.to_parquet(OUTPUT_FILE, index=False)\n",
    "        print(f\"\\n‚úÖ DONE! ƒê√£ l∆∞u {len(df)} b√†i vi·∫øt ch·∫•t l∆∞·ª£ng v√†o {OUTPUT_FILE}\")\n",
    "        \n",
    "        # Preview\n",
    "        print(\"\\nüëÄ V√≠ d·ª• d·ªØ li·ªáu:\")\n",
    "        print(df[['doc_title', 'doc_url']].head())\n",
    "    else:\n",
    "        print(\"\\n‚ùå V·∫´n kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu n√†o. H√£y copy log tr√™n ƒë·ªÉ debug.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "772fad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫ÆT ƒê·∫¶U CRAWL (FIXED VERSION)...\n",
      "\n",
      "üìÇ ƒêang x·ª≠ l√Ω ngu·ªìn: Nguoi_Ke_Su\n",
      "   -> ƒêang qu√©t 150 trang danh m·ª•c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [03:41<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   => T√¨m th·∫•y 453 link b√†i vi·∫øt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 453/453 [11:33<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ùå Kh√¥ng t·∫£i ƒë∆∞·ª£c n·ªôi dung b√†i n√†o.\n",
      "\n",
      "üìÇ ƒêang x·ª≠ l√Ω ngu·ªìn: Tuyen_Giao\n",
      "   -> ƒêang qu√©t 150 trang danh m·ª•c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:00<00:00, 204.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   => T√¨m th·∫•y 0 link b√†i vi·∫øt.\n",
      "   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y link n√†o. B·ªè qua ngu·ªìn n√†y.\n",
      "\n",
      "üìÇ ƒêang x·ª≠ l√Ω ngu·ªìn: Van_Ban_Chinh_Phu\n",
      "   -> ƒêang qu√©t 50 trang danh m·ª•c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:52<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   => T√¨m th·∫•y 0 link b√†i vi·∫øt.\n",
      "   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y link n√†o. B·ªè qua ngu·ªìn n√†y.\n",
      "\n",
      "üéâ T·ªîNG K·∫æT: ƒê√£ thu th·∫≠p 0 b√†i vi·∫øt m·ªõi.\n",
      "üëâ H√£y th√™m c√°c file 'crawl_*.parquet' v√†o build_bm25_incremental.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import urllib3\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "# T·∫Øt c·∫£nh b√°o SSL\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "MAX_PAGES = 50  # S·ªë trang m·ªói m·ª•c\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a l·∫°i Campaign v·ªõi ngu·ªìn d·ªÖ th·ªü h∆°n\n",
    "CAMPAIGNS = [\n",
    "    # 1. L·ªäCH S·ª¨ (ƒê√£ ngon, gi·ªØ nguy√™n)\n",
    "    {\n",
    "        \"name\": \"Nguoi_Ke_Su\",\n",
    "        \"base_url\": \"https://nguoikesu.com\",\n",
    "        \"seed_urls\": [\n",
    "            \"https://nguoikesu.com/nhan-vat\",\n",
    "            \"https://nguoikesu.com/dong-lich-su\",\n",
    "            \"https://nguoikesu.com/di-tich-lich-su\"\n",
    "        ],\n",
    "        \"link_pattern\": r\"nguoikesu.com/(nhan-vat|dong-lich-su|di-tich-lich-su)/\",\n",
    "        \"page_param\": \"?start={}\", # start=0, 5, 10\n",
    "        \"step\": 5\n",
    "    },\n",
    "    \n",
    "    # 2. CH√çNH TR·ªä (C·∫≠p nh·∫≠t Seed URL)\n",
    "    {\n",
    "        \"name\": \"Tuyen_Giao\",\n",
    "        \"base_url\": \"https://tuyengiao.vn\",\n",
    "        \"seed_urls\": [\n",
    "            \"https://tuyengiao.vn/hoc-tap-va-lam-theo-loi-bac\",\n",
    "            \"https://tuyengiao.vn/bao-ve-nen-tang-tu-tuong-cua-dang\",\n",
    "            \"https://tuyengiao.vn/van-hoa-xa-hoi\"\n",
    "        ],\n",
    "        # L·∫•y t·∫•t c·∫£ link b√†i vi·∫øt (th∆∞·ªùng c√≥ .html ho·∫∑c kh√¥ng c√≥ ext nh∆∞ng n·∫±m trong subfolder)\n",
    "        \"link_pattern\": r\"tuyengiao.vn/.*\", \n",
    "        \"page_param\": \"?page={}\", \n",
    "        \"step\": 1\n",
    "    },\n",
    "\n",
    "    # 3. LU·∫¨T (Thay th·∫ø VBPL b·∫±ng ChinhPhu.vn - D·ªÖ crawl h∆°n)\n",
    "    {\n",
    "        \"name\": \"Van_Ban_Chinh_Phu\",\n",
    "        \"base_url\": \"https://vanban.chinhphu.vn\",\n",
    "        \"seed_urls\": [\n",
    "            # Trang t√¨m ki·∫øm vƒÉn b·∫£n m·ªõi\n",
    "            \"https://vanban.chinhphu.vn/?pageid=27160\", \n",
    "        ],\n",
    "        # Link chi ti·∫øt th∆∞·ªùng ch·ª©a 'van-ban'\n",
    "        \"link_pattern\": r\"vanban.chinhphu.vn/.*van-ban\", \n",
    "        \"page_param\": \"&page={}\", \n",
    "        \"step\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        }\n",
    "        # verify=False ƒë·ªÉ bypass l·ªói SSL c·ªßa web ch√≠nh ph·ªß\n",
    "        resp = requests.get(url, headers=headers, timeout=20, verify=False)\n",
    "        if resp.status_code == 200:\n",
    "            return BeautifulSoup(resp.content, 'html.parser')\n",
    "    except Exception as e:\n",
    "        # print(f\"L·ªói connect {url}: {e}\")\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def generate_urls(camp):\n",
    "    urls = []\n",
    "    for seed in camp['seed_urls']:\n",
    "        urls.append(seed)\n",
    "        for i in range(2, MAX_PAGES + 1):\n",
    "            # T√≠nh to√°n tham s·ªë ph√¢n trang\n",
    "            val = i if camp.get('step', 1) == 1 else (i-1) * camp['step']\n",
    "            \n",
    "            if \"{}\" in camp['page_param']:\n",
    "                param = camp['page_param'].format(val)\n",
    "            else:\n",
    "                param = camp['page_param'] + str(val)\n",
    "                \n",
    "            if \"?\" in seed:\n",
    "                full = f\"{seed}&{param.replace('?', '')}\"\n",
    "            else:\n",
    "                full = f\"{seed}{param}\"\n",
    "            urls.append(full)\n",
    "    return urls\n",
    "\n",
    "def extract_links(camp, url):\n",
    "    soup = get_soup(url)\n",
    "    if not soup: return []\n",
    "    \n",
    "    links = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        full = urljoin(camp['base_url'], href)\n",
    "        \n",
    "        # L·ªçc r√°c\n",
    "        if any(x in full for x in ['#', 'jpg', 'pdf', 'doc', 'download', 'print', 'javascript', 'mailto']):\n",
    "            continue\n",
    "            \n",
    "        if re.search(camp['link_pattern'], full):\n",
    "            # V·ªõi chinhphu.vn, link ph·∫£i ch·ª©a 'handle' ho·∫∑c 'chi-tiet'\n",
    "            if camp['name'] == \"Van_Ban_Chinh_Phu\":\n",
    "                if \"chi-tiet\" in full or \"handle\" in full:\n",
    "                    links.add(full)\n",
    "            else:\n",
    "                # C√°c trang kh√°c: ƒë·ªô d√†i URL ph·∫£i ƒë·ªß l·ªõn (tr√°nh link trang ch·ªß)\n",
    "                if len(full) > len(camp['base_url']) + 10:\n",
    "                    links.add(full)\n",
    "    return list(links)\n",
    "\n",
    "def crawl_content(url, category):\n",
    "    try:\n",
    "        # D√πng requests t·∫£i tr∆∞·ªõc ƒë·ªÉ bypass SSL\n",
    "        resp = requests.get(url, verify=False, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        if resp.status_code != 200: return None\n",
    "        \n",
    "        # Parse\n",
    "        data = trafilatura.extract(resp.text, output_format=\"json\", include_comments=False)\n",
    "        if data:\n",
    "            import json\n",
    "            j = json.loads(data)\n",
    "            title = j.get('title', '').strip()\n",
    "            text = j.get('text', '').strip()\n",
    "            \n",
    "            if len(text) < 200: return None\n",
    "            \n",
    "            doc_hash = hashlib.md5(url.encode()).hexdigest()[:10]\n",
    "            \n",
    "            return {\n",
    "                \"chunk_id\": f\"{category}_{doc_hash}\",\n",
    "                \"doc_title\": title,\n",
    "                \"doc_url\": url,\n",
    "                \"doc_category\": category,\n",
    "                \"vector_text\": f\"Ngu·ªìn: {category}\\nTi√™u ƒë·ªÅ: {title}\\nN·ªôi dung:\\n{text}\",\n",
    "                \"display_text\": f\"Title: {title}\\nSource: {url}\\n\\n{text}\"\n",
    "            }\n",
    "    except: pass\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ B·∫ÆT ƒê·∫¶U CRAWL (FIXED VERSION)...\")\n",
    "    \n",
    "    total_collected = 0\n",
    "    \n",
    "    for camp in CAMPAIGNS:\n",
    "        print(f\"\\nüìÇ ƒêang x·ª≠ l√Ω ngu·ªìn: {camp['name']}\")\n",
    "        list_urls = generate_urls(camp)\n",
    "        \n",
    "        # 1. Qu√©t Link\n",
    "        article_links = set()\n",
    "        print(f\"   -> ƒêang qu√©t {len(list_urls)} trang danh m·ª•c...\")\n",
    "        for l_url in tqdm(list_urls, desc=\"Scanning\"):\n",
    "            found = extract_links(camp, l_url)\n",
    "            article_links.update(found)\n",
    "        \n",
    "        print(f\"   => T√¨m th·∫•y {len(article_links)} link b√†i vi·∫øt.\")\n",
    "        if len(article_links) == 0:\n",
    "            print(\"   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y link n√†o. B·ªè qua ngu·ªìn n√†y.\")\n",
    "            continue\n",
    "\n",
    "        # 2. T·∫£i n·ªôi dung\n",
    "        rows = []\n",
    "        for url in tqdm(list(article_links), desc=\"Downloading\"):\n",
    "            row = crawl_content(url, camp['name'])\n",
    "            if row:\n",
    "                rows.append(row)\n",
    "            # Sleep nh·∫π ƒë·ªÉ kh√¥ng b·ªã ban\n",
    "            time.sleep(0.2) \n",
    "            \n",
    "        # 3. L∆ØU NGAY L·∫¨P T·ª®C (Save Checkpoint)\n",
    "        if rows:\n",
    "            filename = f\"crawl_{camp['name']}.parquet\"\n",
    "            df = pd.DataFrame(rows)\n",
    "            df.drop_duplicates(subset=['doc_url'], inplace=True)\n",
    "            df.to_parquet(filename, index=False)\n",
    "            print(f\"   üíæ ƒê√£ l∆∞u {len(df)} b√†i v√†o '{filename}'\")\n",
    "            total_collected += len(df)\n",
    "        else:\n",
    "            print(\"   ‚ùå Kh√¥ng t·∫£i ƒë∆∞·ª£c n·ªôi dung b√†i n√†o.\")\n",
    "\n",
    "    print(f\"\\nüéâ T·ªîNG K·∫æT: ƒê√£ thu th·∫≠p {total_collected} b√†i vi·∫øt m·ªõi.\")\n",
    "    print(\"üëâ H√£y th√™m c√°c file 'crawl_*.parquet' v√†o build_bm25_incremental.py\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43422074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "i1 = pd.read_parquet(path=\"sgk_lich_su_dia_ly.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c088569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a2a19a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**T√†i li·ªáu L·ªãch s·ª≠ 10 - THI247.com**\\n*M√¥n: L·ªãch S·ª≠ - L·ªõp 10*\\n\\nT√†i li·ªáu L·ªãch s·ª≠ 10\\nTuy·ªÉn t·∫≠p c√°c t√†i li·ªáu L·ªãch s·ª≠ 10 hay nh·∫•t v·ªõi ƒë·∫ßy ƒë·ªß c√°c ch·ªß ƒë·ªÅ b√°m s√°t ch∆∞∆°ng tr√¨nh s√°ch gi√°o khoa L·ªãch s·ª≠ 10 hi·ªán h√†nh. C√°c t√†i li·ªáu L·ªãch s·ª≠ 10 ƒë∆∞·ª£c bi√™n so·∫°n b·ªüi qu√Ω th·∫ßy, c√¥, c√°c t√°c gi·∫£ nhi·ªÅu kinh nghi·ªám s·∫Ω gi√∫p c√°c em h·ªçc sinh kh·ªëi 10 t√¨m hi·ªÉu c√°c ki·∫øn th·ª©c m√¥n L·ªãch s·ª≠ 10.\\nC√°c t√†i li·ªáu L·ªãch s·ª≠ 10 s·∫Ω ƒë∆∞·ª£c THI247.com c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n d·ª±a v√†o ngu·ªìn s∆∞u t·∫ßm tr√™n c√°c trang m·∫°ng x√£ h·ªôi v√† c√°c trang web kh√°c tr√™n internet.\\nQu√Ω th·∫ßy, c√¥ gi√°o v√† c√°c em h·ªçc sinh kh·ªëi 10 c√≥ th·ªÉ xem v√† t·∫£i xu·ªëng mi·ªÖn ph√≠ c√°c t√†i li·ªáu L·ªãch s·ª≠ 10 ƒë∆∞·ª£c chia s·∫ª tr√™n THI247.com.\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 1 L·ªãch s·ª≠ 10 nƒÉm 2024 ‚Äì 2025 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k√¨ 2 L·ªãch s·ª≠ 10 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 2 L·ªãch s·ª≠ 10 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k·ª≥ 1 L·ªãch s·ª≠ 10 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Y√™n H√≤a ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng L·ªãch s·ª≠ 10 gi·ªØa k·ª≥ 1 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nTr·ªçn b·ªô l√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám m√¥n L·ªãch s·ª≠ 10\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám phong tr√†o c√¥ng nh√¢n (t·ª´ ƒë·∫ßu th·∫ø k·ªâ XIX ƒë·∫øn ƒë·∫ßu th·∫ø k·ªâ XX)\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám c√°c n∆∞·ªõc √Çu ‚Äì Mƒ© (t·ª´ ƒë·∫ßu th·∫ø k·ªâ XIX ƒë·∫øn ƒë·∫ßu th·∫ø k·ªâ XX)\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám c√°c cu·ªôc c√°ch m·∫°ng t∆∞ s·∫£n (t·ª´ gi·ªØa th·∫ø k·ªâ XVI ƒë·∫øn cu·ªëi th·∫ø k·ªâ XVIII)\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám qu√° tr√¨nh d·ª±ng n∆∞·ªõc, gi·ªØ n∆∞·ªõc v√† truy·ªÅn th·ªëng y√™u n∆∞·ªõc c·ªßa d√¢n t·ªôc Vi·ªát Nam th·ªùi phong ki·∫øn\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám Vi·ªát Nam ·ªü n·ª≠a ƒë·∫ßu th·∫ø k·ªâ XIX\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám Vi·ªát Nam t·ª´ th·∫ø k·ªâ XVI ƒë·∫øn XVIII',\n",
       " '**T√†i li·ªáu L·ªãch s·ª≠ 11 - THI247.com**\\n*M√¥n: L·ªãch S·ª≠ - L·ªõp 11*\\n\\nT√†i li·ªáu L·ªãch s·ª≠ 11\\nTuy·ªÉn t·∫≠p c√°c t√†i li·ªáu L·ªãch s·ª≠ 11 hay nh·∫•t v·ªõi ƒë·∫ßy ƒë·ªß c√°c ch·ªß ƒë·ªÅ b√°m s√°t ch∆∞∆°ng tr√¨nh s√°ch gi√°o khoa L·ªãch s·ª≠ 11 hi·ªán h√†nh. C√°c t√†i li·ªáu L·ªãch s·ª≠ 11 ƒë∆∞·ª£c bi√™n so·∫°n b·ªüi qu√Ω th·∫ßy, c√¥, c√°c t√°c gi·∫£ nhi·ªÅu kinh nghi·ªám s·∫Ω gi√∫p c√°c em h·ªçc sinh kh·ªëi 11 t√¨m hi·ªÉu c√°c ki·∫øn th·ª©c m√¥n L·ªãch s·ª≠ 11.\\nC√°c t√†i li·ªáu L·ªãch s·ª≠ 11 s·∫Ω ƒë∆∞·ª£c THI247.com c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n d·ª±a v√†o ngu·ªìn s∆∞u t·∫ßm tr√™n c√°c trang m·∫°ng x√£ h·ªôi v√† c√°c trang web kh√°c tr√™n internet.\\nQu√Ω th·∫ßy, c√¥ gi√°o v√† c√°c em h·ªçc sinh kh·ªëi 11 c√≥ th·ªÉ xem v√† t·∫£i xu·ªëng mi·ªÖn ph√≠ c√°c t√†i li·ªáu L·ªãch s·ª≠ 11 ƒë∆∞·ª£c chia s·∫ª tr√™n THI247.com.\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 1 L·ªãch s·ª≠ 11 nƒÉm 2024 ‚Äì 2025 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k√¨ 2 L·ªãch s·ª≠ 11 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 2 L·ªãch s·ª≠ 11 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k·ª≥ 1 L·ªãch s·ª≠ 11 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Y√™n H√≤a ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng L·ªãch s·ª≠ 11 gi·ªØa k·ª≥ 1 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nTr·ªçn b·ªô l√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám m√¥n L·ªãch s·ª≠ 11\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám Vi·ªát Nam trong nh·ªØng nƒÉm chi·∫øn tranh th·∫ø gi·ªõi th·ª© nh·∫•t (1914 ‚Äì 1918)\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám Vi·ªát Nam t·ª´ ƒë·∫ßu th·∫ø k·ªâ XX ƒë·∫øn chi·∫øn tranh th·∫ø gi·ªõi th·ª© nh·∫•t (1914)\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám phong tr√†o y√™u n∆∞·ªõc ch·ªëng Ph√°p c·ªßa nh√¢n d√¢n Vi·ªát Nam trong nh·ªØng nƒÉm cu·ªëi th·∫ø k·ªâ XIX\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám cu·ªôc kh√°ng chi·∫øn ch·ªëng Ph√°p c·ªßa nh√¢n d√¢n Vi·ªát Nam t·ª´ 1958 ‚Äì 1884\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám chi·∫øn tranh th·∫ø gi·ªõi th·ª© hai (1939 ‚Äì 1945)\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám c√°c n∆∞·ªõc ch√¢u √Å gi·ªØa hai cu·ªôc chi·∫øn tranh th·∫ø gi·ªõi (1917 ‚Äì 1939)',\n",
       " '**T√†i li·ªáu L·ªãch s·ª≠ 12 - THI247.com**\\n*M√¥n: L·ªãch S·ª≠ - L·ªõp 12*\\n\\nT√†i li·ªáu L·ªãch s·ª≠ 12\\nTuy·ªÉn t·∫≠p c√°c t√†i li·ªáu L·ªãch s·ª≠ 12 hay nh·∫•t v·ªõi ƒë·∫ßy ƒë·ªß c√°c ch·ªß ƒë·ªÅ b√°m s√°t ch∆∞∆°ng tr√¨nh s√°ch gi√°o khoa L·ªãch s·ª≠ 12 hi·ªán h√†nh. C√°c t√†i li·ªáu L·ªãch s·ª≠ 12 ƒë∆∞·ª£c bi√™n so·∫°n b·ªüi qu√Ω th·∫ßy, c√¥, c√°c t√°c gi·∫£ nhi·ªÅu kinh nghi·ªám s·∫Ω gi√∫p c√°c em h·ªçc sinh kh·ªëi 12 t√¨m hi·ªÉu c√°c ki·∫øn th·ª©c m√¥n L·ªãch s·ª≠ 12.\\nC√°c t√†i li·ªáu L·ªãch s·ª≠ 12 s·∫Ω ƒë∆∞·ª£c THI247.com c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n d·ª±a v√†o ngu·ªìn s∆∞u t·∫ßm tr√™n c√°c trang m·∫°ng x√£ h·ªôi v√† c√°c trang web kh√°c tr√™n internet.\\nQu√Ω th·∫ßy, c√¥ gi√°o v√† c√°c em h·ªçc sinh kh·ªëi 12 c√≥ th·ªÉ xem v√† t·∫£i xu·ªëng mi·ªÖn ph√≠ c√°c t√†i li·ªáu L·ªãch s·ª≠ 12 ƒë∆∞·ª£c chia s·∫ª tr√™n THI247.com.\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 1 L·ªãch s·ª≠ 12 nƒÉm 2024 ‚Äì 2025 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k√¨ 2 L·ªãch s·ª≠ 12 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 2 L·ªãch s·ª≠ 12 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k·ª≥ 1 L·ªãch s·ª≠ 12 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Y√™n H√≤a ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng L·ªãch s·ª≠ 12 gi·ªØa k·ª≥ 1 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nTr·ªçn b·ªô l√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám m√¥n L·ªãch s·ª≠ 12\\n1260 c√¢u h·ªèi tr·∫Øc nghi·ªám L·ªãch s·ª≠ 12 ‚Äì Tr∆∞∆°ng Ng·ªçc Th∆°i\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám Vi·ªát Nam tr√™n con ƒë∆∞·ªùng ƒë·ªïi m·ªõi, ƒëi l√™n ch·ªß nghƒ©a x√£ h·ªôi (1986 ‚Äì 2000)\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám Vi·ªát Nam trong nh·ªØng nƒÉm ƒë·∫ßu sau th·∫Øng l·ª£i c·ªßa cu·ªôc kh√°ng chi·∫øn ch·ªëng Mƒ© (1975 ‚Äì 1986)\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám kh√¥i ph·ª•c v√† ph√°t tri·ªÉn kinh t·∫ø ‚Äì x√£ h·ªôi ·ªü mi·ªÅn B·∫Øc, gi·∫£i ph√≥ng ho√†n to√†n mi·ªÅn Nam (1973 ‚Äì 1975)\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám nh√¢n d√¢n hai mi·ªÅn Nam ‚Äì B·∫Øc tr·ª±c ti·∫øp ƒë∆∞∆°ng ƒë·∫ßu v·ªõi ƒë·∫ø qu·ªëc Mƒ© x√¢m l∆∞·ª£c (1965 ‚Äì 1973)\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám x√¢y d·ª±ng ch·ªß nghƒ©a x√£ h·ªôi ·ªü mi·ªÅn B·∫Øc, ƒë·∫•u tranh ch·ªëng ƒë·∫ø qu·ªëc Mƒ© v√† ch√≠nh quy·ªÅn S√†i G√≤n ·ªü mi·ªÅn Nam (1954 ‚Äì 1965)',\n",
       " '**T√†i li·ªáu ƒê·ªãa l√Ω 10 - THI247.com**\\n*M√¥n: ƒê·ªãa L√Ω - L·ªõp 10*\\n\\nT√†i li·ªáu ƒê·ªãa l√Ω 10\\nTuy·ªÉn t·∫≠p c√°c t√†i li·ªáu ƒê·ªãa l√Ω 10 hay nh·∫•t v·ªõi ƒë·∫ßy ƒë·ªß c√°c ch·ªß ƒë·ªÅ b√°m s√°t ch∆∞∆°ng tr√¨nh s√°ch gi√°o khoa ƒê·ªãa l√Ω 10 hi·ªán h√†nh. C√°c t√†i li·ªáu ƒê·ªãa l√Ω 10 ƒë∆∞·ª£c bi√™n so·∫°n b·ªüi qu√Ω th·∫ßy, c√¥, c√°c t√°c gi·∫£ nhi·ªÅu kinh nghi·ªám s·∫Ω gi√∫p c√°c em h·ªçc sinh kh·ªëi 10 t√¨m hi·ªÉu c√°c ki·∫øn th·ª©c m√¥n ƒê·ªãa l√Ω 10.\\nC√°c t√†i li·ªáu ƒê·ªãa l√Ω 10 s·∫Ω ƒë∆∞·ª£c THI247.com c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n d·ª±a v√†o ngu·ªìn s∆∞u t·∫ßm tr√™n c√°c trang m·∫°ng x√£ h·ªôi v√† c√°c trang web kh√°c tr√™n internet.\\nQu√Ω th·∫ßy, c√¥ gi√°o v√† c√°c em h·ªçc sinh kh·ªëi 10 c√≥ th·ªÉ xem v√† t·∫£i xu·ªëng mi·ªÖn ph√≠ c√°c t√†i li·ªáu ƒê·ªãa l√Ω 10 ƒë∆∞·ª£c chia s·∫ª tr√™n THI247.com.\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 1 ƒê·ªãa l√≠ 10 nƒÉm 2024 ‚Äì 2025 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k√¨ 2 ƒê·ªãa l√≠ 10 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 2 ƒê·ªãa l√≠ 10 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k·ª≥ 1 ƒê·ªãa l√≠ 10 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Y√™n H√≤a ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng ƒê·ªãa l√≠ 10 gi·ªØa k·ª≥ 1 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nTr·ªçn b·ªô l√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám m√¥n ƒê·ªãa l√Ω 10\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám m√¥i tr∆∞·ªùng v√† s·ª± ph√°t tri·ªÉn b·ªÅn v·ªØng\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám ƒë·ªãa l√≠ d·ªãch v·ª•\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám ƒë·ªãa l√≠ c√¥ng nghi·ªáp\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám ƒë·ªãa l√≠ n√¥ng nghi·ªáp\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám c∆° c·∫•u n·ªÅn kinh t·∫ø\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám ƒë·ªãa l√≠ d√¢n c∆∞',\n",
       " '**T√†i li·ªáu ƒê·ªãa l√Ω 11 - THI247.com**\\n*M√¥n: ƒê·ªãa L√Ω - L·ªõp 11*\\n\\nT√†i li·ªáu ƒê·ªãa l√Ω 11\\nTuy·ªÉn t·∫≠p c√°c t√†i li·ªáu ƒê·ªãa l√Ω 11 hay nh·∫•t v·ªõi ƒë·∫ßy ƒë·ªß c√°c ch·ªß ƒë·ªÅ b√°m s√°t ch∆∞∆°ng tr√¨nh s√°ch gi√°o khoa ƒê·ªãa l√Ω 11 hi·ªán h√†nh. C√°c t√†i li·ªáu ƒê·ªãa l√Ω 11 ƒë∆∞·ª£c bi√™n so·∫°n b·ªüi qu√Ω th·∫ßy, c√¥, c√°c t√°c gi·∫£ nhi·ªÅu kinh nghi·ªám s·∫Ω gi√∫p c√°c em h·ªçc sinh kh·ªëi 11 t√¨m hi·ªÉu c√°c ki·∫øn th·ª©c m√¥n ƒê·ªãa l√Ω 11.\\nC√°c t√†i li·ªáu ƒê·ªãa l√Ω 11 s·∫Ω ƒë∆∞·ª£c THI247.com c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n d·ª±a v√†o ngu·ªìn s∆∞u t·∫ßm tr√™n c√°c trang m·∫°ng x√£ h·ªôi v√† c√°c trang web kh√°c tr√™n internet.\\nQu√Ω th·∫ßy, c√¥ gi√°o v√† c√°c em h·ªçc sinh kh·ªëi 11 c√≥ th·ªÉ xem v√† t·∫£i xu·ªëng mi·ªÖn ph√≠ c√°c t√†i li·ªáu ƒê·ªãa l√Ω 11 ƒë∆∞·ª£c chia s·∫ª tr√™n THI247.com.\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 1 ƒê·ªãa l√≠ 11 nƒÉm 2024 ‚Äì 2025 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k√¨ 2 ƒê·ªãa l√≠ 11 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 2 ƒê·ªãa l√≠ 11 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k·ª≥ 1 ƒê·ªãa l√≠ 11 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Y√™n H√≤a ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng ƒê·ªãa l√≠ 11 gi·ªØa k·ª≥ 1 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nTr·ªçn b·ªô l√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám m√¥n ƒê·ªãa l√Ω 11\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám d√¢n c∆∞ √î-xtr√¢y-li-a\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám khu v·ª±c ƒê√¥ng Nam √Å\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám C·ªông H√≤a Nh√¢n D√¢n Trung Hoa (Trung Qu·ªëc)\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám Nh·∫≠t B·∫£n\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám Li√™n bang Nga\\nL√Ω thuy·∫øt v√† c√¢u h·ªèi tr·∫Øc nghi·ªám Li√™n minh ch√¢u √Çu (EU)',\n",
       " '**T√†i li·ªáu ƒê·ªãa l√Ω 12 - THI247.com**\\n*M√¥n: ƒê·ªãa L√Ω - L·ªõp 12*\\n\\nT√†i li·ªáu ƒê·ªãa l√Ω 12\\nTuy·ªÉn t·∫≠p c√°c t√†i li·ªáu ƒê·ªãa l√Ω 12 hay nh·∫•t v·ªõi ƒë·∫ßy ƒë·ªß c√°c ch·ªß ƒë·ªÅ b√°m s√°t ch∆∞∆°ng tr√¨nh s√°ch gi√°o khoa ƒê·ªãa l√Ω 12 hi·ªán h√†nh. C√°c t√†i li·ªáu ƒê·ªãa l√Ω 12 ƒë∆∞·ª£c bi√™n so·∫°n b·ªüi qu√Ω th·∫ßy, c√¥, c√°c t√°c gi·∫£ nhi·ªÅu kinh nghi·ªám s·∫Ω gi√∫p c√°c em h·ªçc sinh kh·ªëi 12 t√¨m hi·ªÉu c√°c ki·∫øn th·ª©c m√¥n ƒê·ªãa l√Ω 12.\\nC√°c t√†i li·ªáu ƒê·ªãa l√Ω 12 s·∫Ω ƒë∆∞·ª£c THI247.com c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n d·ª±a v√†o ngu·ªìn s∆∞u t·∫ßm tr√™n c√°c trang m·∫°ng x√£ h·ªôi v√† c√°c trang web kh√°c tr√™n internet.\\nQu√Ω th·∫ßy, c√¥ gi√°o v√† c√°c em h·ªçc sinh kh·ªëi 12 c√≥ th·ªÉ xem v√† t·∫£i xu·ªëng mi·ªÖn ph√≠ c√°c t√†i li·ªáu ƒê·ªãa l√Ω 12 ƒë∆∞·ª£c chia s·∫ª tr√™n THI247.com.\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 1 ƒê·ªãa l√≠ 12 nƒÉm 2024 ‚Äì 2025 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k√¨ 2 ƒê·ªãa l√≠ 12 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng gi·ªØa k·ª≥ 2 ƒê·ªãa l√≠ 12 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng h·ªçc k·ª≥ 1 ƒê·ªãa l√≠ 12 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Y√™n H√≤a ‚Äì H√† N·ªôi\\nƒê·ªÅ c∆∞∆°ng ƒê·ªãa l√≠ 12 gi·ªØa k·ª≥ 1 nƒÉm 2023 ‚Äì 2024 tr∆∞·ªùng THPT Ho√†ng VƒÉn Th·ª• ‚Äì H√† N·ªôi\\nB√†i t·∫≠p m√¥n ƒê·ªãa l√≠ 12 ch·ªß ƒë·ªÅ b·∫£ng s·ªë li·ªáu v√† bi·ªÉu ƒë·ªì\\nC√¢u h·ªèi tr·∫Øc nghi·ªám m√¥n ƒê·ªãa l√≠ 12 ch·ªß ƒë·ªÅ ƒë·ªãa l√≠ v√πng kinh t·∫ø\\nC√¢u h·ªèi tr·∫Øc nghi·ªám m√¥n ƒê·ªãa l√≠ 12 ch·ªß ƒë·ªÅ ƒë·ªãa l√≠ ng√†nh kinh t·∫ø\\nC√¢u h·ªèi tr·∫Øc nghi·ªám m√¥n ƒê·ªãa l√≠ 12 ch·ªß ƒë·ªÅ ƒë·ªãa l√≠ d√¢n c∆∞\\nC√¢u h·ªèi tr·∫Øc nghi·ªám m√¥n ƒê·ªãa l√≠ 12 ch·ªß ƒë·ªÅ ƒë·ªãa l√≠ t·ª± nhi√™n\\nC√¢u h·ªèi v√† b√†i t·∫≠p ki·ªÉm tra kƒ© nƒÉng s·ª≠ d·ª•ng Atlat ƒê·ªãa l√≠ Vi·ªát Nam\\nC√¢u h·ªèi tr·∫Øc nghi·ªám l√Ω thuy·∫øt v√† th·ª±c h√†nh m√¥n ƒê·ªãa l√≠ 12 c√≥ ƒë√°p √°n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1['display_text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a81b0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: luat\n",
      "Crawling list page 1: https://vanbanphapluat.co/loai-van-ban/luat?p=1\n",
      "No items found ‚Äì check selector with DevTools (F12)\n",
      "Processing category: nghi-dinh\n",
      "Crawling list page 1: https://vanbanphapluat.co/loai-van-ban/nghi-dinh?p=1\n",
      "No items found ‚Äì check selector with DevTools (F12)\n",
      "Processing category: thong-tu\n",
      "Crawling list page 1: https://vanbanphapluat.co/loai-van-ban/thong-tu?p=1\n",
      "No items found ‚Äì check selector with DevTools (F12)\n",
      "Processing category: cong-van\n",
      "Crawling list page 1: https://vanbanphapluat.co/loai-van-ban/cong-van?p=1\n",
      "No items found ‚Äì check selector with DevTools (F12)\n",
      "Processing category: tieu-chuan-viet-nam\n",
      "Crawling list page 1: https://vanbanphapluat.co/loai-van-ban/tieu-chuan-viet-nam?p=1\n",
      "No items found ‚Äì check selector with DevTools (F12)\n",
      "Done! Data saved to vanbanphapluat_test_data.jsonl. If still empty, inspect page with F12 and update selectors (e.g., search for 'class' of list items).\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Headers polite\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Referer': 'https://vanbanphapluat.co/'\n",
    "}\n",
    "\n",
    "# Categories ch√≠nh (t·ª´ analysis: luat, nghi-dinh, thong-tu, cong-van, tieu-chuan-viet-nam)\n",
    "categories = {\n",
    "    'luat': 'https://vanbanphapluat.co/loai-van-ban/luat',\n",
    "    'nghi-dinh': 'https://vanbanphapluat.co/loai-van-ban/nghi-dinh',\n",
    "    'thong-tu': 'https://vanbanphapluat.co/loai-van-ban/thong-tu',\n",
    "    'cong-van': 'https://vanbanphapluat.co/loai-van-ban/cong-van',\n",
    "    'tieu-chuan-viet-nam': 'https://vanbanphapluat.co/loai-van-ban/tieu-chuan-viet-nam'\n",
    "}\n",
    "\n",
    "# Function crawl list t·ª´ category + pagination\n",
    "def crawl_list(category_url, max_pages=2):  # Gi·∫£m ƒë·ªÉ test\n",
    "    documents = []\n",
    "    page = 1\n",
    "    while page <= max_pages:\n",
    "        url = f\"{category_url}?p={page}\"\n",
    "        print(f\"Crawling list page {page}: {url}\")\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"Error {resp.status_code}\")\n",
    "            break\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # Updated selector: Th·ª≠ 'ul.list-vanban li' ho·∫∑c 'div.document-list div.item' ‚Äì check F12 n·∫øu miss\n",
    "        items = soup.select('ul.list-vanban li')  # Ho·∫∑c 'div.document-list .item', 'div.vb-item'\n",
    "        if not items:  # H·∫øt page ho·∫∑c selector sai\n",
    "            print(\"No items found ‚Äì check selector with DevTools (F12)\")\n",
    "            break\n",
    "        \n",
    "        for item in items:\n",
    "            title_elem = item.select_one('a.vb-title')  # Ho·∫∑c 'a.document-title', 'a.link'\n",
    "            if title_elem:\n",
    "                title = title_elem.text.strip()\n",
    "                detail_url = 'https://vanbanphapluat.co' + title_elem['href']\n",
    "                meta = item.select_one('span.meta-info').text.strip() if item.select_one('span.meta-info') else ''  # Ho·∫∑c 'div.meta'\n",
    "                documents.append({'title': title, 'detail_url': detail_url, 'meta': meta})\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Function crawl full content t·ª´ detail URL\n",
    "def crawl_detail(detail_url):\n",
    "    print(f\"Crawling detail: {detail_url}\")\n",
    "    resp = requests.get(detail_url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    \n",
    "    # Updated selector for detail: Th·ª≠ 'span.so-hieu' cho number, 'div.noi-dung-van-ban p' cho content\n",
    "    number = soup.select_one('span.so-hieu').text.strip() if soup.select_one('span.so-hieu') else ''  # Ho·∫∑c 'span.doc-number'\n",
    "    issuance_date = soup.select_one('span.ngay-ban-hanh').text.strip() if soup.select_one('span.ngay-ban-hanh') else ''  # Ho·∫∑c 'span.issuance-date'\n",
    "    agency = soup.select_one('span.co-quan-ban-hanh').text.strip() if soup.select_one('span.co-quan-ban-hanh') else ''  # Ho·∫∑c 'span.agency'\n",
    "    status = soup.select_one('span.tinh-trang').text.strip() if soup.select_one('span.tinh-trang') else ''  # Ho·∫∑c 'span.status'\n",
    "    update_time = soup.select_one('span.ngay-cap-nhat').text.strip() if soup.select_one('span.ngay-cap-nhat') else ''  # Ho·∫∑c 'span.update-time'\n",
    "    content = ' '.join([p.text.strip() for p in soup.select('div.noi-dung-van-ban p')])  # Ho·∫∑c 'div.content-van-ban p', 'div.full-text p'\n",
    "    \n",
    "    data = {\n",
    "        'detail_url': detail_url,\n",
    "        'number': number,\n",
    "        'issuance_date': issuance_date,\n",
    "        'agency': agency,\n",
    "        'status': status,\n",
    "        'update_time': update_time,\n",
    "        'content': content\n",
    "    }\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    return data\n",
    "\n",
    "# Main: Crawl all categories\n",
    "output_file = Path(\"vanbanphapluat_test_data.jsonl\")\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for cat_name, cat_url in categories.items():\n",
    "        print(f\"Processing category: {cat_name}\")\n",
    "        docs_list = crawl_list(cat_url, max_pages=2)  # Test nh·ªè\n",
    "        docs_list = docs_list[:5]  # Gi·ªõi h·∫°n 5 items/category ƒë·ªÉ test\n",
    "        \n",
    "        for doc in docs_list:\n",
    "            full_data = crawl_detail(doc['detail_url'])\n",
    "            if full_data:\n",
    "                full_data['category'] = cat_name\n",
    "                full_data.update(doc)  # Add list metadata\n",
    "                f.write(json.dumps(full_data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Done! Data saved to {output_file}. If still empty, inspect page with F12 and update selectors (e.g., search for 'class' of list items).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "879848fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫ÆT ƒê·∫¶U CH·∫†Y TH·ª¨ (DRY RUN)...\n",
      "‚öôÔ∏è  C·∫•u h√¨nh: 2 trang/m·ª•c | Batch: 5\n",
      "\n",
      "üìÇ M·ª•c: test_van_ban_moi\n",
      "   üìÑ Page 1: https://vanbanphapluat.co/van-ban-moi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ L·∫•y ƒë∆∞·ª£c 6 b√†i m·ªõi.\n",
      "   üìÑ Page 2: https://vanbanphapluat.co/van-ban-moi?page=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ L·∫•y ƒë∆∞·ª£c 0 b√†i m·ªõi.\n",
      "\n",
      "üìÇ M·ª•c: test_luat\n",
      "   üìÑ Page 1: https://vanbanphapluat.co/loai-van-ban/luat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ L·∫•y ƒë∆∞·ª£c 80 b√†i m·ªõi.\n",
      "   üìÑ Page 2: https://vanbanphapluat.co/loai-van-ban/luat?page=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ L·∫•y ƒë∆∞·ª£c 2 b√†i m·ªõi.\n",
      "\n",
      "üì¶ ƒêang g·ªôp file k·∫øt qu·∫£...\n",
      "üéâ TH√ÄNH C√îNG! ƒê√£ l∆∞u 88 vƒÉn b·∫£n v√†o: van_ban_phap_luat_test.parquet\n",
      "üí° B·∫°n c√≥ th·ªÉ d√πng pandas ƒë·ªÉ ƒë·ªçc file n√†y ki·ªÉm tra.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import urllib3\n",
    "\n",
    "# T·∫Øt c·∫£nh b√°o SSL (Do verify=False)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# --- C·∫§U H√åNH CH·∫†Y TH·ª¨ (TEST CONFIG) ---\n",
    "DB_FILE = \"vbpl_crawler_test.db\"  # ƒê·ªïi t√™n DB ƒë·ªÉ kh√¥ng l·∫´n v·ªõi b·∫£n th·∫≠t\n",
    "OUTPUT_FILE = \"van_ban_phap_luat_test.parquet\"\n",
    "TEMP_BATCH_DIR = \"vbpl_batches_test\"\n",
    "CHECKPOINT_SIZE = 5       # Test: L∆∞u sau m·ªói 5 b√†i (ƒë·ªÉ th·∫•y k·∫øt qu·∫£ nhanh)\n",
    "MAX_PAGES_PER_CAT = 2     # Test: Ch·ªâ qu√©t 2 trang danh s√°ch m·ªói m·ª•c\n",
    "MIN_CONTENT_LENGTH = 500  # Gi·∫£m xu·ªëng m·ªôt ch√∫t ƒë·ªÉ test\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    filename=\"vbpl_crawler_test.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "os.makedirs(TEMP_BATCH_DIR, exist_ok=True)\n",
    "\n",
    "# --- DANH M·ª§C TEST (R√öT G·ªåN) ---\n",
    "# Ch·ªâ l·∫•y 2 m·ª•c ƒë·∫°i di·ªán ƒë·ªÉ test logic\n",
    "SEED_CATEGORIES = {\n",
    "    \"test_van_ban_moi\": [\"/van-ban-moi\"],\n",
    "    \"test_luat\": [\"/loai-van-ban/luat\"]\n",
    "}\n",
    "\n",
    "# --- DATABASE MANAGER (Gi·ªØ nguy√™n logic) ---\n",
    "class HistoryDB:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self.cursor = None\n",
    "        try:\n",
    "            self.conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "            self.cursor = self.conn.cursor()\n",
    "            self.cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS visited_urls (\n",
    "                    url_hash TEXT PRIMARY KEY,\n",
    "                    url TEXT UNIQUE,\n",
    "                    category TEXT,\n",
    "                    depth INTEGER,\n",
    "                    status TEXT DEFAULT 'pending',\n",
    "                    crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            self.conn.commit()\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"‚ùå Cannot connect to DB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()\n",
    "        return False\n",
    "\n",
    "    def exists(self, url):\n",
    "        try:\n",
    "            h = hashlib.md5(url.encode()).hexdigest()\n",
    "            self.cursor.execute(\"SELECT 1 FROM visited_urls WHERE url_hash = ?\", (h,))\n",
    "            return self.cursor.fetchone() is not None\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def add(self, url, category, depth=0, status='success'):\n",
    "        h = hashlib.md5(url.encode()).hexdigest()\n",
    "        try:\n",
    "            self.cursor.execute(\n",
    "                \"INSERT OR IGNORE INTO visited_urls (url_hash, url, category, depth, status) VALUES (?, ?, ?, ?, ?)\", \n",
    "                (h, url, category, depth, status)\n",
    "            )\n",
    "            self.conn.commit()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"DB insert error: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def get_session():\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'vi-VN,vi;q=0.9'\n",
    "    })\n",
    "    return session\n",
    "\n",
    "def extract_legal_document(session, url):\n",
    "    try:\n",
    "        # Timeout th·∫•p h∆°n cho test\n",
    "        resp = session.get(url, timeout=10, verify=False) \n",
    "        \n",
    "        # Validation s∆° b·ªô\n",
    "        if len(resp.text) < 1000: return None\n",
    "\n",
    "        data = trafilatura.extract(\n",
    "            resp.text,\n",
    "            output_format=\"json\",\n",
    "            include_comments=False,\n",
    "            include_tables=True,\n",
    "            favor_precision=True\n",
    "        )\n",
    "        \n",
    "        if data:\n",
    "            import json\n",
    "            j = json.loads(data)\n",
    "            text = j.get('text', '').strip()\n",
    "            \n",
    "            if len(text) < MIN_CONTENT_LENGTH: return None\n",
    "            \n",
    "            return {\n",
    "                \"title\": j.get('title', '').strip(),\n",
    "                \"text\": text,\n",
    "                \"url\": url,\n",
    "                \"length\": len(text)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Extract error {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def find_document_links(session, url):\n",
    "    links = set()\n",
    "    base_url = \"https://vanbanphapluat.co\"\n",
    "    try:\n",
    "        resp = session.get(url, timeout=10, verify=False)\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        \n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            full_url = urljoin(base_url, href)\n",
    "            \n",
    "            # Logic l·ªçc link chi ti·∫øt vƒÉn b·∫£n\n",
    "            if (base_url in full_url and \n",
    "                '/loai-van-ban/' not in full_url and\n",
    "                '/linh-vuc/' not in full_url and\n",
    "                '/van-ban-moi' not in full_url and\n",
    "                len(full_url) > len(base_url) + 10):\n",
    "                links.add(full_url)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"List page error {url}: {e}\")\n",
    "    return list(links)\n",
    "\n",
    "def save_batch(batch_data, batch_id):\n",
    "    if not batch_data: return\n",
    "    try:\n",
    "        df = pd.DataFrame(batch_data)\n",
    "        batch_file = os.path.join(TEMP_BATCH_DIR, f\"test_batch_{batch_id:04d}.parquet\")\n",
    "        df.to_parquet(batch_file, index=False)\n",
    "        logging.info(f\"üíæ Saved batch {batch_id}: {len(df)} docs\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Save batch error: {e}\")\n",
    "\n",
    "# --- CORE LOGIC (ƒê√£ s·ª≠a thu·∫≠t to√°n Loop) ---\n",
    "def crawl_category_pages(session, db, category_name, seed_urls, batch_buffer, batch_counter):\n",
    "    total_collected = 0\n",
    "    \n",
    "    for seed_url in seed_urls:\n",
    "        page = 1\n",
    "        \n",
    "        # --- TEST LIMIT: Ch·ªâ ch·∫°y s·ªë trang quy ƒë·ªãnh ---\n",
    "        while page <= MAX_PAGES_PER_CAT:\n",
    "            \n",
    "            # X√¢y d·ª±ng URL ph√¢n trang\n",
    "            if page == 1:\n",
    "                current_url = seed_url\n",
    "            else:\n",
    "                current_url = f\"{seed_url}?page={page}\"\n",
    "            \n",
    "            logging.info(f\"Scanning: {current_url}\")\n",
    "            print(f\"   üìÑ Page {page}: {current_url}\")\n",
    "\n",
    "            links = find_document_links(session, current_url)\n",
    "            \n",
    "            if not links:\n",
    "                print(\"   ‚ö†Ô∏è H·∫øt b√†i ho·∫∑c l·ªói m·∫°ng.\")\n",
    "                break\n",
    "\n",
    "            # Duy·ªát qua t·ª´ng b√†i trong trang n√†y\n",
    "            new_in_page = 0\n",
    "            for doc_url in tqdm(links, desc=f\"   Page {page}\", leave=False):\n",
    "                if db.exists(doc_url): continue\n",
    "                \n",
    "                doc = extract_legal_document(session, doc_url)\n",
    "                if doc:\n",
    "                    batch_buffer.append({\n",
    "                        \"doc_title\": doc['title'],\n",
    "                        \"doc_url\": doc['url'],\n",
    "                        \"content\": doc['text'],\n",
    "                        \"category\": category_name,\n",
    "                        \"crawled_at\": datetime.now().isoformat()\n",
    "                    })\n",
    "                    db.add(doc_url, category_name, 0, 'success')\n",
    "                    new_in_page += 1\n",
    "                    total_collected += 1\n",
    "                    \n",
    "                    # Checkpoint save\n",
    "                    if len(batch_buffer) >= CHECKPOINT_SIZE:\n",
    "                        save_batch(batch_buffer, batch_counter[0])\n",
    "                        batch_counter[0] += 1\n",
    "                        batch_buffer.clear()\n",
    "                \n",
    "                time.sleep(0.5) # Delay nh·∫π\n",
    "\n",
    "            print(f\"   ‚úÖ L·∫•y ƒë∆∞·ª£c {new_in_page} b√†i m·ªõi.\")\n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "            \n",
    "    return total_collected\n",
    "\n",
    "# --- MAIN RUN ---\n",
    "def main():\n",
    "    print(\"üöÄ B·∫ÆT ƒê·∫¶U CH·∫†Y TH·ª¨ (DRY RUN)...\")\n",
    "    print(f\"‚öôÔ∏è  C·∫•u h√¨nh: {MAX_PAGES_PER_CAT} trang/m·ª•c | Batch: {CHECKPOINT_SIZE}\")\n",
    "    \n",
    "    session = get_session()\n",
    "    batch_buffer = []\n",
    "    batch_counter = [0]\n",
    "    \n",
    "    with HistoryDB(DB_FILE) as db:\n",
    "        for cat, seeds in SEED_CATEGORIES.items():\n",
    "            print(f\"\\nüìÇ M·ª•c: {cat}\")\n",
    "            full_seeds = [urljoin(\"https://vanbanphapluat.co\", s) for s in seeds]\n",
    "            crawl_category_pages(session, db, cat, full_seeds, batch_buffer, batch_counter)\n",
    "    \n",
    "    # Save n·ªët ph·∫ßn c√≤n l·∫°i\n",
    "    if batch_buffer:\n",
    "        save_batch(batch_buffer, batch_counter[0])\n",
    "\n",
    "    # Merge file\n",
    "    print(\"\\nüì¶ ƒêang g·ªôp file k·∫øt qu·∫£...\")\n",
    "    all_files = [os.path.join(TEMP_BATCH_DIR, f) for f in os.listdir(TEMP_BATCH_DIR) if f.endswith('.parquet')]\n",
    "    if all_files:\n",
    "        combined = pd.concat([pd.read_parquet(f) for f in all_files], ignore_index=True)\n",
    "        combined.to_parquet(OUTPUT_FILE, index=False)\n",
    "        print(f\"üéâ TH√ÄNH C√îNG! ƒê√£ l∆∞u {len(combined)} vƒÉn b·∫£n v√†o: {OUTPUT_FILE}\")\n",
    "        print(\"üí° B·∫°n c√≥ th·ªÉ d√πng pandas ƒë·ªÉ ƒë·ªçc file n√†y ki·ªÉm tra.\")\n",
    "        # D·ªçn d·∫πp file r√°c\n",
    "        for f in all_files: os.remove(f)\n",
    "        os.rmdir(TEMP_BATCH_DIR)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c d·ªØ li·ªáu n√†o.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt1 = pd.read_parquet(path=\"van_ban_phap_luat_test.parquet\")\n",
    "tt1['content'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LapTop\\anaconda3\\envs\\vnpt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-14 21:32:08,647 | INFO | HTTP Request: GET https://70247b74-893b-48d1-9361-5e0f0535c1a0.us-east4-0.gcp.cloud.qdrant.io:6333 \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:08,696 | INFO | ‚ñ∂Ô∏è Processing Q:test_0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng: 370. ƒê√£ l√†m: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LapTop\\anaconda3\\envs\\vnpt\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.idg.vnpt.vn'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "2025-12-14 21:32:09,760 | INFO |    Mode: SEARCH\n",
      "c:\\Users\\LapTop\\anaconda3\\envs\\vnpt\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.idg.vnpt.vn'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QdrantClient' object has no attribute 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 260\u001b[39m\n\u001b[32m    257\u001b[39m         time.sleep(\u001b[32m5\u001b[39m) \n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 228\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    226\u001b[39m context_docs = []\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mSEARCH\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     context_docs = \u001b[43msearch_qdrant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(context_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m docs in Qdrant\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36msearch_qdrant\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m    139\u001b[39m vector = get_embedding_sync(question)\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vector: \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m hits = \u001b[43mqdrant_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m(\n\u001b[32m    143\u001b[39m     collection_name=Config.COLLECTION_NAME,\n\u001b[32m    144\u001b[39m     query_vector=vector,\n\u001b[32m    145\u001b[39m     limit=\u001b[32m5\u001b[39m \u001b[38;5;66;03m# L·∫•y 5 t√†i li·ªáu t·ªët nh·∫•t\u001b[39;00m\n\u001b[32m    146\u001b[39m )\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [hit.payload[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m hit \u001b[38;5;129;01min\u001b[39;00m hits]\n",
      "\u001b[31mAttributeError\u001b[39m: 'QdrantClient' object has no attribute 'search'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9846d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46961, 6)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "geng = pd.read_parquet(path=\"output/delta_chunks_to_index.parquet\")\n",
    "\n",
    "geng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a450f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LapTop\\anaconda3\\envs\\vnpt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to Qdrant: https://70247b74-893b-48d1-9361-5e0f0535c1a0.us-east4-0.gcp.cloud.qdrant.io:6333...\n",
      "üîç Scrolling 1 point from 'vnpt_hackathon_rag'...\n",
      "\n",
      "--- DATA FOUND ---\n",
      "üìù Payload chunk_id: 'T√¢n Ph∆∞·ªõc 2_1'\n",
      "üîë Real DB UUID    : '0000bf2a-58c4-58c8-9fde-93891ecf8ce7'\n",
      "\n",
      "--- VERIFICATION ---\n",
      "1. Testing NAMESPACE_DNS: 0000bf2a-58c4-58c8-9fde-93891ecf8ce7\n",
      "‚úÖ K·∫æT QU·∫¢: CH√çNH X√ÅC! (B·∫°n ƒëang d√πng NAMESPACE_DNS)\n",
      "üëâ Code HybridRetriever hi·ªán t·∫°i ƒë√£ ƒê√öNG. Kh√¥ng c·∫ßn s·ª≠a g√¨ c·∫£.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from qdrant_client import QdrantClient\n",
    "from config import Config\n",
    "\n",
    "def check_uuid_consistency():\n",
    "    print(f\"üîå Connecting to Qdrant: {Config.QDRANT_URL}...\")\n",
    "    client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)\n",
    "    \n",
    "    # L·∫•y th·ª≠ 1 point b·∫•t k·ª≥ t·ª´ Qdrant\n",
    "    print(f\"üîç Scrolling 1 point from '{Config.COLLECTION_NAME}'...\")\n",
    "    try:\n",
    "        points, _ = client.scroll(\n",
    "            collection_name=Config.COLLECTION_NAME,\n",
    "            limit=1,\n",
    "            with_payload=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error connecting/scrolling: {e}\")\n",
    "        return\n",
    "\n",
    "    if not points:\n",
    "        print(\"‚ùå Collection r·ªóng! Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ check.\")\n",
    "        return\n",
    "\n",
    "    point = points[0]\n",
    "    \n",
    "    # 1. L·∫•y ID th·∫≠t trong DB v√† chunk_id trong payload\n",
    "    real_db_id = str(point.id)\n",
    "    payload_chunk_id = str(point.payload.get('chunk_id'))\n",
    "    \n",
    "    print(f\"\\n--- DATA FOUND ---\")\n",
    "    print(f\"üìù Payload chunk_id: '{payload_chunk_id}'\")\n",
    "    print(f\"üîë Real DB UUID    : '{real_db_id}'\")\n",
    "    \n",
    "    # 2. T√≠nh to√°n th·ª≠ b·∫±ng NAMESPACE_DNS (C√°i ta ƒëang d√πng)\n",
    "    generated_dns = str(uuid.uuid5(uuid.NAMESPACE_DNS, payload_chunk_id))\n",
    "    \n",
    "    # 3. T√≠nh to√°n th·ª≠ b·∫±ng NAMESPACE_URL (Tr∆∞·ªùng h·ª£p nghi ng·ªù)\n",
    "    generated_url = str(uuid.uuid5(uuid.NAMESPACE_URL, payload_chunk_id))\n",
    "    \n",
    "    print(f\"\\n--- VERIFICATION ---\")\n",
    "    print(f\"1. Testing NAMESPACE_DNS: {generated_dns}\")\n",
    "    \n",
    "    if real_db_id == generated_dns:\n",
    "        print(\"‚úÖ K·∫æT QU·∫¢: CH√çNH X√ÅC! (B·∫°n ƒëang d√πng NAMESPACE_DNS)\")\n",
    "        print(\"üëâ Code HybridRetriever hi·ªán t·∫°i ƒë√£ ƒê√öNG. Kh√¥ng c·∫ßn s·ª≠a g√¨ c·∫£.\")\n",
    "    elif real_db_id == generated_url:\n",
    "        print(\"‚ö†Ô∏è K·∫æT QU·∫¢: SAI NAMESPACE! (D·ªØ li·ªáu th·ª±c t·∫ø d√πng NAMESPACE_URL)\")\n",
    "        print(\"üëâ B·∫°n c·∫ßn s·ª≠a d√≤ng uuid.NAMESPACE_DNS th√†nh uuid.NAMESPACE_URL trong class HybridRetriever.\")\n",
    "    else:\n",
    "        print(\"‚ùå K·∫æT QU·∫¢: KH√îNG KH·ªöP C·∫¢ 2!\")\n",
    "        print(\"C√≥ th·ªÉ b·∫°n ƒë√£ d√πng uuid3 (MD5), uuid4 (Random) ho·∫∑c chunk_id b·ªã bi·∫øn ƒë·ªïi (th√™m prefix/suffix) tr∆∞·ªõc khi hash.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_uuid_consistency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf9d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c787777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ tr√≠ch xu·∫•t 123 c√¢u h·ªèi.\n"
     ]
    }
   ],
   "source": [
    "target_ids = {\n",
    "    \"0003\",\"0004\",\"0005\",\"0006\",\"0009\",\"0011\",\"0013\",\"0022\",\"0025\",\"0027\",\"0029\",\"0031\",\n",
    "    \"0032\",\"0033\",\"0034\",\"0037\",\"0041\",\"0047\",\"0048\",\"0049\",\"0051\",\"0053\",\"0055\",\"0057\",\n",
    "    \"0058\",\"0060\",\"0066\",\"0068\",\"0072\",\"0076\",\"0081\",\"0084\",\"0088\",\"0090\",\"0094\",\"0097\",\n",
    "    \"0101\",\"0102\",\"0104\",\"0106\",\"0108\",\"0110\",\"0116\",\"0119\",\"0123\",\"0124\",\"0125\",\"0130\",\n",
    "    \"0137\",\"0138\",\"0144\",\"0147\",\"0148\",\"0151\",\"0155\",\"0157\",\"0168\",\"0170\",\"0171\",\"0172\",\n",
    "    \"0176\",\"0183\",\"0189\",\"0196\",\"0197\",\"0201\",\"0203\",\"0204\",\"0205\",\"0208\",\"0210\",\"0213\",\n",
    "    \"0218\",\"0219\",\"0225\",\"0232\",\"0236\",\"0238\",\"0239\",\"0240\",\"0250\",\"0260\",\"0262\",\"0265\",\n",
    "    \"0266\",\"0275\",\"0279\",\"0280\",\"0282\",\"0283\",\"0286\",\"0287\",\"0292\",\"0293\",\"0298\",\"0299\",\n",
    "    \"0300\",\"0302\",\"0303\",\"0306\",\"0308\",\"0309\",\"0313\",\"0314\",\"0321\",\"0322\",\"0325\",\"0326\",\n",
    "    \"0328\",\"0329\",\"0330\",\"0334\",\"0335\",\"0338\",\"0345\",\"0353\",\"0355\",\"0358\",\"0360\",\"0361\",\n",
    "    \"0362\",\"0363\",\"0368\"\n",
    "}\n",
    "\n",
    "filtered_data = [\n",
    "    item for item in data\n",
    "    if item[\"qid\"].split(\"_\")[1] in target_ids\n",
    "]\n",
    "\n",
    "# Ghi ra file m·ªõi\n",
    "with open(\"data/COMPULSORY.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"ƒê√£ tr√≠ch xu·∫•t {len(filtered_data)} c√¢u h·ªèi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46622b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e7c4d863-ced7-5e26-89cb-6ba7cf72c938\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# H√†m y h·ªát trong code c·ªßa b·∫°n\n",
    "def generate_uuid5(unique_string):\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_DNS, str(unique_string)))\n",
    "\n",
    "chunk_id_can_tim = \"L·ªãch s·ª≠ h√†nh ch√≠nh Y√™n B√°i_9\" # Thay t√™n chunk b·ªã l·ªói v√†o ƒë√¢y\n",
    "print(generate_uuid5(chunk_id_can_tim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203edefd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 174\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sys.platform == \u001b[33m'\u001b[39m\u001b[33mwin32\u001b[39m\u001b[33m'\u001b[39m: asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug_test_0004\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LapTop\\anaconda3\\envs\\vnpt\\Lib\\asyncio\\runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from config import Config\n",
    "\n",
    "# --- SETUP LOGGING RA M√ÄN H√åNH ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(\"DEBUG_ROUTER\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. COPY C√ÅC H√ÄM C·∫¶N THI·∫æT (HO·∫∂C IMPORT T·ª™ FILE C·ª¶A B·∫†N)\n",
    "# ==============================================================================\n",
    "\n",
    "# H√†m t·∫°o Options Map t·ª´ d·ªØ li·ªáu test\n",
    "options_map = {\n",
    "    \"A\": \"X·ª≠ l√Ω k·ª∑ lu·∫≠t\",\n",
    "    \"B\": \"X·ª≠ ph·∫°t vi ph·∫°m h√†nh ch√≠nh\",\n",
    "    \"C\": \"Truy c·ª©u tr√°ch nhi·ªám h√¨nh s·ª±\",\n",
    "    \"D\": \"Kh√¥ng x·ª≠ l√Ω\"\n",
    "}\n",
    "question_text = \"Ch·∫ø t√†i n√†o s·∫Ω √°p d·ª•ng ƒë·ªëi v·ªõi c√° nh√¢n vi ph·∫°m quy ƒë·ªãnh c·ªßa Ngh·ªã ƒë·ªãnh v·ªÅ l·ªÖ h·ªôi?\"\n",
    "\n",
    "# --- H√†m Call API (C√≥ log raw response) ---\n",
    "async def call_llm_debug(session, messages):\n",
    "    url = f\"{Config.VNPT_API_URL}/{Config.LLM_MODEL_SMALL.replace('_', '-')}\"\n",
    "    creds = Config.VNPT_CREDENTIALS.get(Config.LLM_MODEL_SMALL)\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}',\n",
    "        'Token-id': creds['token_id'],\n",
    "        'Token-key': creds['token_key'],\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": Config.LLM_MODEL_SMALL,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_completion_tokens\": 300 # ƒê·ªß d√†i ƒë·ªÉ kh√¥ng b·ªã c·∫Øt\n",
    "    }\n",
    "\n",
    "    print(\"\\nüöÄ ƒêang g·ª≠i Request l√™n Model Small...\")\n",
    "    try:\n",
    "        async with session.post(url, json=payload, headers=headers, timeout=30) as resp:\n",
    "            print(f\"üì° Status Code: {resp.status}\")\n",
    "            if resp.status == 200:\n",
    "                data = await resp.json()\n",
    "                content = data['choices'][0]['message']['content']\n",
    "                print(\"-\" * 50)\n",
    "                print(\"üìù RAW RESPONSE T·ª™ MODEL:\")\n",
    "                print(content)\n",
    "                print(\"-\" * 50)\n",
    "                return content\n",
    "            else:\n",
    "                print(f\"‚ùå Error: {await resp.text()}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- C√°c h√†m Parse (Copy t·ª´ code hi·ªán t·∫°i c·ªßa b·∫°n) ---\n",
    "def extract_balanced_json(text):\n",
    "    start = text.find('{')\n",
    "    if start == -1: return None\n",
    "    depth = 0\n",
    "    in_string = False\n",
    "    escape = False\n",
    "    for i in range(start, len(text)):\n",
    "        char = text[i]\n",
    "        if escape: escape = False; continue\n",
    "        if char == '\\\\': escape = True; continue\n",
    "        if char == '\"': in_string = not in_string; continue\n",
    "        if in_string: continue\n",
    "        if char == '{': depth += 1\n",
    "        elif char == '}': depth -= 1; \n",
    "        if depth == 0: return text[start:i+1]\n",
    "    return None\n",
    "\n",
    "def parse_json_strict(raw_response):\n",
    "    if not raw_response: return None\n",
    "    cleaned = raw_response.strip()\n",
    "    \n",
    "    # 1. Try direct\n",
    "    try: return json.loads(cleaned)\n",
    "    except: pass\n",
    "    \n",
    "    # 2. Try markdown fence\n",
    "    match = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", cleaned, re.DOTALL)\n",
    "    if match:\n",
    "        try: return json.loads(match.group(1))\n",
    "        except: pass\n",
    "        \n",
    "    # 3. Try balanced\n",
    "    json_str = extract_balanced_json(cleaned)\n",
    "    if json_str:\n",
    "        try: return json.loads(json_str)\n",
    "        except: pass\n",
    "        \n",
    "    # 4. Fallback extraction (Quan tr·ªçng n·∫øu JSON l·ªói)\n",
    "    print(\"‚ö†Ô∏è JSON Parse chu·∫©n th·∫•t b·∫°i -> Th·ª≠ tr√≠ch xu·∫•t t·ª´ kh√≥a...\")\n",
    "    safety = re.search(r'(?:safety|an to√†n)[\":\\s]*(SAFE|UNSAFE)', cleaned, re.IGNORECASE)\n",
    "    domain = re.search(r'(?:domain|lƒ©nh v·ª±c)[\":\\s]*(STEM|LEGAL|SOCIAL)', cleaned, re.IGNORECASE)\n",
    "    \n",
    "    if safety and domain:\n",
    "        return {\n",
    "            \"safety\": safety.group(1).upper(),\n",
    "            \"domain\": domain.group(1).upper(),\n",
    "            \"refusal_status\": \"NONE\" # Default\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. H√ÄM MAIN DEBUG\n",
    "# ==============================================================================\n",
    "async def debug_test_0004():\n",
    "    # 1. Setup Prompt (Copy y h·ªát t·ª´ unified_router_fixed)\n",
    "    options_text = \"\\n\".join([f\"{k}. {v}\" for k, v in options_map.items()])\n",
    "    \n",
    "    system_prompt = \"\"\"Ph√¢n lo·∫°i c√¢u h·ªèi thi theo 2 ti√™u ch√≠:\n",
    "\n",
    "1. SAFETY (An to√†n):\n",
    "   UNSAFE: H∆∞·ªõng d·∫´n C√ÅCH L√ÄM h√†nh vi vi ph·∫°m (tr·ªën thu·∫ø, l√†m gi·∫£, ph√° ho·∫°i...).\n",
    "   SAFE: H·ªèi ki·∫øn th·ª©c (k·ªÉ c·∫£ chi·∫øn tranh, lu·∫≠t h√¨nh s·ª±, y h·ªçc).\n",
    "\n",
    "2. DOMAIN (Lƒ©nh v·ª±c):\n",
    "   STEM: To√°n, L√Ω, H√≥a (c√≥ c√¥ng th·ª©c).\n",
    "   LEGAL: Lu·∫≠t ph√°p (ƒêi·ªÅu X, B·ªô lu·∫≠t, Ngh·ªã ƒë·ªãnh, Ch·∫ø t√†i).\n",
    "   SOCIAL: L·ªãch s·ª≠, ƒê·ªãa l√Ω, VƒÉn h·ªçc.\n",
    "\n",
    "OUTPUT ch·ªâ 1 d√≤ng JSON:\n",
    "{\"safety\":\"SAFE|UNSAFE\",\"domain\":\"STEM|LEGAL|SOCIAL\"}\"\"\"\n",
    "\n",
    "    user_content = f\"C√¢u h·ªèi: {question_text}\\n\\nƒê√°p √°n:\\n{options_text}\\n\\nJSON:\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    # 2. G·ªçi API v√† Debug\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        raw_resp = await call_llm_debug(session, messages)\n",
    "        \n",
    "        if raw_resp:\n",
    "            print(\"\\nüîç ƒêANG TH·ª¨ PARSE JSON...\")\n",
    "            result = parse_json_strict(raw_resp)\n",
    "            \n",
    "            if result:\n",
    "                print(f\"‚úÖ PARSE TH√ÄNH C√îNG: {result}\")\n",
    "                \n",
    "                # Gi·∫£ l·∫≠p logic x·ª≠ l√Ω\n",
    "                safety = result.get(\"safety\", \"SAFE\").upper()\n",
    "                domain = result.get(\"domain\", \"SOCIAL\").upper()\n",
    "                print(f\"üëâ K·∫æT LU·∫¨N: Safety={safety}, Domain={domain}\")\n",
    "                \n",
    "                if domain == \"LEGAL\":\n",
    "                    print(\"=> S·∫Ω d√πng Model LARGE (ƒê√∫ng mong ƒë·ª£i)\")\n",
    "                else:\n",
    "                    print(\"=> C·∫£nh b√°o: Domain ch∆∞a chu·∫©n.\")\n",
    "            else:\n",
    "                print(\"‚ùå PARSE TH·∫§T B·∫†I HO√ÄN TO√ÄN! (L·ªói NoneType s·∫Ω x·∫£y ra ·ªü ƒë√¢y n·∫øu code kh√¥ng check)\")\n",
    "        else:\n",
    "            print(\"‚ùå KH√îNG NH·∫¨N ƒê∆Ø·ª¢C PH·∫¢N H·ªíI (Empty/Null)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if sys.platform == 'win32': asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "    asyncio.run(debug_test_0004())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08137312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t5 = pd.read_parquet(path=\"output_batch_chunking/vbpl_raw.parquet\")\n",
    "t6 = pd.read_parquet(path=\"output_batch_chunking/delta_chunks_to_index_new.parquet\")\n",
    "t7 = pd.read_parquet(path=\"output_batch_chunking/delta_chunks_to_index.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f4dbc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63898, 7), (209010, 9), (209457, 6))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5.shape, t6.shape, t7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "462b420c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482365"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0 = t5.shape[0] + t6.shape[0] + t7.shape[0]\n",
    "a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "618d8a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>doc_category</th>\n",
       "      <th>doc_url</th>\n",
       "      <th>vector_text</th>\n",
       "      <th>display_text</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vbpl_raw.parquet_0_0</td>\n",
       "      <td>C√¥ng vƒÉn 95/BTC-TCT</td>\n",
       "      <td>Doanh Nghiep</td>\n",
       "      <td>https://vanbanphapluat.co/cong-van-95-btc-tct-...</td>\n",
       "      <td>VƒÉn b·∫£n: C√¥ng vƒÉn 95/BTC-TCT. Lƒ©nh v·ª±c: Doanh ...</td>\n",
       "      <td>B·ªò T√ÄI CH√çNH CH·ª¶ NGHƒ®A VI·ªÜT NAM S·ªë: 95/BTC-TCT...</td>\n",
       "      <td>974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vbpl_raw.parquet_0_1</td>\n",
       "      <td>C√¥ng vƒÉn 95/BTC-TCT</td>\n",
       "      <td>Doanh Nghiep</td>\n",
       "      <td>https://vanbanphapluat.co/cong-van-95-btc-tct-...</td>\n",
       "      <td>VƒÉn b·∫£n: C√¥ng vƒÉn 95/BTC-TCT. Lƒ©nh v·ª±c: Doanh ...</td>\n",
       "      <td>. 2. Nguy√™n t·∫Øc v·∫≠n h√†nh c·ªßa ch·ª©c nƒÉng BƒêSHKD ...</td>\n",
       "      <td>990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vbpl_raw.parquet_1_0</td>\n",
       "      <td>Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP</td>\n",
       "      <td>Doanh Nghiep</td>\n",
       "      <td>https://vanbanphapluat.co/nghi-dinh-huong-dan-...</td>\n",
       "      <td>VƒÉn b·∫£n: Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP. Lƒ©nh v·ª±c: D...</td>\n",
       "      <td>CH√çNH PH·ª¶ CH·ª¶ NGHƒ®A VI·ªÜT NAM S·ªë: 113/2024/Nƒê-C...</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vbpl_raw.parquet_1_1</td>\n",
       "      <td>Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP</td>\n",
       "      <td>Doanh Nghiep</td>\n",
       "      <td>https://vanbanphapluat.co/nghi-dinh-huong-dan-...</td>\n",
       "      <td>VƒÉn b·∫£n: Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP. Lƒ©nh v·ª±c: D...</td>\n",
       "      <td>. 2. Trang thi·∫øt b·ªã bao g·ªìm m√°y m√≥c, thi·∫øt b·ªã ...</td>\n",
       "      <td>928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vbpl_raw.parquet_1_2</td>\n",
       "      <td>Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP</td>\n",
       "      <td>Doanh Nghiep</td>\n",
       "      <td>https://vanbanphapluat.co/nghi-dinh-huong-dan-...</td>\n",
       "      <td>VƒÉn b·∫£n: Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP. Lƒ©nh v·ª±c: D...</td>\n",
       "      <td>. Tr∆∞·ªùng h·ª£p h·ª£p t√°c x√£ ho·∫°t ƒë·ªông d∆∞·ªõi 01 nƒÉm ...</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               chunk_id                 doc_title  doc_category  \\\n",
       "0  vbpl_raw.parquet_0_0       C√¥ng vƒÉn 95/BTC-TCT  Doanh Nghiep   \n",
       "1  vbpl_raw.parquet_0_1       C√¥ng vƒÉn 95/BTC-TCT  Doanh Nghiep   \n",
       "2  vbpl_raw.parquet_1_0  Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP  Doanh Nghiep   \n",
       "3  vbpl_raw.parquet_1_1  Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP  Doanh Nghiep   \n",
       "4  vbpl_raw.parquet_1_2  Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP  Doanh Nghiep   \n",
       "\n",
       "                                             doc_url  \\\n",
       "0  https://vanbanphapluat.co/cong-van-95-btc-tct-...   \n",
       "1  https://vanbanphapluat.co/cong-van-95-btc-tct-...   \n",
       "2  https://vanbanphapluat.co/nghi-dinh-huong-dan-...   \n",
       "3  https://vanbanphapluat.co/nghi-dinh-huong-dan-...   \n",
       "4  https://vanbanphapluat.co/nghi-dinh-huong-dan-...   \n",
       "\n",
       "                                         vector_text  \\\n",
       "0  VƒÉn b·∫£n: C√¥ng vƒÉn 95/BTC-TCT. Lƒ©nh v·ª±c: Doanh ...   \n",
       "1  VƒÉn b·∫£n: C√¥ng vƒÉn 95/BTC-TCT. Lƒ©nh v·ª±c: Doanh ...   \n",
       "2  VƒÉn b·∫£n: Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP. Lƒ©nh v·ª±c: D...   \n",
       "3  VƒÉn b·∫£n: Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP. Lƒ©nh v·ª±c: D...   \n",
       "4  VƒÉn b·∫£n: Ngh·ªã ƒë·ªãnh 113/2024/Nƒê-CP. Lƒ©nh v·ª±c: D...   \n",
       "\n",
       "                                        display_text  token_count  \n",
       "0  B·ªò T√ÄI CH√çNH CH·ª¶ NGHƒ®A VI·ªÜT NAM S·ªë: 95/BTC-TCT...          974  \n",
       "1  . 2. Nguy√™n t·∫Øc v·∫≠n h√†nh c·ªßa ch·ª©c nƒÉng BƒêSHKD ...          990  \n",
       "2  CH√çNH PH·ª¶ CH·ª¶ NGHƒ®A VI·ªÜT NAM S·ªë: 113/2024/Nƒê-C...          835  \n",
       "3  . 2. Trang thi·∫øt b·ªã bao g·ªìm m√°y m√≥c, thi·∫øt b·ªã ...          928  \n",
       "4  . Tr∆∞·ªùng h·ª£p h·ª£p t√°c x√£ ho·∫°t ƒë·ªông d∆∞·ªõi 01 nƒÉm ...          965  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "436a455b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>doc_category</th>\n",
       "      <th>vector_text</th>\n",
       "      <th>display_text</th>\n",
       "      <th>doc_url</th>\n",
       "      <th>has_math</th>\n",
       "      <th>has_legal</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arifmetika_0</td>\n",
       "      <td>Arifmetika</td>\n",
       "      <td>['S√°ch_gi√°o_khoa_to√°n_h·ªçc']</td>\n",
       "      <td>[['S√°ch gi√°o khoa to√°n h·ªçc']] Arifmetika\\n# Ar...</td>\n",
       "      <td># Arifmetika\\n\\nArifmetika (ti·∫øng Nga: –ê—Ä–∏—Ñ–º–µ—Ç...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Arifmetika</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>stem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arifmetika_1</td>\n",
       "      <td>Arifmetika</td>\n",
       "      <td>['S√°ch_gi√°o_khoa_to√°n_h·ªçc']</td>\n",
       "      <td>[['S√°ch gi√°o khoa to√°n h·ªçc']] Arifmetika\\nNgu·ªì...</td>\n",
       "      <td>Ngu·ªìn g·ªëc c·ªßa cu·ªën s√°ch n·∫±m ·ªü vi·ªác Peter ƒê·∫°i ƒë...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Arifmetika</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>stem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gi·∫£i Amp√®re_0</td>\n",
       "      <td>Gi·∫£i Amp√®re</td>\n",
       "      <td>['Gi·∫£i_th∆∞·ªüng_to√°n_h·ªçc']</td>\n",
       "      <td>[['Gi·∫£i th∆∞·ªüng to√°n h·ªçc']] Gi·∫£i Amp√®re\\n# Gi·∫£i...</td>\n",
       "      <td># Gi·∫£i Amp√®re\\n\\nGi·∫£i Amp√®re l√† m·ªôt gi·∫£i th∆∞·ªün...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Gi%E1%BA%A3i_Amp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>stem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MathWorld_0</td>\n",
       "      <td>MathWorld</td>\n",
       "      <td>['Gi·∫£ng_d·∫°y_to√°n_h·ªçc']</td>\n",
       "      <td>[['Gi·∫£ng d·∫°y to√°n h·ªçc']] MathWorld\\n# MathWorl...</td>\n",
       "      <td># MathWorld\\n\\nMathWorld l√† m·ªôt trang web tham...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/MathWorld</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>stem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MathWorld_1</td>\n",
       "      <td>MathWorld</td>\n",
       "      <td>['Gi·∫£ng_d·∫°y_to√°n_h·ªçc']</td>\n",
       "      <td>[['Gi·∫£ng d·∫°y to√°n h·ªçc']] MathWorld\\nL·ªãch s·ª≠\\n\\...</td>\n",
       "      <td>L·ªãch s·ª≠\\n\\nEric W. Weisstein, ng∆∞·ªùi kh·ªüi t·∫°o t...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/MathWorld</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>stem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        chunk_id    doc_title                 doc_category  \\\n",
       "0   Arifmetika_0   Arifmetika  ['S√°ch_gi√°o_khoa_to√°n_h·ªçc']   \n",
       "1   Arifmetika_1   Arifmetika  ['S√°ch_gi√°o_khoa_to√°n_h·ªçc']   \n",
       "2  Gi·∫£i Amp√®re_0  Gi·∫£i Amp√®re     ['Gi·∫£i_th∆∞·ªüng_to√°n_h·ªçc']   \n",
       "3    MathWorld_0    MathWorld       ['Gi·∫£ng_d·∫°y_to√°n_h·ªçc']   \n",
       "4    MathWorld_1    MathWorld       ['Gi·∫£ng_d·∫°y_to√°n_h·ªçc']   \n",
       "\n",
       "                                         vector_text  \\\n",
       "0  [['S√°ch gi√°o khoa to√°n h·ªçc']] Arifmetika\\n# Ar...   \n",
       "1  [['S√°ch gi√°o khoa to√°n h·ªçc']] Arifmetika\\nNgu·ªì...   \n",
       "2  [['Gi·∫£i th∆∞·ªüng to√°n h·ªçc']] Gi·∫£i Amp√®re\\n# Gi·∫£i...   \n",
       "3  [['Gi·∫£ng d·∫°y to√°n h·ªçc']] MathWorld\\n# MathWorl...   \n",
       "4  [['Gi·∫£ng d·∫°y to√°n h·ªçc']] MathWorld\\nL·ªãch s·ª≠\\n\\...   \n",
       "\n",
       "                                        display_text  \\\n",
       "0  # Arifmetika\\n\\nArifmetika (ti·∫øng Nga: –ê—Ä–∏—Ñ–º–µ—Ç...   \n",
       "1  Ngu·ªìn g·ªëc c·ªßa cu·ªën s√°ch n·∫±m ·ªü vi·ªác Peter ƒê·∫°i ƒë...   \n",
       "2  # Gi·∫£i Amp√®re\\n\\nGi·∫£i Amp√®re l√† m·ªôt gi·∫£i th∆∞·ªün...   \n",
       "3  # MathWorld\\n\\nMathWorld l√† m·ªôt trang web tham...   \n",
       "4  L·ªãch s·ª≠\\n\\nEric W. Weisstein, ng∆∞·ªùi kh·ªüi t·∫°o t...   \n",
       "\n",
       "                                             doc_url  has_math  has_legal  \\\n",
       "0           https://vi.wikipedia.org/wiki/Arifmetika     False      False   \n",
       "1           https://vi.wikipedia.org/wiki/Arifmetika     False      False   \n",
       "2  https://vi.wikipedia.org/wiki/Gi%E1%BA%A3i_Amp...     False      False   \n",
       "3            https://vi.wikipedia.org/wiki/MathWorld     False      False   \n",
       "4            https://vi.wikipedia.org/wiki/MathWorld     False      False   \n",
       "\n",
       "  domain  \n",
       "0   stem  \n",
       "1   stem  \n",
       "2   stem  \n",
       "3   stem  \n",
       "4   stem  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "523d5396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>doc_category</th>\n",
       "      <th>vector_text</th>\n",
       "      <th>display_text</th>\n",
       "      <th>doc_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam_0</td>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam</td>\n",
       "      <td>T·ªïng h·ª£p</td>\n",
       "      <td>Lƒ©nh v·ª±c: T·ªïng h·ª£p. Ch·ªß ƒë·ªÅ: Anh h√πng d√¢n t·ªôc V...</td>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam l√† thu·∫≠t ng·ªØ ch·ªâ nh·ªØ...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam_1</td>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam</td>\n",
       "      <td>T·ªïng h·ª£p</td>\n",
       "      <td>Lƒ©nh v·ª±c: T·ªïng h·ª£p. Ch·ªß ƒë·ªÅ: Anh h√πng d√¢n t·ªôc V...</td>\n",
       "      <td>. Trong su·ªët l·ªãch s·ª≠ h√†ng ngh√¨n nƒÉm b·∫£o v·ªá v√† ...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam_2</td>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam</td>\n",
       "      <td>T·ªïng h·ª£p</td>\n",
       "      <td>Lƒ©nh v·ª±c: T·ªïng h·ª£p. Ch·ªß ƒë·ªÅ: Anh h√πng d√¢n t·ªôc V...</td>\n",
       "      <td>. ƒêinh Ti√™n Ho√†ng, t·ª©c ƒêinh B·ªô Lƒ©nh: ng∆∞·ªùi ƒë√°n...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam_3</td>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam</td>\n",
       "      <td>T·ªïng h·ª£p</td>\n",
       "      <td>Lƒ©nh v·ª±c: T·ªïng h·ª£p. Ch·ªß ƒë·ªÅ: Anh h√πng d√¢n t·ªôc V...</td>\n",
       "      <td>. Nguy·ªÖn Tr√£i: nh√† vƒÉn h√≥a v√† t∆∞ t∆∞·ªüng l·ªói l·∫°c...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam_4</td>\n",
       "      <td>Anh h√πng d√¢n t·ªôc Vi·ªát Nam</td>\n",
       "      <td>T·ªïng h·ª£p</td>\n",
       "      <td>Lƒ©nh v·ª±c: T·ªïng h·ª£p. Ch·ªß ƒë·ªÅ: Anh h√πng d√¢n t·ªôc V...</td>\n",
       "      <td>. Ng∆∞·ªùi ƒë·ª©ng ƒë·∫ßu 1 v∆∞∆°ng tri·ªÅu c√≥ ƒë√≥ng g√≥p ƒë·∫∑c...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      chunk_id                  doc_title doc_category  \\\n",
       "0  Anh h√πng d√¢n t·ªôc Vi·ªát Nam_0  Anh h√πng d√¢n t·ªôc Vi·ªát Nam     T·ªïng h·ª£p   \n",
       "1  Anh h√πng d√¢n t·ªôc Vi·ªát Nam_1  Anh h√πng d√¢n t·ªôc Vi·ªát Nam     T·ªïng h·ª£p   \n",
       "2  Anh h√πng d√¢n t·ªôc Vi·ªát Nam_2  Anh h√πng d√¢n t·ªôc Vi·ªát Nam     T·ªïng h·ª£p   \n",
       "3  Anh h√πng d√¢n t·ªôc Vi·ªát Nam_3  Anh h√πng d√¢n t·ªôc Vi·ªát Nam     T·ªïng h·ª£p   \n",
       "4  Anh h√πng d√¢n t·ªôc Vi·ªát Nam_4  Anh h√πng d√¢n t·ªôc Vi·ªát Nam     T·ªïng h·ª£p   \n",
       "\n",
       "                                         vector_text  \\\n",
       "0  Lƒ©nh v·ª±c: T·ªïng h·ª£p. Ch·ªß ƒë·ªÅ: Anh h√πng d√¢n t·ªôc V...   \n",
       "1  Lƒ©nh v·ª±c: T·ªïng h·ª£p. Ch·ªß ƒë·ªÅ: Anh h√πng d√¢n t·ªôc V...   \n",
       "2  Lƒ©nh v·ª±c: T·ªïng h·ª£p. Ch·ªß ƒë·ªÅ: Anh h√πng d√¢n t·ªôc V...   \n",
       "3  Lƒ©nh v·ª±c: T·ªïng h·ª£p. Ch·ªß ƒë·ªÅ: Anh h√πng d√¢n t·ªôc V...   \n",
       "4  Lƒ©nh v·ª±c: T·ªïng h·ª£p. Ch·ªß ƒë·ªÅ: Anh h√πng d√¢n t·ªôc V...   \n",
       "\n",
       "                                        display_text  \\\n",
       "0  Anh h√πng d√¢n t·ªôc Vi·ªát Nam l√† thu·∫≠t ng·ªØ ch·ªâ nh·ªØ...   \n",
       "1  . Trong su·ªët l·ªãch s·ª≠ h√†ng ngh√¨n nƒÉm b·∫£o v·ªá v√† ...   \n",
       "2  . ƒêinh Ti√™n Ho√†ng, t·ª©c ƒêinh B·ªô Lƒ©nh: ng∆∞·ªùi ƒë√°n...   \n",
       "3  . Nguy·ªÖn Tr√£i: nh√† vƒÉn h√≥a v√† t∆∞ t∆∞·ªüng l·ªói l·∫°c...   \n",
       "4  . Ng∆∞·ªùi ƒë·ª©ng ƒë·∫ßu 1 v∆∞∆°ng tri·ªÅu c√≥ ƒë√≥ng g√≥p ƒë·∫∑c...   \n",
       "\n",
       "                                             doc_url  \n",
       "0  https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...  \n",
       "1  https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...  \n",
       "2  https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...  \n",
       "3  https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...  \n",
       "4  https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45183cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VƒÉn b·∫£n: Hi·∫øn ph√°p n∆∞·ªõc CHXHCN Vi·ªát Nam 2013\\nCH∆Ø∆†NG III - KINH T·∫æ, X√É H·ªòI, VƒÇN H√ìA, GI√ÅO D·ª§C, KHOA H·ªåC, C√îNG NGH·ªÜ V√Ä M√îI TR∆Ø·ªúNG\\nƒêi·ªÅu 51\\n1. N·ªÅn kinh t·∫ø Vi·ªát Nam l√† n·ªÅn kinh t·∫ø th·ªã tr∆∞·ªùng ƒë·ªãnh h∆∞·ªõng x√£ h·ªôi ch·ªß nghƒ©a v·ªõi nhi·ªÅu h√¨nh th·ª©c s·ªü h·ªØu, nhi·ªÅu th√†nh ph·∫ßn kinh t·∫ø; kinh t·∫ø nh√† n∆∞·ªõc gi·ªØ vai tr√≤ ch·ªß ƒë·∫°o.\\n2. C√°c th√†nh ph·∫ßn kinh t·∫ø ƒë·ªÅu l√† b·ªô ph·∫≠n c·∫•u th√†nh quan tr·ªçng c·ªßa n·ªÅn kinh t·∫ø qu·ªëc d√¢n. C√°c ch·ªß th·ªÉ thu·ªôc c√°c th√†nh ph·∫ßn kinh t·∫ø b√¨nh ƒë·∫≥ng, h·ª£p t√°c v√† c·∫°nh tranh theo ph√°p lu·∫≠t.\\n3. Nh√† n∆∞·ªõc khuy·∫øn kh√≠ch, t·∫°o ƒëi·ªÅu ki·ªán ƒë·ªÉ doanh nh√¢n, doanh nghi·ªáp v√† c√° nh√¢n, t·ªï ch·ª©c kh√°c ƒë·∫ßu t∆∞, s·∫£n xu·∫•t, kinh doanh; ph√°t tri·ªÉn b·ªÅn v·ªØng c√°c ng√†nh kinh t·∫ø, g√≥p ph·∫ßn x√¢y d·ª±ng ƒë·∫•t n∆∞·ªõc. T√†i s·∫£n h·ª£p ph√°p c·ªßa c√° nh√¢n, t√¥Ãâ ch∆∞ÃÅc ƒë·∫ßu t∆∞, s·∫£n xu·∫•t, kinh doanh ƒë∆∞·ª£c ph√°p lu·∫≠t b·∫£o h·ªô v√† kh√¥ng b·ªã qu·ªëc h·ªØu h√≥a.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t8 = pd.read_parquet(path=\"output_batch_chunking/1_manual_law_strict.parquet\")\n",
    "\n",
    "t8['display_text'].iloc[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8bc155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209010, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "geng = pd.read_parquet(path=\"output_batch_chunking/delta_chunks_to_index_new.parquet\")\n",
    "\n",
    "geng.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
