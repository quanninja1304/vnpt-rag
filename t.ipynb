{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07104997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b88cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "huhu = pd.read_parquet(path=\"output/delta_chunks_to_index.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722c1b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114797, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huhu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a62ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hihi = pd.read_parquet(path=\"output/1_manual_law_strict.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088381aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2084, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hihi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efb13d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>doc_category</th>\n",
       "      <th>doc_url</th>\n",
       "      <th>vector_text</th>\n",
       "      <th>display_text</th>\n",
       "      <th>crawled_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>law_luat_hon_nhan_va_gia_dinh.txt_0</td>\n",
       "      <td>Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014</td>\n",
       "      <td>PhÃ¡p luáº­t</td>\n",
       "      <td>local/luat_hon_nhan_va_gia_dinh.txt</td>\n",
       "      <td>VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...</td>\n",
       "      <td>VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...</td>\n",
       "      <td>2025-12-11T10:46:46.879494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>law_luat_hon_nhan_va_gia_dinh.txt_1</td>\n",
       "      <td>Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014</td>\n",
       "      <td>PhÃ¡p luáº­t</td>\n",
       "      <td>local/luat_hon_nhan_va_gia_dinh.txt</td>\n",
       "      <td>VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...</td>\n",
       "      <td>VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...</td>\n",
       "      <td>2025-12-11T10:46:46.879494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>law_luat_hon_nhan_va_gia_dinh.txt_2</td>\n",
       "      <td>Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014</td>\n",
       "      <td>PhÃ¡p luáº­t</td>\n",
       "      <td>local/luat_hon_nhan_va_gia_dinh.txt</td>\n",
       "      <td>VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...</td>\n",
       "      <td>VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...</td>\n",
       "      <td>2025-12-11T10:46:46.879494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>law_luat_hon_nhan_va_gia_dinh.txt_3</td>\n",
       "      <td>Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014</td>\n",
       "      <td>PhÃ¡p luáº­t</td>\n",
       "      <td>local/luat_hon_nhan_va_gia_dinh.txt</td>\n",
       "      <td>VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...</td>\n",
       "      <td>VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...</td>\n",
       "      <td>2025-12-11T10:46:46.879494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>law_luat_hon_nhan_va_gia_dinh.txt_4</td>\n",
       "      <td>Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014</td>\n",
       "      <td>PhÃ¡p luáº­t</td>\n",
       "      <td>local/luat_hon_nhan_va_gia_dinh.txt</td>\n",
       "      <td>VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...</td>\n",
       "      <td>VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...</td>\n",
       "      <td>2025-12-11T10:46:46.879494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              chunk_id                       doc_title  \\\n",
       "0  law_luat_hon_nhan_va_gia_dinh.txt_0  Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014   \n",
       "1  law_luat_hon_nhan_va_gia_dinh.txt_1  Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014   \n",
       "2  law_luat_hon_nhan_va_gia_dinh.txt_2  Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014   \n",
       "3  law_luat_hon_nhan_va_gia_dinh.txt_3  Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014   \n",
       "4  law_luat_hon_nhan_va_gia_dinh.txt_4  Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014   \n",
       "\n",
       "  doc_category                              doc_url  \\\n",
       "0    PhÃ¡p luáº­t  local/luat_hon_nhan_va_gia_dinh.txt   \n",
       "1    PhÃ¡p luáº­t  local/luat_hon_nhan_va_gia_dinh.txt   \n",
       "2    PhÃ¡p luáº­t  local/luat_hon_nhan_va_gia_dinh.txt   \n",
       "3    PhÃ¡p luáº­t  local/luat_hon_nhan_va_gia_dinh.txt   \n",
       "4    PhÃ¡p luáº­t  local/luat_hon_nhan_va_gia_dinh.txt   \n",
       "\n",
       "                                         vector_text  \\\n",
       "0  VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...   \n",
       "1  VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...   \n",
       "2  VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...   \n",
       "3  VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...   \n",
       "4  VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...   \n",
       "\n",
       "                                        display_text  \\\n",
       "0  VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...   \n",
       "1  VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...   \n",
       "2  VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...   \n",
       "3  VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...   \n",
       "4  VÄƒn báº£n: Luáº­t HÃ´n nhÃ¢n vÃ  Gia Ä‘Ã¬nh 2014\\nChÆ°Æ¡n...   \n",
       "\n",
       "                   crawled_at  \n",
       "0  2025-12-11T10:46:46.879494  \n",
       "1  2025-12-11T10:46:46.879494  \n",
       "2  2025-12-11T10:46:46.879494  \n",
       "3  2025-12-11T10:46:46.879494  \n",
       "4  2025-12-11T10:46:46.879494  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hihi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1bb87d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>doc_category</th>\n",
       "      <th>vector_text</th>\n",
       "      <th>display_text</th>\n",
       "      <th>doc_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam_0</td>\n",
       "      <td>Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam</td>\n",
       "      <td>NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam</td>\n",
       "      <td>LÄ©nh vá»±c: NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam. Chá»§ Ä‘á»: A...</td>\n",
       "      <td>Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam lÃ  thuáº­t ngá»¯ chá»‰ nhá»¯...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam_1</td>\n",
       "      <td>Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam</td>\n",
       "      <td>NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam</td>\n",
       "      <td>LÄ©nh vá»±c: NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam. Chá»§ Ä‘á»: A...</td>\n",
       "      <td>Äinh TiÃªn HoÃ ng, tá»©c Äinh Bá»™ LÄ©nh: ngÆ°á»i Ä‘Ã¡nh ...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam_2</td>\n",
       "      <td>Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam</td>\n",
       "      <td>NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam</td>\n",
       "      <td>LÄ©nh vá»±c: NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam. Chá»§ Ä‘á»: A...</td>\n",
       "      <td>TiÃªu chuáº©n\\n\\n14 vá»‹ Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam ...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An DÆ°Æ¡ng VÆ°Æ¡ng_0</td>\n",
       "      <td>An DÆ°Æ¡ng VÆ°Æ¡ng</td>\n",
       "      <td>Triá»u Ä‘áº¡i Viá»‡t Nam</td>\n",
       "      <td>LÄ©nh vá»±c: Triá»u Ä‘áº¡i Viá»‡t Nam. Chá»§ Ä‘á»: An DÆ°Æ¡ng...</td>\n",
       "      <td>An DÆ°Æ¡ng VÆ°Æ¡ng (chá»¯ HÃ¡n: å®‰é™½ç‹), tÃªn tháº­t lÃ  Thá»¥...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/An_D%C6%B0%C6%A1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NhÃ  nÆ°á»›c Viá»‡t Nam_0</td>\n",
       "      <td>NhÃ  nÆ°á»›c Viá»‡t Nam</td>\n",
       "      <td>NhÃ  nÆ°á»›c Viá»‡t Nam</td>\n",
       "      <td>LÄ©nh vá»±c: NhÃ  nÆ°á»›c Viá»‡t Nam. Chá»§ Ä‘á»: NhÃ  nÆ°á»›c ...</td>\n",
       "      <td>NhÃ  nÆ°á»›c Cá»™ng hÃ²a xÃ£ há»™i chá»§ nghÄ©a Viá»‡t Nam lÃ ...</td>\n",
       "      <td>https://vi.wikipedia.org/wiki/Nh%C3%A0_n%C6%B0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      chunk_id                  doc_title  \\\n",
       "0  Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam_0  Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam   \n",
       "1  Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam_1  Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam   \n",
       "2  Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam_2  Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam   \n",
       "3             An DÆ°Æ¡ng VÆ°Æ¡ng_0             An DÆ°Æ¡ng VÆ°Æ¡ng   \n",
       "4          NhÃ  nÆ°á»›c Viá»‡t Nam_0          NhÃ  nÆ°á»›c Viá»‡t Nam   \n",
       "\n",
       "                doc_category  \\\n",
       "0  NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam   \n",
       "1  NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam   \n",
       "2  NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam   \n",
       "3         Triá»u Ä‘áº¡i Viá»‡t Nam   \n",
       "4          NhÃ  nÆ°á»›c Viá»‡t Nam   \n",
       "\n",
       "                                         vector_text  \\\n",
       "0  LÄ©nh vá»±c: NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam. Chá»§ Ä‘á»: A...   \n",
       "1  LÄ©nh vá»±c: NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam. Chá»§ Ä‘á»: A...   \n",
       "2  LÄ©nh vá»±c: NhÃ¢n váº­t lá»‹ch sá»­ Viá»‡t Nam. Chá»§ Ä‘á»: A...   \n",
       "3  LÄ©nh vá»±c: Triá»u Ä‘áº¡i Viá»‡t Nam. Chá»§ Ä‘á»: An DÆ°Æ¡ng...   \n",
       "4  LÄ©nh vá»±c: NhÃ  nÆ°á»›c Viá»‡t Nam. Chá»§ Ä‘á»: NhÃ  nÆ°á»›c ...   \n",
       "\n",
       "                                        display_text  \\\n",
       "0  Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam lÃ  thuáº­t ngá»¯ chá»‰ nhá»¯...   \n",
       "1  Äinh TiÃªn HoÃ ng, tá»©c Äinh Bá»™ LÄ©nh: ngÆ°á»i Ä‘Ã¡nh ...   \n",
       "2  TiÃªu chuáº©n\\n\\n14 vá»‹ Anh hÃ¹ng dÃ¢n tá»™c Viá»‡t Nam ...   \n",
       "3  An DÆ°Æ¡ng VÆ°Æ¡ng (chá»¯ HÃ¡n: å®‰é™½ç‹), tÃªn tháº­t lÃ  Thá»¥...   \n",
       "4  NhÃ  nÆ°á»›c Cá»™ng hÃ²a xÃ£ há»™i chá»§ nghÄ©a Viá»‡t Nam lÃ ...   \n",
       "\n",
       "                                             doc_url  \n",
       "0  https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...  \n",
       "1  https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...  \n",
       "2  https://vi.wikipedia.org/wiki/Anh_h%C3%B9ng_d%...  \n",
       "3  https://vi.wikipedia.org/wiki/An_D%C6%B0%C6%A1...  \n",
       "4  https://vi.wikipedia.org/wiki/Nh%C3%A0_n%C6%B0...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huhu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e56067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ•·ï¸ STARTING TARGETED CRAWL (V2 - SSL BYPASS)...\n",
      "\n",
      "ğŸš€ Campaign: Tu_Lieu_Dang\n",
      "  ğŸ” Inspecting: https://tulieuvankien.dangcongsan.vn/ho-chi-minh-toan-tap\n",
      "    âœ… Connected (Size: 6021 bytes)\n",
      "    â„¹ï¸ TÃ¬m tháº¥y 1 tháº» <a> trong trang.\n",
      "      ğŸ‘€ Sample link: https://tulieuvankien.dangcongsan.vn/\n",
      "    => Lá»c Ä‘Æ°á»£c 0 link phÃ¹ há»£p pattern.\n",
      "  ğŸ‘‰ Thá»­ trang 2: https://tulieuvankien.dangcongsan.vn/ho-chi-minh-toan-tap?page=2\n",
      "  ğŸ” Inspecting: https://tulieuvankien.dangcongsan.vn/ho-chi-minh-toan-tap?page=2\n",
      "    âœ… Connected (Size: 6021 bytes)\n",
      "    â„¹ï¸ TÃ¬m tháº¥y 1 tháº» <a> trong trang.\n",
      "      ğŸ‘€ Sample link: https://tulieuvankien.dangcongsan.vn/\n",
      "    => Lá»c Ä‘Æ°á»£c 0 link phÃ¹ há»£p pattern.\n",
      "  ğŸ” Inspecting: https://tulieuvankien.dangcongsan.vn/van-kien-tu-lieu-ve-dang\n",
      "    âœ… Connected (Size: 6021 bytes)\n",
      "    â„¹ï¸ TÃ¬m tháº¥y 1 tháº» <a> trong trang.\n",
      "      ğŸ‘€ Sample link: https://tulieuvankien.dangcongsan.vn/\n",
      "    => Lá»c Ä‘Æ°á»£c 0 link phÃ¹ há»£p pattern.\n",
      "  ğŸ‘‰ Thá»­ trang 2: https://tulieuvankien.dangcongsan.vn/van-kien-tu-lieu-ve-dang?page=2\n",
      "  ğŸ” Inspecting: https://tulieuvankien.dangcongsan.vn/van-kien-tu-lieu-ve-dang?page=2\n",
      "    âœ… Connected (Size: 6021 bytes)\n",
      "    â„¹ï¸ TÃ¬m tháº¥y 1 tháº» <a> trong trang.\n",
      "      ğŸ‘€ Sample link: https://tulieuvankien.dangcongsan.vn/\n",
      "    => Lá»c Ä‘Æ°á»£c 0 link phÃ¹ há»£p pattern.\n",
      "  => Tá»•ng link thu Ä‘Æ°á»£c cho Tu_Lieu_Dang: 0\n",
      "\n",
      "ğŸš€ Campaign: Di_San_Van_Hoa\n",
      "  ğŸ” Inspecting: http://dsvh.gov.vn/di-tich-quoc-gia-dac-biet-1756\n",
      "    âœ… Connected (Size: 66596 bytes)\n",
      "    â„¹ï¸ TÃ¬m tháº¥y 160 tháº» <a> trong trang.\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/gioi-thieu-15\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/thong-tin-chung-1727\n",
      "    => Lá»c Ä‘Æ°á»£c 63 link phÃ¹ há»£p pattern.\n",
      "  ğŸ‘‰ Thá»­ trang 2: http://dsvh.gov.vn/di-tich-quoc-gia-dac-biet-1756?page=2\n",
      "  ğŸ” Inspecting: http://dsvh.gov.vn/di-tich-quoc-gia-dac-biet-1756?page=2\n",
      "    âœ… Connected (Size: 66596 bytes)\n",
      "    â„¹ï¸ TÃ¬m tháº¥y 160 tháº» <a> trong trang.\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/gioi-thieu-15\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/thong-tin-chung-1727\n",
      "    => Lá»c Ä‘Æ°á»£c 56 link phÃ¹ há»£p pattern.\n",
      "  ğŸ” Inspecting: http://dsvh.gov.vn/di-tich-quoc-gia-1757\n",
      "    âœ… Connected (Size: 66596 bytes)\n",
      "    â„¹ï¸ TÃ¬m tháº¥y 160 tháº» <a> trong trang.\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/gioi-thieu-15\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/thong-tin-chung-1727\n",
      "    => Lá»c Ä‘Æ°á»£c 85 link phÃ¹ há»£p pattern.\n",
      "  ğŸ‘‰ Thá»­ trang 2: http://dsvh.gov.vn/di-tich-quoc-gia-1757?page=2\n",
      "  ğŸ” Inspecting: http://dsvh.gov.vn/di-tich-quoc-gia-1757?page=2\n",
      "    âœ… Connected (Size: 66596 bytes)\n",
      "    â„¹ï¸ TÃ¬m tháº¥y 160 tháº» <a> trong trang.\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/gioi-thieu-15\n",
      "      ğŸ‘€ Sample link: http://dsvh.gov.vn/thong-tin-chung-1727\n",
      "    => Lá»c Ä‘Æ°á»£c 63 link phÃ¹ há»£p pattern.\n",
      "  => Tá»•ng link thu Ä‘Æ°á»£c cho Di_San_Van_Hoa: 65\n",
      "  Downloading content (65 bÃ i)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [12:08<00:00, 11.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… DONE! ÄÃ£ lÆ°u 1 bÃ i viáº¿t cháº¥t lÆ°á»£ng vÃ o targeted_knowledge.parquet\n",
      "\n",
      "ğŸ‘€ VÃ­ dá»¥ dá»¯ liá»‡u:\n",
      "  doc_title                                            doc_url\n",
      "0            http://dsvh.gov.vn/mo-cu-pho-bang-nguyen-sinh-...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import urllib3\n",
    "\n",
    "# [QUAN TRá»ŒNG] Táº¯t cáº£nh bÃ¡o báº£o máº­t (SSL Warning) Ä‘á»ƒ crawl web chÃ­nh phá»§ cÅ©\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# --- Cáº¤U HÃŒNH ---\n",
    "OUTPUT_FILE = \"targeted_knowledge.parquet\"\n",
    "MAX_PAGES = 5\n",
    "\n",
    "CAMPAIGNS = [\n",
    "    {\n",
    "        \"name\": \"Tu_Lieu_Dang\",\n",
    "        \"base_url\": \"https://tulieuvankien.dangcongsan.vn\",\n",
    "        \"seed_urls\": [\n",
    "            \"https://tulieuvankien.dangcongsan.vn/ho-chi-minh-toan-tap\",\n",
    "            \"https://tulieuvankien.dangcongsan.vn/van-kien-tu-lieu-ve-dang\"\n",
    "        ],\n",
    "        # Regex linh hoáº¡t hÆ¡n: Cháº¥p nháº­n má»i link con chá»©a tá»« khÃ³a\n",
    "        \"link_pattern\": r\"(ho-chi-minh-toan-tap|van-kien-tu-lieu-ve-dang)/\" \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Di_San_Van_Hoa\",\n",
    "        \"base_url\": \"http://dsvh.gov.vn\", # Äá»•i sang trang Cá»¥c Di sáº£n vÄƒn hÃ³a (chuyÃªn sÃ¢u hÆ¡n)\n",
    "        \"seed_urls\": [\n",
    "            \"http://dsvh.gov.vn/di-tich-quoc-gia-dac-biet-1756\",\n",
    "            \"http://dsvh.gov.vn/di-tich-quoc-gia-1757\"\n",
    "        ],\n",
    "        \"link_pattern\": r\"dsvh.gov.vn/.*\" \n",
    "    }\n",
    "]\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"Gá»­i request vá»›i cháº¿ Ä‘á»™ 'giáº£ danh' trÃ¬nh duyá»‡t tháº­t\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "            'Referer': 'https://google.com'\n",
    "        }\n",
    "        # [QUAN TRá»ŒNG] verify=False Ä‘á»ƒ bá» qua lá»—i SSL\n",
    "        resp = requests.get(url, headers=headers, timeout=20, verify=False)\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            # In ra Ä‘á»™ dÃ i ná»™i dung Ä‘á»ƒ debug\n",
    "            print(f\"    âœ… Connected (Size: {len(resp.content)} bytes)\")\n",
    "            return BeautifulSoup(resp.content, 'html.parser')\n",
    "        else:\n",
    "            print(f\"    âŒ Status Code: {resp.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    âš ï¸ Connect Error: {url} - {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_links_from_list(campaign, url):\n",
    "    links = set()\n",
    "    print(f\"  ğŸ” Inspecting: {url}\")\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    if not soup: \n",
    "        print(\"    âŒ KhÃ´ng láº¥y Ä‘Æ°á»£c HTML.\")\n",
    "        return []\n",
    "\n",
    "    found_count = 0\n",
    "    match_count = 0\n",
    "    \n",
    "    # TÃ¬m táº¥t cáº£ tháº» a\n",
    "    all_tags = soup.find_all('a', href=True)\n",
    "    print(f\"    â„¹ï¸ TÃ¬m tháº¥y {len(all_tags)} tháº» <a> trong trang.\")\n",
    "\n",
    "    for a in all_tags:\n",
    "        href = a['href']\n",
    "        full_url = urljoin(campaign['base_url'], href)\n",
    "        \n",
    "        # [DEBUG] In thá»­ 3 link Ä‘áº§u tiÃªn Ä‘á»ƒ xem format\n",
    "        if found_count < 3:\n",
    "            print(f\"      ğŸ‘€ Sample link: {full_url}\")\n",
    "        found_count += 1\n",
    "\n",
    "        # Lá»c link\n",
    "        if re.search(campaign['link_pattern'], full_url):\n",
    "            if any(x in full_url for x in ['print', 'javascript', '#', 'download', 'jpg', 'png']):\n",
    "                continue\n",
    "            \n",
    "            # Chá»‰ láº¥y link chi tiáº¿t (thÆ°á»ng dÃ i hÆ¡n link danh má»¥c)\n",
    "            if len(full_url) > len(url) + 5: \n",
    "                links.add(full_url)\n",
    "                match_count += 1\n",
    "    \n",
    "    print(f\"    => Lá»c Ä‘Æ°á»£c {match_count} link phÃ¹ há»£p pattern.\")\n",
    "    return list(links)\n",
    "\n",
    "def crawl_article(url, category):\n",
    "    try:\n",
    "        # Trafilatura cÅ©ng cáº§n config Ä‘á»ƒ bá» qua SSL\n",
    "        config = trafilatura.settings.use_config()\n",
    "        config.set(\"DEFAULT\", \"USER_AGENT\", \"Mozilla/5.0\")\n",
    "        \n",
    "        # Download thá»§ cÃ´ng báº±ng requests Ä‘á»ƒ bypass SSL, sau Ä‘Ã³ Ä‘Æ°a vÃ o trafilatura\n",
    "        resp = requests.get(url, verify=False, timeout=10)\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            data = trafilatura.extract(resp.text, output_format=\"json\", include_comments=False)\n",
    "            if data:\n",
    "                import json\n",
    "                j = json.loads(data)\n",
    "                title = j.get('title', '')\n",
    "                text = j.get('text', '')\n",
    "                \n",
    "                # Validation: Bá» qua bÃ i quÃ¡ ngáº¯n (lá»—i parse)\n",
    "                if len(text) < 100: return None\n",
    "                \n",
    "                return {\n",
    "                    \"doc_title\": title,\n",
    "                    \"doc_url\": url,\n",
    "                    \"doc_category\": category,\n",
    "                    \"original_text\": text,\n",
    "                    \"vector_text\": f\"Nguá»“n: {category}\\nTiÃªu Ä‘á»: {title}\\nNá»™i dung:\\n{text}\",\n",
    "                    \"display_text\": f\"Nguá»“n: {category}\\nTiÃªu Ä‘á»: {title}\\nNá»™i dung:\\n{text}\",\n",
    "                    \"chunk_id\": f\"crawl_{int(time.time())}_{random.randint(1000,9999)}\"\n",
    "                }\n",
    "    except: pass\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸ•·ï¸ STARTING TARGETED CRAWL (V2 - SSL BYPASS)...\")\n",
    "    all_data = []\n",
    "    \n",
    "    for camp in CAMPAIGNS:\n",
    "        print(f\"\\nğŸš€ Campaign: {camp['name']}\")\n",
    "        campaign_links = set()\n",
    "        \n",
    "        # 1. QuÃ©t danh sÃ¡ch\n",
    "        for seed in camp['seed_urls']:\n",
    "            links = extract_links_from_list(camp, seed)\n",
    "            campaign_links.update(links)\n",
    "            \n",
    "            # Pagination cÆ¡ báº£n (Trang 2)\n",
    "            # ThÆ°á»ng web VN dÃ¹ng ?page=2 hoáº·c /p2\n",
    "            next_page = f\"{seed}?page=2\"\n",
    "            print(f\"  ğŸ‘‰ Thá»­ trang 2: {next_page}\")\n",
    "            links_p2 = extract_links_from_list(camp, next_page)\n",
    "            campaign_links.update(links_p2)\n",
    "\n",
    "        unique_links = list(campaign_links)\n",
    "        print(f\"  => Tá»•ng link thu Ä‘Æ°á»£c cho {camp['name']}: {len(unique_links)}\")\n",
    "        \n",
    "        # 2. Crawl chi tiáº¿t\n",
    "        if unique_links:\n",
    "            print(f\"  Downloading content ({len(unique_links)} bÃ i)...\")\n",
    "            for url in tqdm(unique_links):\n",
    "                row = crawl_article(url, camp['name'])\n",
    "                if row:\n",
    "                    all_data.append(row)\n",
    "                time.sleep(random.uniform(0.5, 1.0))\n",
    "\n",
    "    # 3. LÆ°u\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        # Loáº¡i bá» trÃ¹ng láº·p ná»™i dung\n",
    "        df.drop_duplicates(subset=['doc_title'], inplace=True)\n",
    "        \n",
    "        df.to_parquet(OUTPUT_FILE, index=False)\n",
    "        print(f\"\\nâœ… DONE! ÄÃ£ lÆ°u {len(df)} bÃ i viáº¿t cháº¥t lÆ°á»£ng vÃ o {OUTPUT_FILE}\")\n",
    "        \n",
    "        # Preview\n",
    "        print(\"\\nğŸ‘€ VÃ­ dá»¥ dá»¯ liá»‡u:\")\n",
    "        print(df[['doc_title', 'doc_url']].head())\n",
    "    else:\n",
    "        print(\"\\nâŒ Váº«n khÃ´ng láº¥y Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o. HÃ£y copy log trÃªn Ä‘á»ƒ debug.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "772fad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Báº®T Äáº¦U CRAWL (FIXED VERSION)...\n",
      "\n",
      "ğŸ“‚ Äang xá»­ lÃ½ nguá»“n: Nguoi_Ke_Su\n",
      "   -> Äang quÃ©t 150 trang danh má»¥c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [03:41<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   => TÃ¬m tháº¥y 453 link bÃ i viáº¿t.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 453/453 [11:33<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âŒ KhÃ´ng táº£i Ä‘Æ°á»£c ná»™i dung bÃ i nÃ o.\n",
      "\n",
      "ğŸ“‚ Äang xá»­ lÃ½ nguá»“n: Tuyen_Giao\n",
      "   -> Äang quÃ©t 150 trang danh má»¥c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 204.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   => TÃ¬m tháº¥y 0 link bÃ i viáº¿t.\n",
      "   âš ï¸ KhÃ´ng tÃ¬m tháº¥y link nÃ o. Bá» qua nguá»“n nÃ y.\n",
      "\n",
      "ğŸ“‚ Äang xá»­ lÃ½ nguá»“n: Van_Ban_Chinh_Phu\n",
      "   -> Äang quÃ©t 50 trang danh má»¥c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:52<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   => TÃ¬m tháº¥y 0 link bÃ i viáº¿t.\n",
      "   âš ï¸ KhÃ´ng tÃ¬m tháº¥y link nÃ o. Bá» qua nguá»“n nÃ y.\n",
      "\n",
      "ğŸ‰ Tá»”NG Káº¾T: ÄÃ£ thu tháº­p 0 bÃ i viáº¿t má»›i.\n",
      "ğŸ‘‰ HÃ£y thÃªm cÃ¡c file 'crawl_*.parquet' vÃ o build_bm25_incremental.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import urllib3\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "# Táº¯t cáº£nh bÃ¡o SSL\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# --- Cáº¤U HÃŒNH ---\n",
    "MAX_PAGES = 50  # Sá»‘ trang má»—i má»¥c\n",
    "\n",
    "# Äá»‹nh nghÄ©a láº¡i Campaign vá»›i nguá»“n dá»… thá»Ÿ hÆ¡n\n",
    "CAMPAIGNS = [\n",
    "    # 1. Lá»ŠCH Sá»¬ (ÄÃ£ ngon, giá»¯ nguyÃªn)\n",
    "    {\n",
    "        \"name\": \"Nguoi_Ke_Su\",\n",
    "        \"base_url\": \"https://nguoikesu.com\",\n",
    "        \"seed_urls\": [\n",
    "            \"https://nguoikesu.com/nhan-vat\",\n",
    "            \"https://nguoikesu.com/dong-lich-su\",\n",
    "            \"https://nguoikesu.com/di-tich-lich-su\"\n",
    "        ],\n",
    "        \"link_pattern\": r\"nguoikesu.com/(nhan-vat|dong-lich-su|di-tich-lich-su)/\",\n",
    "        \"page_param\": \"?start={}\", # start=0, 5, 10\n",
    "        \"step\": 5\n",
    "    },\n",
    "    \n",
    "    # 2. CHÃNH TRá»Š (Cáº­p nháº­t Seed URL)\n",
    "    {\n",
    "        \"name\": \"Tuyen_Giao\",\n",
    "        \"base_url\": \"https://tuyengiao.vn\",\n",
    "        \"seed_urls\": [\n",
    "            \"https://tuyengiao.vn/hoc-tap-va-lam-theo-loi-bac\",\n",
    "            \"https://tuyengiao.vn/bao-ve-nen-tang-tu-tuong-cua-dang\",\n",
    "            \"https://tuyengiao.vn/van-hoa-xa-hoi\"\n",
    "        ],\n",
    "        # Láº¥y táº¥t cáº£ link bÃ i viáº¿t (thÆ°á»ng cÃ³ .html hoáº·c khÃ´ng cÃ³ ext nhÆ°ng náº±m trong subfolder)\n",
    "        \"link_pattern\": r\"tuyengiao.vn/.*\", \n",
    "        \"page_param\": \"?page={}\", \n",
    "        \"step\": 1\n",
    "    },\n",
    "\n",
    "    # 3. LUáº¬T (Thay tháº¿ VBPL báº±ng ChinhPhu.vn - Dá»… crawl hÆ¡n)\n",
    "    {\n",
    "        \"name\": \"Van_Ban_Chinh_Phu\",\n",
    "        \"base_url\": \"https://vanban.chinhphu.vn\",\n",
    "        \"seed_urls\": [\n",
    "            # Trang tÃ¬m kiáº¿m vÄƒn báº£n má»›i\n",
    "            \"https://vanban.chinhphu.vn/?pageid=27160\", \n",
    "        ],\n",
    "        # Link chi tiáº¿t thÆ°á»ng chá»©a 'van-ban'\n",
    "        \"link_pattern\": r\"vanban.chinhphu.vn/.*van-ban\", \n",
    "        \"page_param\": \"&page={}\", \n",
    "        \"step\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        }\n",
    "        # verify=False Ä‘á»ƒ bypass lá»—i SSL cá»§a web chÃ­nh phá»§\n",
    "        resp = requests.get(url, headers=headers, timeout=20, verify=False)\n",
    "        if resp.status_code == 200:\n",
    "            return BeautifulSoup(resp.content, 'html.parser')\n",
    "    except Exception as e:\n",
    "        # print(f\"Lá»—i connect {url}: {e}\")\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def generate_urls(camp):\n",
    "    urls = []\n",
    "    for seed in camp['seed_urls']:\n",
    "        urls.append(seed)\n",
    "        for i in range(2, MAX_PAGES + 1):\n",
    "            # TÃ­nh toÃ¡n tham sá»‘ phÃ¢n trang\n",
    "            val = i if camp.get('step', 1) == 1 else (i-1) * camp['step']\n",
    "            \n",
    "            if \"{}\" in camp['page_param']:\n",
    "                param = camp['page_param'].format(val)\n",
    "            else:\n",
    "                param = camp['page_param'] + str(val)\n",
    "                \n",
    "            if \"?\" in seed:\n",
    "                full = f\"{seed}&{param.replace('?', '')}\"\n",
    "            else:\n",
    "                full = f\"{seed}{param}\"\n",
    "            urls.append(full)\n",
    "    return urls\n",
    "\n",
    "def extract_links(camp, url):\n",
    "    soup = get_soup(url)\n",
    "    if not soup: return []\n",
    "    \n",
    "    links = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        full = urljoin(camp['base_url'], href)\n",
    "        \n",
    "        # Lá»c rÃ¡c\n",
    "        if any(x in full for x in ['#', 'jpg', 'pdf', 'doc', 'download', 'print', 'javascript', 'mailto']):\n",
    "            continue\n",
    "            \n",
    "        if re.search(camp['link_pattern'], full):\n",
    "            # Vá»›i chinhphu.vn, link pháº£i chá»©a 'handle' hoáº·c 'chi-tiet'\n",
    "            if camp['name'] == \"Van_Ban_Chinh_Phu\":\n",
    "                if \"chi-tiet\" in full or \"handle\" in full:\n",
    "                    links.add(full)\n",
    "            else:\n",
    "                # CÃ¡c trang khÃ¡c: Ä‘á»™ dÃ i URL pháº£i Ä‘á»§ lá»›n (trÃ¡nh link trang chá»§)\n",
    "                if len(full) > len(camp['base_url']) + 10:\n",
    "                    links.add(full)\n",
    "    return list(links)\n",
    "\n",
    "def crawl_content(url, category):\n",
    "    try:\n",
    "        # DÃ¹ng requests táº£i trÆ°á»›c Ä‘á»ƒ bypass SSL\n",
    "        resp = requests.get(url, verify=False, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        if resp.status_code != 200: return None\n",
    "        \n",
    "        # Parse\n",
    "        data = trafilatura.extract(resp.text, output_format=\"json\", include_comments=False)\n",
    "        if data:\n",
    "            import json\n",
    "            j = json.loads(data)\n",
    "            title = j.get('title', '').strip()\n",
    "            text = j.get('text', '').strip()\n",
    "            \n",
    "            if len(text) < 200: return None\n",
    "            \n",
    "            doc_hash = hashlib.md5(url.encode()).hexdigest()[:10]\n",
    "            \n",
    "            return {\n",
    "                \"chunk_id\": f\"{category}_{doc_hash}\",\n",
    "                \"doc_title\": title,\n",
    "                \"doc_url\": url,\n",
    "                \"doc_category\": category,\n",
    "                \"vector_text\": f\"Nguá»“n: {category}\\nTiÃªu Ä‘á»: {title}\\nNá»™i dung:\\n{text}\",\n",
    "                \"display_text\": f\"Title: {title}\\nSource: {url}\\n\\n{text}\"\n",
    "            }\n",
    "    except: pass\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸš€ Báº®T Äáº¦U CRAWL (FIXED VERSION)...\")\n",
    "    \n",
    "    total_collected = 0\n",
    "    \n",
    "    for camp in CAMPAIGNS:\n",
    "        print(f\"\\nğŸ“‚ Äang xá»­ lÃ½ nguá»“n: {camp['name']}\")\n",
    "        list_urls = generate_urls(camp)\n",
    "        \n",
    "        # 1. QuÃ©t Link\n",
    "        article_links = set()\n",
    "        print(f\"   -> Äang quÃ©t {len(list_urls)} trang danh má»¥c...\")\n",
    "        for l_url in tqdm(list_urls, desc=\"Scanning\"):\n",
    "            found = extract_links(camp, l_url)\n",
    "            article_links.update(found)\n",
    "        \n",
    "        print(f\"   => TÃ¬m tháº¥y {len(article_links)} link bÃ i viáº¿t.\")\n",
    "        if len(article_links) == 0:\n",
    "            print(\"   âš ï¸ KhÃ´ng tÃ¬m tháº¥y link nÃ o. Bá» qua nguá»“n nÃ y.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Táº£i ná»™i dung\n",
    "        rows = []\n",
    "        for url in tqdm(list(article_links), desc=\"Downloading\"):\n",
    "            row = crawl_content(url, camp['name'])\n",
    "            if row:\n",
    "                rows.append(row)\n",
    "            # Sleep nháº¹ Ä‘á»ƒ khÃ´ng bá»‹ ban\n",
    "            time.sleep(0.2) \n",
    "            \n",
    "        # 3. LÆ¯U NGAY Láº¬P Tá»¨C (Save Checkpoint)\n",
    "        if rows:\n",
    "            filename = f\"crawl_{camp['name']}.parquet\"\n",
    "            df = pd.DataFrame(rows)\n",
    "            df.drop_duplicates(subset=['doc_url'], inplace=True)\n",
    "            df.to_parquet(filename, index=False)\n",
    "            print(f\"   ğŸ’¾ ÄÃ£ lÆ°u {len(df)} bÃ i vÃ o '{filename}'\")\n",
    "            total_collected += len(df)\n",
    "        else:\n",
    "            print(\"   âŒ KhÃ´ng táº£i Ä‘Æ°á»£c ná»™i dung bÃ i nÃ o.\")\n",
    "\n",
    "    print(f\"\\nğŸ‰ Tá»”NG Káº¾T: ÄÃ£ thu tháº­p {total_collected} bÃ i viáº¿t má»›i.\")\n",
    "    print(\"ğŸ‘‰ HÃ£y thÃªm cÃ¡c file 'crawl_*.parquet' vÃ o build_bm25_incremental.py\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43422074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "i1 = pd.read_parquet(path=\"sgk_lich_su_dia_ly.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c088569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a2a19a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**TÃ i liá»‡u Lá»‹ch sá»­ 10 - THI247.com**\\n*MÃ´n: Lá»‹ch Sá»­ - Lá»›p 10*\\n\\nTÃ i liá»‡u Lá»‹ch sá»­ 10\\nTuyá»ƒn táº­p cÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 10 hay nháº¥t vá»›i Ä‘áº§y Ä‘á»§ cÃ¡c chá»§ Ä‘á» bÃ¡m sÃ¡t chÆ°Æ¡ng trÃ¬nh sÃ¡ch giÃ¡o khoa Lá»‹ch sá»­ 10 hiá»‡n hÃ nh. CÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 10 Ä‘Æ°á»£c biÃªn soáº¡n bá»Ÿi quÃ½ tháº§y, cÃ´, cÃ¡c tÃ¡c giáº£ nhiá»u kinh nghiá»‡m sáº½ giÃºp cÃ¡c em há»c sinh khá»‘i 10 tÃ¬m hiá»ƒu cÃ¡c kiáº¿n thá»©c mÃ´n Lá»‹ch sá»­ 10.\\nCÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 10 sáº½ Ä‘Æ°á»£c THI247.com cáº­p nháº­t thÆ°á»ng xuyÃªn dá»±a vÃ o nguá»“n sÆ°u táº§m trÃªn cÃ¡c trang máº¡ng xÃ£ há»™i vÃ  cÃ¡c trang web khÃ¡c trÃªn internet.\\nQuÃ½ tháº§y, cÃ´ giÃ¡o vÃ  cÃ¡c em há»c sinh khá»‘i 10 cÃ³ thá»ƒ xem vÃ  táº£i xuá»‘ng miá»…n phÃ­ cÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 10 Ä‘Æ°á»£c chia sáº» trÃªn THI247.com.\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 1 Lá»‹ch sá»­ 10 nÄƒm 2024 â€“ 2025 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c kÃ¬ 2 Lá»‹ch sá»­ 10 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 2 Lá»‹ch sá»­ 10 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c ká»³ 1 Lá»‹ch sá»­ 10 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT YÃªn HÃ²a â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng Lá»‹ch sá»­ 10 giá»¯a ká»³ 1 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nTrá»n bá»™ lÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m mÃ´n Lá»‹ch sá»­ 10\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m phong trÃ o cÃ´ng nhÃ¢n (tá»« Ä‘áº§u tháº¿ ká»‰ XIX Ä‘áº¿n Ä‘áº§u tháº¿ ká»‰ XX)\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m cÃ¡c nÆ°á»›c Ã‚u â€“ MÄ© (tá»« Ä‘áº§u tháº¿ ká»‰ XIX Ä‘áº¿n Ä‘áº§u tháº¿ ká»‰ XX)\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m cÃ¡c cuá»™c cÃ¡ch máº¡ng tÆ° sáº£n (tá»« giá»¯a tháº¿ ká»‰ XVI Ä‘áº¿n cuá»‘i tháº¿ ká»‰ XVIII)\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m quÃ¡ trÃ¬nh dá»±ng nÆ°á»›c, giá»¯ nÆ°á»›c vÃ  truyá»n thá»‘ng yÃªu nÆ°á»›c cá»§a dÃ¢n tá»™c Viá»‡t Nam thá»i phong kiáº¿n\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Viá»‡t Nam á»Ÿ ná»­a Ä‘áº§u tháº¿ ká»‰ XIX\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Viá»‡t Nam tá»« tháº¿ ká»‰ XVI Ä‘áº¿n XVIII',\n",
       " '**TÃ i liá»‡u Lá»‹ch sá»­ 11 - THI247.com**\\n*MÃ´n: Lá»‹ch Sá»­ - Lá»›p 11*\\n\\nTÃ i liá»‡u Lá»‹ch sá»­ 11\\nTuyá»ƒn táº­p cÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 11 hay nháº¥t vá»›i Ä‘áº§y Ä‘á»§ cÃ¡c chá»§ Ä‘á» bÃ¡m sÃ¡t chÆ°Æ¡ng trÃ¬nh sÃ¡ch giÃ¡o khoa Lá»‹ch sá»­ 11 hiá»‡n hÃ nh. CÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 11 Ä‘Æ°á»£c biÃªn soáº¡n bá»Ÿi quÃ½ tháº§y, cÃ´, cÃ¡c tÃ¡c giáº£ nhiá»u kinh nghiá»‡m sáº½ giÃºp cÃ¡c em há»c sinh khá»‘i 11 tÃ¬m hiá»ƒu cÃ¡c kiáº¿n thá»©c mÃ´n Lá»‹ch sá»­ 11.\\nCÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 11 sáº½ Ä‘Æ°á»£c THI247.com cáº­p nháº­t thÆ°á»ng xuyÃªn dá»±a vÃ o nguá»“n sÆ°u táº§m trÃªn cÃ¡c trang máº¡ng xÃ£ há»™i vÃ  cÃ¡c trang web khÃ¡c trÃªn internet.\\nQuÃ½ tháº§y, cÃ´ giÃ¡o vÃ  cÃ¡c em há»c sinh khá»‘i 11 cÃ³ thá»ƒ xem vÃ  táº£i xuá»‘ng miá»…n phÃ­ cÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 11 Ä‘Æ°á»£c chia sáº» trÃªn THI247.com.\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 1 Lá»‹ch sá»­ 11 nÄƒm 2024 â€“ 2025 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c kÃ¬ 2 Lá»‹ch sá»­ 11 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 2 Lá»‹ch sá»­ 11 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c ká»³ 1 Lá»‹ch sá»­ 11 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT YÃªn HÃ²a â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng Lá»‹ch sá»­ 11 giá»¯a ká»³ 1 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nTrá»n bá»™ lÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m mÃ´n Lá»‹ch sá»­ 11\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Viá»‡t Nam trong nhá»¯ng nÄƒm chiáº¿n tranh tháº¿ giá»›i thá»© nháº¥t (1914 â€“ 1918)\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Viá»‡t Nam tá»« Ä‘áº§u tháº¿ ká»‰ XX Ä‘áº¿n chiáº¿n tranh tháº¿ giá»›i thá»© nháº¥t (1914)\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m phong trÃ o yÃªu nÆ°á»›c chá»‘ng PhÃ¡p cá»§a nhÃ¢n dÃ¢n Viá»‡t Nam trong nhá»¯ng nÄƒm cuá»‘i tháº¿ ká»‰ XIX\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m cuá»™c khÃ¡ng chiáº¿n chá»‘ng PhÃ¡p cá»§a nhÃ¢n dÃ¢n Viá»‡t Nam tá»« 1958 â€“ 1884\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m chiáº¿n tranh tháº¿ giá»›i thá»© hai (1939 â€“ 1945)\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m cÃ¡c nÆ°á»›c chÃ¢u Ã giá»¯a hai cuá»™c chiáº¿n tranh tháº¿ giá»›i (1917 â€“ 1939)',\n",
       " '**TÃ i liá»‡u Lá»‹ch sá»­ 12 - THI247.com**\\n*MÃ´n: Lá»‹ch Sá»­ - Lá»›p 12*\\n\\nTÃ i liá»‡u Lá»‹ch sá»­ 12\\nTuyá»ƒn táº­p cÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 12 hay nháº¥t vá»›i Ä‘áº§y Ä‘á»§ cÃ¡c chá»§ Ä‘á» bÃ¡m sÃ¡t chÆ°Æ¡ng trÃ¬nh sÃ¡ch giÃ¡o khoa Lá»‹ch sá»­ 12 hiá»‡n hÃ nh. CÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 12 Ä‘Æ°á»£c biÃªn soáº¡n bá»Ÿi quÃ½ tháº§y, cÃ´, cÃ¡c tÃ¡c giáº£ nhiá»u kinh nghiá»‡m sáº½ giÃºp cÃ¡c em há»c sinh khá»‘i 12 tÃ¬m hiá»ƒu cÃ¡c kiáº¿n thá»©c mÃ´n Lá»‹ch sá»­ 12.\\nCÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 12 sáº½ Ä‘Æ°á»£c THI247.com cáº­p nháº­t thÆ°á»ng xuyÃªn dá»±a vÃ o nguá»“n sÆ°u táº§m trÃªn cÃ¡c trang máº¡ng xÃ£ há»™i vÃ  cÃ¡c trang web khÃ¡c trÃªn internet.\\nQuÃ½ tháº§y, cÃ´ giÃ¡o vÃ  cÃ¡c em há»c sinh khá»‘i 12 cÃ³ thá»ƒ xem vÃ  táº£i xuá»‘ng miá»…n phÃ­ cÃ¡c tÃ i liá»‡u Lá»‹ch sá»­ 12 Ä‘Æ°á»£c chia sáº» trÃªn THI247.com.\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 1 Lá»‹ch sá»­ 12 nÄƒm 2024 â€“ 2025 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c kÃ¬ 2 Lá»‹ch sá»­ 12 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 2 Lá»‹ch sá»­ 12 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c ká»³ 1 Lá»‹ch sá»­ 12 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT YÃªn HÃ²a â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng Lá»‹ch sá»­ 12 giá»¯a ká»³ 1 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nTrá»n bá»™ lÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m mÃ´n Lá»‹ch sá»­ 12\\n1260 cÃ¢u há»i tráº¯c nghiá»‡m Lá»‹ch sá»­ 12 â€“ TrÆ°Æ¡ng Ngá»c ThÆ¡i\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Viá»‡t Nam trÃªn con Ä‘Æ°á»ng Ä‘á»•i má»›i, Ä‘i lÃªn chá»§ nghÄ©a xÃ£ há»™i (1986 â€“ 2000)\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Viá»‡t Nam trong nhá»¯ng nÄƒm Ä‘áº§u sau tháº¯ng lá»£i cá»§a cuá»™c khÃ¡ng chiáº¿n chá»‘ng MÄ© (1975 â€“ 1986)\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m khÃ´i phá»¥c vÃ  phÃ¡t triá»ƒn kinh táº¿ â€“ xÃ£ há»™i á»Ÿ miá»n Báº¯c, giáº£i phÃ³ng hoÃ n toÃ n miá»n Nam (1973 â€“ 1975)\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m nhÃ¢n dÃ¢n hai miá»n Nam â€“ Báº¯c trá»±c tiáº¿p Ä‘Æ°Æ¡ng Ä‘áº§u vá»›i Ä‘áº¿ quá»‘c MÄ© xÃ¢m lÆ°á»£c (1965 â€“ 1973)\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m xÃ¢y dá»±ng chá»§ nghÄ©a xÃ£ há»™i á»Ÿ miá»n Báº¯c, Ä‘áº¥u tranh chá»‘ng Ä‘áº¿ quá»‘c MÄ© vÃ  chÃ­nh quyá»n SÃ i GÃ²n á»Ÿ miá»n Nam (1954 â€“ 1965)',\n",
       " '**TÃ i liá»‡u Äá»‹a lÃ½ 10 - THI247.com**\\n*MÃ´n: Äá»‹a LÃ½ - Lá»›p 10*\\n\\nTÃ i liá»‡u Äá»‹a lÃ½ 10\\nTuyá»ƒn táº­p cÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 10 hay nháº¥t vá»›i Ä‘áº§y Ä‘á»§ cÃ¡c chá»§ Ä‘á» bÃ¡m sÃ¡t chÆ°Æ¡ng trÃ¬nh sÃ¡ch giÃ¡o khoa Äá»‹a lÃ½ 10 hiá»‡n hÃ nh. CÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 10 Ä‘Æ°á»£c biÃªn soáº¡n bá»Ÿi quÃ½ tháº§y, cÃ´, cÃ¡c tÃ¡c giáº£ nhiá»u kinh nghiá»‡m sáº½ giÃºp cÃ¡c em há»c sinh khá»‘i 10 tÃ¬m hiá»ƒu cÃ¡c kiáº¿n thá»©c mÃ´n Äá»‹a lÃ½ 10.\\nCÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 10 sáº½ Ä‘Æ°á»£c THI247.com cáº­p nháº­t thÆ°á»ng xuyÃªn dá»±a vÃ o nguá»“n sÆ°u táº§m trÃªn cÃ¡c trang máº¡ng xÃ£ há»™i vÃ  cÃ¡c trang web khÃ¡c trÃªn internet.\\nQuÃ½ tháº§y, cÃ´ giÃ¡o vÃ  cÃ¡c em há»c sinh khá»‘i 10 cÃ³ thá»ƒ xem vÃ  táº£i xuá»‘ng miá»…n phÃ­ cÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 10 Ä‘Æ°á»£c chia sáº» trÃªn THI247.com.\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 1 Äá»‹a lÃ­ 10 nÄƒm 2024 â€“ 2025 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c kÃ¬ 2 Äá»‹a lÃ­ 10 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 2 Äá»‹a lÃ­ 10 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c ká»³ 1 Äá»‹a lÃ­ 10 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT YÃªn HÃ²a â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng Äá»‹a lÃ­ 10 giá»¯a ká»³ 1 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nTrá»n bá»™ lÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m mÃ´n Äá»‹a lÃ½ 10\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m mÃ´i trÆ°á»ng vÃ  sá»± phÃ¡t triá»ƒn bá»n vá»¯ng\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Ä‘á»‹a lÃ­ dá»‹ch vá»¥\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Ä‘á»‹a lÃ­ cÃ´ng nghiá»‡p\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Ä‘á»‹a lÃ­ nÃ´ng nghiá»‡p\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m cÆ¡ cáº¥u ná»n kinh táº¿\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Ä‘á»‹a lÃ­ dÃ¢n cÆ°',\n",
       " '**TÃ i liá»‡u Äá»‹a lÃ½ 11 - THI247.com**\\n*MÃ´n: Äá»‹a LÃ½ - Lá»›p 11*\\n\\nTÃ i liá»‡u Äá»‹a lÃ½ 11\\nTuyá»ƒn táº­p cÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 11 hay nháº¥t vá»›i Ä‘áº§y Ä‘á»§ cÃ¡c chá»§ Ä‘á» bÃ¡m sÃ¡t chÆ°Æ¡ng trÃ¬nh sÃ¡ch giÃ¡o khoa Äá»‹a lÃ½ 11 hiá»‡n hÃ nh. CÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 11 Ä‘Æ°á»£c biÃªn soáº¡n bá»Ÿi quÃ½ tháº§y, cÃ´, cÃ¡c tÃ¡c giáº£ nhiá»u kinh nghiá»‡m sáº½ giÃºp cÃ¡c em há»c sinh khá»‘i 11 tÃ¬m hiá»ƒu cÃ¡c kiáº¿n thá»©c mÃ´n Äá»‹a lÃ½ 11.\\nCÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 11 sáº½ Ä‘Æ°á»£c THI247.com cáº­p nháº­t thÆ°á»ng xuyÃªn dá»±a vÃ o nguá»“n sÆ°u táº§m trÃªn cÃ¡c trang máº¡ng xÃ£ há»™i vÃ  cÃ¡c trang web khÃ¡c trÃªn internet.\\nQuÃ½ tháº§y, cÃ´ giÃ¡o vÃ  cÃ¡c em há»c sinh khá»‘i 11 cÃ³ thá»ƒ xem vÃ  táº£i xuá»‘ng miá»…n phÃ­ cÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 11 Ä‘Æ°á»£c chia sáº» trÃªn THI247.com.\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 1 Äá»‹a lÃ­ 11 nÄƒm 2024 â€“ 2025 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c kÃ¬ 2 Äá»‹a lÃ­ 11 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 2 Äá»‹a lÃ­ 11 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c ká»³ 1 Äá»‹a lÃ­ 11 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT YÃªn HÃ²a â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng Äá»‹a lÃ­ 11 giá»¯a ká»³ 1 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nTrá»n bá»™ lÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m mÃ´n Äá»‹a lÃ½ 11\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m dÃ¢n cÆ° Ã”-xtrÃ¢y-li-a\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m khu vá»±c ÄÃ´ng Nam Ã\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Cá»™ng HÃ²a NhÃ¢n DÃ¢n Trung Hoa (Trung Quá»‘c)\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m Nháº­t Báº£n\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m LiÃªn bang Nga\\nLÃ½ thuyáº¿t vÃ  cÃ¢u há»i tráº¯c nghiá»‡m LiÃªn minh chÃ¢u Ã‚u (EU)',\n",
       " '**TÃ i liá»‡u Äá»‹a lÃ½ 12 - THI247.com**\\n*MÃ´n: Äá»‹a LÃ½ - Lá»›p 12*\\n\\nTÃ i liá»‡u Äá»‹a lÃ½ 12\\nTuyá»ƒn táº­p cÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 12 hay nháº¥t vá»›i Ä‘áº§y Ä‘á»§ cÃ¡c chá»§ Ä‘á» bÃ¡m sÃ¡t chÆ°Æ¡ng trÃ¬nh sÃ¡ch giÃ¡o khoa Äá»‹a lÃ½ 12 hiá»‡n hÃ nh. CÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 12 Ä‘Æ°á»£c biÃªn soáº¡n bá»Ÿi quÃ½ tháº§y, cÃ´, cÃ¡c tÃ¡c giáº£ nhiá»u kinh nghiá»‡m sáº½ giÃºp cÃ¡c em há»c sinh khá»‘i 12 tÃ¬m hiá»ƒu cÃ¡c kiáº¿n thá»©c mÃ´n Äá»‹a lÃ½ 12.\\nCÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 12 sáº½ Ä‘Æ°á»£c THI247.com cáº­p nháº­t thÆ°á»ng xuyÃªn dá»±a vÃ o nguá»“n sÆ°u táº§m trÃªn cÃ¡c trang máº¡ng xÃ£ há»™i vÃ  cÃ¡c trang web khÃ¡c trÃªn internet.\\nQuÃ½ tháº§y, cÃ´ giÃ¡o vÃ  cÃ¡c em há»c sinh khá»‘i 12 cÃ³ thá»ƒ xem vÃ  táº£i xuá»‘ng miá»…n phÃ­ cÃ¡c tÃ i liá»‡u Äá»‹a lÃ½ 12 Ä‘Æ°á»£c chia sáº» trÃªn THI247.com.\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 1 Äá»‹a lÃ­ 12 nÄƒm 2024 â€“ 2025 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c kÃ¬ 2 Äá»‹a lÃ­ 12 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng giá»¯a ká»³ 2 Äá»‹a lÃ­ 12 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng há»c ká»³ 1 Äá»‹a lÃ­ 12 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT YÃªn HÃ²a â€“ HÃ  Ná»™i\\nÄá» cÆ°Æ¡ng Äá»‹a lÃ­ 12 giá»¯a ká»³ 1 nÄƒm 2023 â€“ 2024 trÆ°á»ng THPT HoÃ ng VÄƒn Thá»¥ â€“ HÃ  Ná»™i\\nBÃ i táº­p mÃ´n Äá»‹a lÃ­ 12 chá»§ Ä‘á» báº£ng sá»‘ liá»‡u vÃ  biá»ƒu Ä‘á»“\\nCÃ¢u há»i tráº¯c nghiá»‡m mÃ´n Äá»‹a lÃ­ 12 chá»§ Ä‘á» Ä‘á»‹a lÃ­ vÃ¹ng kinh táº¿\\nCÃ¢u há»i tráº¯c nghiá»‡m mÃ´n Äá»‹a lÃ­ 12 chá»§ Ä‘á» Ä‘á»‹a lÃ­ ngÃ nh kinh táº¿\\nCÃ¢u há»i tráº¯c nghiá»‡m mÃ´n Äá»‹a lÃ­ 12 chá»§ Ä‘á» Ä‘á»‹a lÃ­ dÃ¢n cÆ°\\nCÃ¢u há»i tráº¯c nghiá»‡m mÃ´n Äá»‹a lÃ­ 12 chá»§ Ä‘á» Ä‘á»‹a lÃ­ tá»± nhiÃªn\\nCÃ¢u há»i vÃ  bÃ i táº­p kiá»ƒm tra kÄ© nÄƒng sá»­ dá»¥ng Atlat Äá»‹a lÃ­ Viá»‡t Nam\\nCÃ¢u há»i tráº¯c nghiá»‡m lÃ½ thuyáº¿t vÃ  thá»±c hÃ nh mÃ´n Äá»‹a lÃ­ 12 cÃ³ Ä‘Ã¡p Ã¡n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1['display_text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a81b0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: luat\n",
      "Crawling list page 1: https://vanbanphapluat.co/loai-van-ban/luat?p=1\n",
      "No items found â€“ check selector with DevTools (F12)\n",
      "Processing category: nghi-dinh\n",
      "Crawling list page 1: https://vanbanphapluat.co/loai-van-ban/nghi-dinh?p=1\n",
      "No items found â€“ check selector with DevTools (F12)\n",
      "Processing category: thong-tu\n",
      "Crawling list page 1: https://vanbanphapluat.co/loai-van-ban/thong-tu?p=1\n",
      "No items found â€“ check selector with DevTools (F12)\n",
      "Processing category: cong-van\n",
      "Crawling list page 1: https://vanbanphapluat.co/loai-van-ban/cong-van?p=1\n",
      "No items found â€“ check selector with DevTools (F12)\n",
      "Processing category: tieu-chuan-viet-nam\n",
      "Crawling list page 1: https://vanbanphapluat.co/loai-van-ban/tieu-chuan-viet-nam?p=1\n",
      "No items found â€“ check selector with DevTools (F12)\n",
      "Done! Data saved to vanbanphapluat_test_data.jsonl. If still empty, inspect page with F12 and update selectors (e.g., search for 'class' of list items).\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Headers polite\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Referer': 'https://vanbanphapluat.co/'\n",
    "}\n",
    "\n",
    "# Categories chÃ­nh (tá»« analysis: luat, nghi-dinh, thong-tu, cong-van, tieu-chuan-viet-nam)\n",
    "categories = {\n",
    "    'luat': 'https://vanbanphapluat.co/loai-van-ban/luat',\n",
    "    'nghi-dinh': 'https://vanbanphapluat.co/loai-van-ban/nghi-dinh',\n",
    "    'thong-tu': 'https://vanbanphapluat.co/loai-van-ban/thong-tu',\n",
    "    'cong-van': 'https://vanbanphapluat.co/loai-van-ban/cong-van',\n",
    "    'tieu-chuan-viet-nam': 'https://vanbanphapluat.co/loai-van-ban/tieu-chuan-viet-nam'\n",
    "}\n",
    "\n",
    "# Function crawl list tá»« category + pagination\n",
    "def crawl_list(category_url, max_pages=2):  # Giáº£m Ä‘á»ƒ test\n",
    "    documents = []\n",
    "    page = 1\n",
    "    while page <= max_pages:\n",
    "        url = f\"{category_url}?p={page}\"\n",
    "        print(f\"Crawling list page {page}: {url}\")\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"Error {resp.status_code}\")\n",
    "            break\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # Updated selector: Thá»­ 'ul.list-vanban li' hoáº·c 'div.document-list div.item' â€“ check F12 náº¿u miss\n",
    "        items = soup.select('ul.list-vanban li')  # Hoáº·c 'div.document-list .item', 'div.vb-item'\n",
    "        if not items:  # Háº¿t page hoáº·c selector sai\n",
    "            print(\"No items found â€“ check selector with DevTools (F12)\")\n",
    "            break\n",
    "        \n",
    "        for item in items:\n",
    "            title_elem = item.select_one('a.vb-title')  # Hoáº·c 'a.document-title', 'a.link'\n",
    "            if title_elem:\n",
    "                title = title_elem.text.strip()\n",
    "                detail_url = 'https://vanbanphapluat.co' + title_elem['href']\n",
    "                meta = item.select_one('span.meta-info').text.strip() if item.select_one('span.meta-info') else ''  # Hoáº·c 'div.meta'\n",
    "                documents.append({'title': title, 'detail_url': detail_url, 'meta': meta})\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Function crawl full content tá»« detail URL\n",
    "def crawl_detail(detail_url):\n",
    "    print(f\"Crawling detail: {detail_url}\")\n",
    "    resp = requests.get(detail_url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    \n",
    "    # Updated selector for detail: Thá»­ 'span.so-hieu' cho number, 'div.noi-dung-van-ban p' cho content\n",
    "    number = soup.select_one('span.so-hieu').text.strip() if soup.select_one('span.so-hieu') else ''  # Hoáº·c 'span.doc-number'\n",
    "    issuance_date = soup.select_one('span.ngay-ban-hanh').text.strip() if soup.select_one('span.ngay-ban-hanh') else ''  # Hoáº·c 'span.issuance-date'\n",
    "    agency = soup.select_one('span.co-quan-ban-hanh').text.strip() if soup.select_one('span.co-quan-ban-hanh') else ''  # Hoáº·c 'span.agency'\n",
    "    status = soup.select_one('span.tinh-trang').text.strip() if soup.select_one('span.tinh-trang') else ''  # Hoáº·c 'span.status'\n",
    "    update_time = soup.select_one('span.ngay-cap-nhat').text.strip() if soup.select_one('span.ngay-cap-nhat') else ''  # Hoáº·c 'span.update-time'\n",
    "    content = ' '.join([p.text.strip() for p in soup.select('div.noi-dung-van-ban p')])  # Hoáº·c 'div.content-van-ban p', 'div.full-text p'\n",
    "    \n",
    "    data = {\n",
    "        'detail_url': detail_url,\n",
    "        'number': number,\n",
    "        'issuance_date': issuance_date,\n",
    "        'agency': agency,\n",
    "        'status': status,\n",
    "        'update_time': update_time,\n",
    "        'content': content\n",
    "    }\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    return data\n",
    "\n",
    "# Main: Crawl all categories\n",
    "output_file = Path(\"vanbanphapluat_test_data.jsonl\")\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for cat_name, cat_url in categories.items():\n",
    "        print(f\"Processing category: {cat_name}\")\n",
    "        docs_list = crawl_list(cat_url, max_pages=2)  # Test nhá»\n",
    "        docs_list = docs_list[:5]  # Giá»›i háº¡n 5 items/category Ä‘á»ƒ test\n",
    "        \n",
    "        for doc in docs_list:\n",
    "            full_data = crawl_detail(doc['detail_url'])\n",
    "            if full_data:\n",
    "                full_data['category'] = cat_name\n",
    "                full_data.update(doc)  # Add list metadata\n",
    "                f.write(json.dumps(full_data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Done! Data saved to {output_file}. If still empty, inspect page with F12 and update selectors (e.g., search for 'class' of list items).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "879848fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Báº®T Äáº¦U CHáº Y THá»¬ (DRY RUN)...\n",
      "âš™ï¸  Cáº¥u hÃ¬nh: 2 trang/má»¥c | Batch: 5\n",
      "\n",
      "ğŸ“‚ Má»¥c: test_van_ban_moi\n",
      "   ğŸ“„ Page 1: https://vanbanphapluat.co/van-ban-moi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Láº¥y Ä‘Æ°á»£c 6 bÃ i má»›i.\n",
      "   ğŸ“„ Page 2: https://vanbanphapluat.co/van-ban-moi?page=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Láº¥y Ä‘Æ°á»£c 0 bÃ i má»›i.\n",
      "\n",
      "ğŸ“‚ Má»¥c: test_luat\n",
      "   ğŸ“„ Page 1: https://vanbanphapluat.co/loai-van-ban/luat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Láº¥y Ä‘Æ°á»£c 80 bÃ i má»›i.\n",
      "   ğŸ“„ Page 2: https://vanbanphapluat.co/loai-van-ban/luat?page=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Láº¥y Ä‘Æ°á»£c 2 bÃ i má»›i.\n",
      "\n",
      "ğŸ“¦ Äang gá»™p file káº¿t quáº£...\n",
      "ğŸ‰ THÃ€NH CÃ”NG! ÄÃ£ lÆ°u 88 vÄƒn báº£n vÃ o: van_ban_phap_luat_test.parquet\n",
      "ğŸ’¡ Báº¡n cÃ³ thá»ƒ dÃ¹ng pandas Ä‘á»ƒ Ä‘á»c file nÃ y kiá»ƒm tra.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import urllib3\n",
    "\n",
    "# Táº¯t cáº£nh bÃ¡o SSL (Do verify=False)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# --- Cáº¤U HÃŒNH CHáº Y THá»¬ (TEST CONFIG) ---\n",
    "DB_FILE = \"vbpl_crawler_test.db\"  # Äá»•i tÃªn DB Ä‘á»ƒ khÃ´ng láº«n vá»›i báº£n tháº­t\n",
    "OUTPUT_FILE = \"van_ban_phap_luat_test.parquet\"\n",
    "TEMP_BATCH_DIR = \"vbpl_batches_test\"\n",
    "CHECKPOINT_SIZE = 5       # Test: LÆ°u sau má»—i 5 bÃ i (Ä‘á»ƒ tháº¥y káº¿t quáº£ nhanh)\n",
    "MAX_PAGES_PER_CAT = 2     # Test: Chá»‰ quÃ©t 2 trang danh sÃ¡ch má»—i má»¥c\n",
    "MIN_CONTENT_LENGTH = 500  # Giáº£m xuá»‘ng má»™t chÃºt Ä‘á»ƒ test\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    filename=\"vbpl_crawler_test.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "os.makedirs(TEMP_BATCH_DIR, exist_ok=True)\n",
    "\n",
    "# --- DANH Má»¤C TEST (RÃšT Gá»ŒN) ---\n",
    "# Chá»‰ láº¥y 2 má»¥c Ä‘áº¡i diá»‡n Ä‘á»ƒ test logic\n",
    "SEED_CATEGORIES = {\n",
    "    \"test_van_ban_moi\": [\"/van-ban-moi\"],\n",
    "    \"test_luat\": [\"/loai-van-ban/luat\"]\n",
    "}\n",
    "\n",
    "# --- DATABASE MANAGER (Giá»¯ nguyÃªn logic) ---\n",
    "class HistoryDB:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self.cursor = None\n",
    "        try:\n",
    "            self.conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "            self.cursor = self.conn.cursor()\n",
    "            self.cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS visited_urls (\n",
    "                    url_hash TEXT PRIMARY KEY,\n",
    "                    url TEXT UNIQUE,\n",
    "                    category TEXT,\n",
    "                    depth INTEGER,\n",
    "                    status TEXT DEFAULT 'pending',\n",
    "                    crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            self.conn.commit()\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"âŒ Cannot connect to DB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()\n",
    "        return False\n",
    "\n",
    "    def exists(self, url):\n",
    "        try:\n",
    "            h = hashlib.md5(url.encode()).hexdigest()\n",
    "            self.cursor.execute(\"SELECT 1 FROM visited_urls WHERE url_hash = ?\", (h,))\n",
    "            return self.cursor.fetchone() is not None\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def add(self, url, category, depth=0, status='success'):\n",
    "        h = hashlib.md5(url.encode()).hexdigest()\n",
    "        try:\n",
    "            self.cursor.execute(\n",
    "                \"INSERT OR IGNORE INTO visited_urls (url_hash, url, category, depth, status) VALUES (?, ?, ?, ?, ?)\", \n",
    "                (h, url, category, depth, status)\n",
    "            )\n",
    "            self.conn.commit()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"DB insert error: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def get_session():\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'vi-VN,vi;q=0.9'\n",
    "    })\n",
    "    return session\n",
    "\n",
    "def extract_legal_document(session, url):\n",
    "    try:\n",
    "        # Timeout tháº¥p hÆ¡n cho test\n",
    "        resp = session.get(url, timeout=10, verify=False) \n",
    "        \n",
    "        # Validation sÆ¡ bá»™\n",
    "        if len(resp.text) < 1000: return None\n",
    "\n",
    "        data = trafilatura.extract(\n",
    "            resp.text,\n",
    "            output_format=\"json\",\n",
    "            include_comments=False,\n",
    "            include_tables=True,\n",
    "            favor_precision=True\n",
    "        )\n",
    "        \n",
    "        if data:\n",
    "            import json\n",
    "            j = json.loads(data)\n",
    "            text = j.get('text', '').strip()\n",
    "            \n",
    "            if len(text) < MIN_CONTENT_LENGTH: return None\n",
    "            \n",
    "            return {\n",
    "                \"title\": j.get('title', '').strip(),\n",
    "                \"text\": text,\n",
    "                \"url\": url,\n",
    "                \"length\": len(text)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Extract error {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def find_document_links(session, url):\n",
    "    links = set()\n",
    "    base_url = \"https://vanbanphapluat.co\"\n",
    "    try:\n",
    "        resp = session.get(url, timeout=10, verify=False)\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        \n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            full_url = urljoin(base_url, href)\n",
    "            \n",
    "            # Logic lá»c link chi tiáº¿t vÄƒn báº£n\n",
    "            if (base_url in full_url and \n",
    "                '/loai-van-ban/' not in full_url and\n",
    "                '/linh-vuc/' not in full_url and\n",
    "                '/van-ban-moi' not in full_url and\n",
    "                len(full_url) > len(base_url) + 10):\n",
    "                links.add(full_url)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"List page error {url}: {e}\")\n",
    "    return list(links)\n",
    "\n",
    "def save_batch(batch_data, batch_id):\n",
    "    if not batch_data: return\n",
    "    try:\n",
    "        df = pd.DataFrame(batch_data)\n",
    "        batch_file = os.path.join(TEMP_BATCH_DIR, f\"test_batch_{batch_id:04d}.parquet\")\n",
    "        df.to_parquet(batch_file, index=False)\n",
    "        logging.info(f\"ğŸ’¾ Saved batch {batch_id}: {len(df)} docs\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Save batch error: {e}\")\n",
    "\n",
    "# --- CORE LOGIC (ÄÃ£ sá»­a thuáº­t toÃ¡n Loop) ---\n",
    "def crawl_category_pages(session, db, category_name, seed_urls, batch_buffer, batch_counter):\n",
    "    total_collected = 0\n",
    "    \n",
    "    for seed_url in seed_urls:\n",
    "        page = 1\n",
    "        \n",
    "        # --- TEST LIMIT: Chá»‰ cháº¡y sá»‘ trang quy Ä‘á»‹nh ---\n",
    "        while page <= MAX_PAGES_PER_CAT:\n",
    "            \n",
    "            # XÃ¢y dá»±ng URL phÃ¢n trang\n",
    "            if page == 1:\n",
    "                current_url = seed_url\n",
    "            else:\n",
    "                current_url = f\"{seed_url}?page={page}\"\n",
    "            \n",
    "            logging.info(f\"Scanning: {current_url}\")\n",
    "            print(f\"   ğŸ“„ Page {page}: {current_url}\")\n",
    "\n",
    "            links = find_document_links(session, current_url)\n",
    "            \n",
    "            if not links:\n",
    "                print(\"   âš ï¸ Háº¿t bÃ i hoáº·c lá»—i máº¡ng.\")\n",
    "                break\n",
    "\n",
    "            # Duyá»‡t qua tá»«ng bÃ i trong trang nÃ y\n",
    "            new_in_page = 0\n",
    "            for doc_url in tqdm(links, desc=f\"   Page {page}\", leave=False):\n",
    "                if db.exists(doc_url): continue\n",
    "                \n",
    "                doc = extract_legal_document(session, doc_url)\n",
    "                if doc:\n",
    "                    batch_buffer.append({\n",
    "                        \"doc_title\": doc['title'],\n",
    "                        \"doc_url\": doc['url'],\n",
    "                        \"content\": doc['text'],\n",
    "                        \"category\": category_name,\n",
    "                        \"crawled_at\": datetime.now().isoformat()\n",
    "                    })\n",
    "                    db.add(doc_url, category_name, 0, 'success')\n",
    "                    new_in_page += 1\n",
    "                    total_collected += 1\n",
    "                    \n",
    "                    # Checkpoint save\n",
    "                    if len(batch_buffer) >= CHECKPOINT_SIZE:\n",
    "                        save_batch(batch_buffer, batch_counter[0])\n",
    "                        batch_counter[0] += 1\n",
    "                        batch_buffer.clear()\n",
    "                \n",
    "                time.sleep(0.5) # Delay nháº¹\n",
    "\n",
    "            print(f\"   âœ… Láº¥y Ä‘Æ°á»£c {new_in_page} bÃ i má»›i.\")\n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "            \n",
    "    return total_collected\n",
    "\n",
    "# --- MAIN RUN ---\n",
    "def main():\n",
    "    print(\"ğŸš€ Báº®T Äáº¦U CHáº Y THá»¬ (DRY RUN)...\")\n",
    "    print(f\"âš™ï¸  Cáº¥u hÃ¬nh: {MAX_PAGES_PER_CAT} trang/má»¥c | Batch: {CHECKPOINT_SIZE}\")\n",
    "    \n",
    "    session = get_session()\n",
    "    batch_buffer = []\n",
    "    batch_counter = [0]\n",
    "    \n",
    "    with HistoryDB(DB_FILE) as db:\n",
    "        for cat, seeds in SEED_CATEGORIES.items():\n",
    "            print(f\"\\nğŸ“‚ Má»¥c: {cat}\")\n",
    "            full_seeds = [urljoin(\"https://vanbanphapluat.co\", s) for s in seeds]\n",
    "            crawl_category_pages(session, db, cat, full_seeds, batch_buffer, batch_counter)\n",
    "    \n",
    "    # Save ná»‘t pháº§n cÃ²n láº¡i\n",
    "    if batch_buffer:\n",
    "        save_batch(batch_buffer, batch_counter[0])\n",
    "\n",
    "    # Merge file\n",
    "    print(\"\\nğŸ“¦ Äang gá»™p file káº¿t quáº£...\")\n",
    "    all_files = [os.path.join(TEMP_BATCH_DIR, f) for f in os.listdir(TEMP_BATCH_DIR) if f.endswith('.parquet')]\n",
    "    if all_files:\n",
    "        combined = pd.concat([pd.read_parquet(f) for f in all_files], ignore_index=True)\n",
    "        combined.to_parquet(OUTPUT_FILE, index=False)\n",
    "        print(f\"ğŸ‰ THÃ€NH CÃ”NG! ÄÃ£ lÆ°u {len(combined)} vÄƒn báº£n vÃ o: {OUTPUT_FILE}\")\n",
    "        print(\"ğŸ’¡ Báº¡n cÃ³ thá»ƒ dÃ¹ng pandas Ä‘á»ƒ Ä‘á»c file nÃ y kiá»ƒm tra.\")\n",
    "        # Dá»n dáº¹p file rÃ¡c\n",
    "        for f in all_files: os.remove(f)\n",
    "        os.rmdir(TEMP_BATCH_DIR)\n",
    "    else:\n",
    "        print(\"âš ï¸ KhÃ´ng thu tháº­p Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt1 = pd.read_parquet(path=\"van_ban_phap_luat_test.parquet\")\n",
    "tt1['content'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831df11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
