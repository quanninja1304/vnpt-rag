import pandas as pd
from langchain_text_splitters import RecursiveCharacterTextSplitter
from tqdm import tqdm
import re
import os
import json
from pathlib import Path
from config import Config

# ===========================
# 1. STATE MANAGEMENT (NEW)
# ===========================
def load_processed_state():
    """Äá»c danh sÃ¡ch cÃ¡c bÃ i Ä‘Ã£ chunk trÆ°á»›c Ä‘Ã³"""
    if Config.CHUNKING_STATE_FILE.exists():
        with open(Config.CHUNKING_STATE_FILE, 'r', encoding='utf-8') as f:
            return set(json.load(f))
    return set()

def save_processed_state(processed_titles):
    """LÆ°u láº¡i danh sÃ¡ch cÃ¡c bÃ i Ä‘Ã£ chunk"""
    with open(Config.CHUNKING_STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(list(processed_titles), f, ensure_ascii=False)

# ===========================
# 2. CLEAN TEXT (CORE LOGIC)
# ===========================
def clean_wiki_text(text: str) -> str:
    """
    LÃ m sáº¡ch vÄƒn báº£n Wikipedia (Fix triá»‡t Ä‘á»ƒ lá»—i chunk cuá»‘i bá»‹ dÃ­nh footer)
    """
    if not isinstance(text, str) or not text: return ""
    
    # --- 1. Cáº®T Bá» FOOTER (Logic dÃ²ng Ä‘Æ¡n) ---
    # Thay vÃ¬ tÃ¬m regex phá»©c táº¡p, ta duyá»‡t tá»«ng dÃ²ng.
    # Náº¿u gáº·p dÃ²ng nÃ o ngáº¯n (< 50 kÃ½ tá»±) mÃ  chá»©a tá»« khÃ³a dá»«ng -> Cáº®T Háº¾T tá»« Ä‘Ã³ vá» sau.
    
    stop_phrases = [
        'tham kháº£o', 'thao kháº£o', 'liÃªn káº¿t ngoÃ i', 'chÃº thÃ­ch', 'xem thÃªm',
        'tÃ i liá»‡u tham kháº£o', 'Ä‘á»c thÃªm', 'nguá»“n', 'ghi chÃº'
    ]
    
    lines = text.split('\n')
    cut_index = len(lines)
    
    for i, line in enumerate(lines):
        # Chuáº©n hÃ³a dÃ²ng Ä‘á»ƒ kiá»ƒm tra
        line_clean = line.strip().lower()
        
        # Bá» decorators
        line_clean = re.sub(r'[=:\-\.]', '', line_clean).strip()
        
        # Náº¿u dÃ²ng ngáº¯n (lÃ  tiÃªu Ä‘á») vÃ  khá»›p tá»« khÃ³a dá»«ng
        if len(line_clean) < 40 and line_clean in stop_phrases:
            cut_index = i
            break
            
    # Cáº¯t bá» pháº§n rÃ¡c
    text = '\n'.join(lines[:cut_index])

    # --- 2. XÃ“A RÃC ARTIFACTS ---
    text = re.sub(r'\[\d+\]', '', text)
    text = re.sub(r'\[[a-zÃ -á»¹\s]+\]', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\[\[.*?\]\]', '', text)
    
    # --- 3. Gá»˜P DÃ’NG TIÃŠU Äá»€ (Fix lá»—i cá»¥t lá»§n) ---
    # Biáº¿n cÃ¡c dÃ²ng tiÃªu Ä‘á» cÃ´ láº­p thÃ nh cÃ¢u Ä‘á»ƒ dÃ­nh vÃ o Ä‘oáº¡n sau
    text = text.replace('\r\n', '\n')
    # Regex: TÃ¬m dáº¥u xuá»‘ng dÃ²ng Ä‘Æ¡n (\n) khÃ´ng Ä‘i kÃ¨m \n khÃ¡c
    text = re.sub(r'(?<!\n)\n(?!\n)', '. ', text)
    text = re.sub(r'\.\.', '.', text) # Sá»­a lá»—i 2 dáº¥u cháº¥m
    text = re.sub(r'\. \.', '.', text)
    text = re.sub(r' +', ' ', text) # XÃ³a khoáº£ng tráº¯ng thá»«a
    
    return text.strip()

# ===========================
# 3. CHUNKING PROCESS
# ===========================
def process_chunking():
    # --- A. LOAD Dá»® LIá»†U ---
    print(f"File input: {Config.CHUNKING_INPUT_FILE}")
    if not Config.CHUNKING_INPUT_FILE.exists():
        print(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y file {Config.CHUNKING_INPUT_FILE}")
        return

    try:
        if Config.CHUNKING_INPUT_FILE.suffix == '.parquet':
            df = pd.read_parquet(Config.CHUNKING_INPUT_FILE)
        else:
            df = pd.read_csv(Config.CHUNKING_INPUT_FILE)
    except Exception as e:
        print(f"Lá»—i Ä‘á»c file: {e}")
        return
        
    print(f"Sá»‘ lÆ°á»£ng bÃ i viáº¿t gá»‘c: {len(df)}")
    
    # --- LOGIC INCREMENTAL: Lá»ŒC BÃ€I Má»šI ---
    processed_titles = load_processed_state()
    print(f"ğŸ“¦ Tá»•ng bÃ i viáº¿t trong kho: {len(df)}")
    print(f"ğŸ”„ ÄÃ£ xá»­ lÃ½ trÆ°á»›c Ä‘Ã³: {len(processed_titles)}")
    
    # Lá»c ra cÃ¡c bÃ i chÆ°a cÃ³ trong state
    df_new = df[~df['title'].isin(processed_titles)]
    
    if len(df_new) == 0:
        print("âœ… KhÃ´ng cÃ³ bÃ i viáº¿t má»›i. Pipeline nghá»‰ ngÆ¡i!")
        # XÃ³a file delta cÅ© Ä‘á»ƒ trÃ¡nh Indexing náº¡p láº¡i thá»«a
        if Config.LATEST_CHUNKS_FILE.exists():
            os.remove(Config.LATEST_CHUNKS_FILE)
        return

    print(f"âš¡ PhÃ¡t hiá»‡n {len(df_new)} bÃ i viáº¿t má»›i. Báº¯t Ä‘áº§u chunking...")
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=Config.CHUNK_SIZE,
        chunk_overlap=Config.CHUNK_OVERLAP,
        separators=["\n\n", "\n", ". ", ".", "!", "?", ";", " ", ""],
        length_function=len,
        is_separator_regex=False
    )
    
    new_chunks = []
    
    # --- VÃ’NG Láº¶P Xá»¬ LÃ (Chá»‰ cháº¡y trÃªn df_new) ---
    for idx, row in tqdm(df_new.iterrows(), total=len(df_new)):
        original_text = row.get('text', '')
        title = row.get('title', 'KhÃ´ng tiÃªu Ä‘á»')
        url = row.get('url', '')
        categories = row.get('categories', [])
        cat_str = str(categories) if categories else ""

        # Cleaning
        clean_text = clean_wiki_text(original_text)
        if len(clean_text) < 50: 
            processed_titles.add(title) # ÄÃ¡nh dáº¥u Ä‘Ã£ xá»­ lÃ½ (dÃ¹ lÃ  rÃ¡c)
            continue

        # Chunking
        chunks = splitter.create_documents([clean_text])
        
        for i, chunk in enumerate(chunks):
            content = re.sub(r'^[.,;\s]+', '', chunk.page_content).strip()
            
            # --- FILTERS ---
            if len(content) < 60: continue
            if content.endswith(':'): continue
            
            bad_keywords = ["NiÃªn biá»ƒu", "Má»¥c lá»¥c", "Danh sÃ¡ch", "CÃ¡c vua", "Tiá»ƒu sá»­"]
            if len(content) < 100 and any(kw in content for kw in bad_keywords):
                if content.count('.') > 2: continue
            
            if len(content) < 150 and content[-1] not in ['.', '!', '?', '"', "'", ')']:
                continue
            if not any(char in content for char in ['.', '?', '!', ';']):
                if len(content) < 100: continue
            if content.count("ISBN") > 0 or content.count("Xuáº¥t báº£n") > 1:
                continue

            # Context Injection
            if content[-1] not in ['.', '!', '?', ';', '"', "'", ')']:
                content += "."
            
            vector_text = f"Chá»§ Ä‘á»: {title}\nNá»™i dung: {content}"
            
            new_chunks.append({
                "chunk_id": f"{idx}_{i}", # LÆ°u Ã½: idx nÃ y lÃ  cá»§a df_new
                "doc_title": title,
                "doc_url": url,
                "doc_category": cat_str,
                "vector_text": vector_text,
                "display_text": content,
                "char_len": len(vector_text)
            })
            
        # ÄÃ¡nh dáº¥u bÃ i nÃ y Ä‘Ã£ xong
        processed_titles.add(title)

    # --- LÆ¯U Káº¾T QUáº¢ ---
    if not new_chunks:
        print("âš ï¸ CÃ¡c bÃ i má»›i khÃ´ng táº¡o Ä‘Æ°á»£c chunk nÃ o.")
        save_processed_state(processed_titles) # Váº«n lÆ°u state Ä‘á»ƒ láº§n sau khÃ´ng check láº¡i
        return

    df_delta = pd.DataFrame(new_chunks)
    
    # 1. LÆ°u file DELTA (Chá»‰ chá»©a cÃ¡i má»›i Ä‘á»ƒ Indexing dÃ¹ng)
    Config.setup_dirs()
    df_delta.to_parquet(Config.LATEST_CHUNKS_FILE, index=False, compression='snappy')
    print(f"ğŸ’¾ [Delta] ÄÃ£ lÆ°u {len(df_delta)} chunks má»›i vÃ o: {Config.LATEST_CHUNKS_FILE}")
    
    # 2. Append vÃ o file MASTER (Äá»ƒ backup toÃ n bá»™)
    if Config.MASTER_CHUNKS_FILE.exists():
        try:
            df_master = pd.read_parquet(Config.MASTER_CHUNKS_FILE)
            df_combined = pd.concat([df_master, df_delta], ignore_index=True)
            df_combined.to_parquet(Config.MASTER_CHUNKS_FILE, index=False, compression='snappy')
        except:
            df_delta.to_parquet(Config.MASTER_CHUNKS_FILE, index=False)
    else:
        df_delta.to_parquet(Config.MASTER_CHUNKS_FILE, index=False, compression='snappy')
    print(f"ğŸ’¾ [Master] ÄÃ£ cáº­p nháº­t file tá»•ng: {Config.MASTER_CHUNKS_FILE}")

    # 3. LÆ°u tráº¡ng thÃ¡i
    save_processed_state(processed_titles)
    print("âœ… ÄÃ£ cáº­p nháº­t tráº¡ng thÃ¡i xá»­ lÃ½.")

if __name__ == "__main__":
    process_chunking()