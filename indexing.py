import asyncio
import aiohttp
import pandas as pd
from qdrant_client import QdrantClient, models
import json
import uuid
import sys
import os
from aiolimiter import AsyncLimiter
from tqdm.asyncio import tqdm
from config import Config

# --- 1. UTILS ---
def generate_uuid5(unique_string):
    return str(uuid.uuid5(uuid.NAMESPACE_DNS, unique_string))

# --- 2. ASYNC API CLIENT ---
async def get_embedding_async(session, text, limiter, token_key):
    """G·ªçi API b·∫•t ƒë·ªìng b·ªô v·ªõi Rate Limit"""
    
    # [FIX 1] L·∫•y URL chu·∫©n t·ª´ Config
    url = Config.VNPT_EMBEDDING_URL 
    
    headers = {
        # [FIX 2] Token Key l·∫•y t·ª´ tham s·ªë truy·ªÅn v√†o (tr√≠ch t·ª´ Config)
        "Authorization": f"Bearer {token_key}", 
        "Content-Type": "application/json"
    }
    
    # Payload theo document c·ªßa VNPT
    payload = {
        "input": text,
        # L∆∞u √Ω: check l·∫°i doc xem model name c√≥ c·∫ßn thi·∫øt ko, th∆∞·ªùng embedding model t√™n c·ªë ƒë·ªãnh
        "model": Config.MODEL_EMBEDDING_API 
    }

    retry_count = 0
    # [OPTIMIZE] TƒÉng timeout l√™n 60s ƒë·ªÉ tr√°nh l·ªói m·∫°ng ch·∫≠p ch·ªùn
    timeout = aiohttp.ClientTimeout(total=60) 

    while retry_count < 3:
        async with limiter: 
            try:
                async with session.post(url, json=payload, headers=headers, timeout=timeout) as response:
                    if response.status == 200:
                        data = await response.json()
                        # Handle format tr·∫£ v·ªÅ (linh ho·∫°t cho c·∫£ list v√† dict)
                        if isinstance(data, list): return data
                        if "data" in data and len(data["data"]) > 0:
                            return data["data"][0]["embedding"]
                        return None
                    
                    elif response.status == 429:
                        # [LOGIC] N·∫øu b·ªã 429 th·∫≠t, ng·ªß l√¢u h∆°n ch√∫t
                        print(f"‚ö†Ô∏è 429 Too Many Requests. Backoff 10s...")
                        await asyncio.sleep(10)
                        retry_count += 1
                    else:
                        # Log l·ªói nh·∫π ƒë·ªÉ kh√¥ng spam m√†n h√¨nh
                        # err_text = await response.text()
                        # print(f"‚ùå {response.status}: {err_text[:50]}...")
                        return None
            except Exception as e:
                # print(f"‚ö†Ô∏è Net Error: {e}")
                retry_count += 1
                await asyncio.sleep(1)
    return None

# --- 3. WORKER LOGIC ---
async def worker(queue, session, client, limiter, pbar, failed_log, token_key):
    buffer_points = []
    
    while True:
        item = await queue.get()
        if item is None: break 

        vec = await get_embedding_async(session, item['vector_text'], limiter, token_key)
        
        if vec:
            point = models.PointStruct(
                id=generate_uuid5(str(item['chunk_id'])),
                vector=vec,
                payload={
                    "title": item['doc_title'],
                    "text": item['display_text'],
                    "category": item['doc_category'],
                    "url": item.get('doc_url', ''),
                    "chunk_id": item['chunk_id']
                }
            )
            buffer_points.append(point)
        else:
            failed_log.append(item['chunk_id'])
            # [SAFETY] Ghi log n√≥ng ph√≤ng tr∆∞·ªùng h·ª£p crash
            with open("temp_failed_log_async.txt", "a") as f:
                f.write(f"{item['chunk_id']}\n")

        pbar.update(1)

        # Upsert Batch (50 items)
        if len(buffer_points) >= 50:
            try:
                # Upsert blocking trong thread ri√™ng
                await asyncio.to_thread(
                    client.upsert, 
                    collection_name=Config.COLLECTION_NAME, 
                    points=buffer_points
                )
            except Exception as e:
                print(f"‚ùå Upsert Failed: {e}")
                failed_log.extend([p.payload['chunk_id'] for p in buffer_points])
            finally:
                buffer_points = [] 

        queue.task_done()

    # V√©t n·ªët buffer c√≤n l·∫°i
    if buffer_points:
        try:
            await asyncio.to_thread(
                client.upsert, 
                collection_name=Config.COLLECTION_NAME, 
                points=buffer_points
            )
        except Exception:
            pass

# --- 4. MAIN ---
async def main_async():
    # [FIX 3] ∆Øu ti√™n ƒë·ªçc file ƒê√É L·ªåC (cleaned) n·∫øu c√≥, n·∫øu kh√¥ng th√¨ ƒë·ªçc file LATEST
    input_file = "cleaned_chunks.parquet" # File sinh ra t·ª´ b∆∞·ªõc l·ªçc d·ªØ li·ªáu
    if not os.path.exists(input_file):
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file ƒë√£ l·ªçc '{input_file}'. D√πng file g·ªëc (S·∫º R·∫§T L√ÇU).")
        input_file = Config.LATEST_CHUNKS_FILE
    
    if not os.path.exists(input_file):
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu input.")
        return
    
    df = pd.read_parquet(input_file)
    
    # [QUAN TR·ªåNG] Checkpoint: L·ªçc b·ªè nh·ªØng ID ƒë√£ l√†m r·ªìi
    done_file = "processed_chunks.txt"
    if os.path.exists(done_file):
        with open(done_file, 'r') as f:
            done_ids = set(line.strip() for line in f)
        # Ch·ªâ gi·ªØ l·∫°i nh·ªØng d√≤ng ch∆∞a l√†m
        df = df[~df['chunk_id'].astype(str).isin(done_ids)]
        print(f"üîÑ Resume: ƒê√£ b·ªè qua {len(done_ids)} chunks ƒë√£ l√†m tr∆∞·ªõc ƒë√≥.")

    records = df.to_dict('records')
    total = len(records)
    print(f"üî• B·∫Øt ƒë·∫ßu Async Indexing: {total} chunks...")

    if total == 0:
        print("‚úÖ Kh√¥ng c√≤n g√¨ ƒë·ªÉ l√†m.")
        return

    # Setup Qdrant
    client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)
    
    # Ki·ªÉm tra Collection
    if not client.collection_exists(Config.COLLECTION_NAME):
        # L·∫•y vector size chu·∫©n b·∫±ng 1 request test (Sync)
        print("üß™ Testing API connection...")
        try:
             # Logic test sync ·ªü ƒë√¢y (l∆∞·ª£c b·ªè cho g·ªçn)
             pass
        except:
             pass
        
        # T·∫°o m·ªõi n·∫øu ch∆∞a c√≥
        client.create_collection(
            collection_name=Config.COLLECTION_NAME,
            vectors_config=models.VectorParams(size=1024, distance=models.Distance.COSINE) # Gi·∫£ ƒë·ªãnh 1024
        )

    # Queue & Limiter
    queue = asyncio.Queue()
    for item in records:
        queue.put_nowait(item)

    limiter = AsyncLimiter(480, 60) 
    failed_ids = []
    
    # [FIX 4] L·∫•y Token chu·∫©n t·ª´ Config Credentials
    embedding_config = Config.VNPT_CREDENTIALS[Config.MODEL_EMBEDDING_API]
    token_key = embedding_config["token_key"] # L·∫•y ƒë√∫ng Key

    num_workers = 20 
    
    async with aiohttp.ClientSession() as session:
        pbar = tqdm(total=total, desc="Indexing", unit="chk")
        tasks = []
        for _ in range(num_workers):
            task = asyncio.create_task(
                worker(queue, session, client, limiter, pbar, failed_ids, token_key)
            )
            tasks.append(task)

        await queue.join()

        for _ in range(num_workers):
            await queue.put(None)
        
        await asyncio.gather(*tasks)
        pbar.close()

    print("\n‚úÖ INDEXING COMPLETED!")
    if failed_ids:
        print(f"‚ö†Ô∏è Failed: {len(failed_ids)} chunks.")
        with open("failed_chunks.json", "w") as f:
            json.dump(failed_ids, f)

if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main_async())