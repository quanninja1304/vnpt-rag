import asyncio
import aiohttp
import pandas as pd
import json
import uuid
import sys
import os
from aiolimiter import AsyncLimiter
from tqdm.asyncio import tqdm
from qdrant_client import QdrantClient, models
from config import Config

# --- 1. C·∫§U H√åNH T·ªêI ∆ØU ---
# Rate Limit an to√†n: 480 req/ph√∫t (t·ªëi ƒëa 500)
RATE_LIMITER = AsyncLimiter(480, 60)

# S·ªë l∆∞·ª£ng Concurrent Workers (Async nh·∫π n√™n c√≥ th·ªÉ ƒë·ªÉ 20-30)
NUM_WORKERS = 30

# S·ªë l∆∞·ª£ng vector gom l·∫°i tr∆∞·ªõc khi Upsert v√†o Qdrant
UPSERT_BATCH_SIZE = 50

# --- 2. H√ÄM H·ªñ TR·ª¢ ---
def save_checkpoint(ids_list):
    """L∆∞u danh s√°ch ID ƒë√£ xong v√†o file (append mode)"""
    if not ids_list: return
    try:
        with open(Config.CHECKPOINT_FILE, "a", encoding="utf-8") as f:
            for chunk_id in ids_list:
                f.write(f"{chunk_id}\n")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói ghi checkpoint: {e}")

def generate_uuid5(unique_string):
    """T·∫°o UUID c·ªë ƒë·ªãnh d·ª±a tr√™n chu·ªói nh·∫≠p v√†o (Idempotent)"""
    return str(uuid.uuid5(uuid.NAMESPACE_DNS, str(unique_string)))

async def get_embedding_async(session, text, retry_attempts=3):
    """
    Phi√™n b·∫£n Async c·ªßa get_vnpt_embedding.
    S·ª≠ d·ª•ng aiohttp ƒë·ªÉ kh√¥ng ch·∫∑n lu·ªìng khi ch·ªù server ph·∫£n h·ªìi.
    """
    model_name = Config.MODEL_EMBEDDING_API
    url = Config.VNPT_EMBEDDING_URL
    
    # L·∫•y Credential t·ª´ Config
    creds = Config.VNPT_CREDENTIALS.get(model_name)
    if not creds:
        print(f"‚ùå Config Error: Missing credentials for {model_name}")
        return None

    headers = {
        'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}',
        'Token-id': creds['token_id'],
        'Token-key': creds['token_key'],
        'Content-Type': 'application/json'
    }
    
    payload = {
        "model": model_name,
        "input": text,
        "encoding_format": "float"
    }

    for attempt in range(retry_attempts):
        async with RATE_LIMITER: # ƒê·ª£i slot (kh√¥ng block thread)
            try:
                async with session.post(url, json=payload, headers=headers, timeout=30) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        # X·ª≠ l√Ω c√°c format tr·∫£ v·ªÅ c√≥ th·ªÉ c√≥
                        if 'data' in data and len(data['data']) > 0:
                            return data['data'][0]['embedding']
                        else:
                            return None
                    
                    elif response.status == 429: # Too Many Requests
                        # Backoff nh·∫π
                        await asyncio.sleep(2 * (attempt + 1))
                        continue
                    
                    elif response.status >= 500: # Server Error
                        await asyncio.sleep(1)
                        continue
                    
                    else:
                        # L·ªói 400, 401... (L·ªói client/auth) -> Kh√¥ng retry
                        print(f"‚ùå API Error {response.status}: {await response.text()}")
                        return None

            except Exception as e:
                # L·ªói m·∫°ng, timeout...
                await asyncio.sleep(1)
                
    return None

# --- 3. WORKER LOGIC ---
async def worker(queue, session, client, pbar, failed_log):
    """
    Worker nh·∫≠n job t·ª´ queue -> G·ªçi API -> Gom Batch -> Upsert Qdrant
    """
    buffer_points = []
    
    while True:
        item = await queue.get()
        if item is None: # T√≠n hi·ªáu d·ª´ng
            break

        # 1. G·ªçi Embedding API
        try:
            # text ƒë·ªÉ embed: k·∫øt h·ª£p title + content
            vector = await get_embedding_async(session, item['vector_text'])
            
            if vector:
                # 2. T·∫°o Point Struct
                point = models.PointStruct(
                    id=generate_uuid5(item['chunk_id']),
                    vector=vector,
                    payload={
                        "title": item.get('doc_title', ''),
                        "text": item.get('display_text', ''), # Text hi·ªÉn th·ªã
                        "category": item.get('doc_category', ''),
                        "url": item.get('doc_url', ''),
                        "chunk_id": item['chunk_id']
                    }
                )
                buffer_points.append(point)
            else:
                failed_log.append(item['chunk_id'])
                
        except Exception as e:
            print(f"‚ö†Ô∏è Worker Error item {item.get('chunk_id')}: {e}")
            failed_log.append(item['chunk_id'])
        
        # Update progress bar
        pbar.update(1)

        # 3. Upsert Batch n·∫øu buffer ƒë·∫ßy
        if len(buffer_points) >= UPSERT_BATCH_SIZE:
            try:
                # Ch·∫°y upsert trong thread kh√°c ƒë·ªÉ kh√¥ng block event loop
                await asyncio.to_thread(
                    client.upsert,
                    collection_name=Config.COLLECTION_NAME,
                    points=buffer_points
                )
                # UPSERT TH√ÄNH C√îNG -> GHI CHECKPOINT NGAY
                # L·∫•y danh s√°ch chunk_id t·ª´ payload ƒë·ªÉ l∆∞u
                processed_ids = [p.payload['chunk_id'] for p in buffer_points]
                save_checkpoint(processed_ids)

            except Exception as e:
                print(f"‚ùå Qdrant Upsert Error: {e}")
                # L∆∞u l·∫°i id b·ªã l·ªói upsert
                failed_log.extend([p.payload['chunk_id'] for p in buffer_points])
            finally:
                buffer_points = [] # Clear buffer

        queue.task_done()

    # 4. V√©t n·ªët buffer c√≤n l·∫°i tr∆∞·ªõc khi ngh·ªâ
    if buffer_points:
        try:
            await asyncio.to_thread(
                client.upsert,
                collection_name=Config.COLLECTION_NAME,
                points=buffer_points
            )
            # Ghi n·ªët checkpoint cu·ªëi
            processed_ids = [p.payload['chunk_id'] for p in buffer_points]
            save_checkpoint(processed_ids)

        except Exception as e:
             print(f"‚ùå Final Upsert Error: {e}")
             failed_log.extend([p.payload['chunk_id'] for p in buffer_points])

# --- 4. MAIN PROCESS ---
async def main():
    # Setup Directories
    Config.setup_dirs()

    # 1. Load Data
    input_file = Config.INDEXING_INPUT_FILE # File delta chunks
    if not input_file.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file input: {input_file}")
        print("üí° H√£y ch·∫°y chunking.py tr∆∞·ªõc.")
        return

    print(f"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {input_file}")
    df = pd.read_parquet(input_file)
    
    df['chunk_id'] = df['chunk_id'].astype(str)
    
    # 2. Load Checkpoint
    completed_ids = set()
    if Config.CHECKPOINT_FILE.exists():
        print("üîÑ Checking checkpoint file...")
        with open(Config.CHECKPOINT_FILE, "r", encoding="utf-8") as f:
            # [FIX 1] √âp ki·ªÉu d√≤ng ƒë·ªçc ƒë∆∞·ª£c sang String v√† strip()
            completed_ids = set(str(line.strip()) for line in f if line.strip())
        print(f"üìä Found {len(completed_ids)} completed chunks.")

    # 3. Filter Data
    df_to_process = df[~df['chunk_id'].isin(completed_ids)]
    total_records = len(df_to_process)
    
    if total_records == 0:
        print("üéâ ALL DONE! Everything is indexed.")
        return

    print(f"üî• Remaining to index: {total_records} chunks")

    # 2. Setup Qdrant
    print(f"üîå Connecting to Qdrant at {Config.QDRANT_URL}...")
    client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)

    is_resuming = len(completed_ids) > 0
    
    if is_resuming:
        print("‚ö†Ô∏è DETECTED RESUME MODE: Ignoring Config.FORCE_RECREATE.")
        print("   -> Will NOT delete existing collection.")
    else:
        # Ch·ªâ cho ph√©p x√≥a n·∫øu KH√îNG ph·∫£i l√† resume (Start Fresh)
        if Config.FORCE_RECREATE and client.collection_exists(Config.COLLECTION_NAME):
            print(f"üóëÔ∏è FRESH START: Deleting collection '{Config.COLLECTION_NAME}'...")
            client.delete_collection(Config.COLLECTION_NAME)
            # X√≥a lu√¥n file checkpoint (n·∫øu c√≥ r√°c) ƒë·ªÉ s·∫°ch s·∫Ω
            if Config.CHECKPOINT_FILE.exists():
                os.remove(Config.CHECKPOINT_FILE)

    if not client.collection_exists(Config.COLLECTION_NAME):
        print(f"üÜï Creating collection '{Config.COLLECTION_NAME}'...")
        
        # L·∫•y th·ª≠ 1 vector ƒë·ªÉ check dimension (ho·∫∑c hardcode 1024 n·∫øu bi·∫øt ch·∫Øc)
        # ·ªû ƒë√¢y ta g·ªçi th·ª≠ 1 request th·∫≠t ƒë·ªÉ l·∫•y size chu·∫©n
        async with aiohttp.ClientSession() as temp_session:
            print("üß™ Testing API to get vector size...")
            sample_vec = await get_embedding_async(temp_session, "test dimension")
            if not sample_vec:
                print("‚ùå Fatal: Kh√¥ng g·ªçi ƒë∆∞·ª£c API Embedding ƒë·ªÉ l·∫•y size. D·ª´ng ch∆∞∆°ng tr√¨nh.")
                return
            vec_size = len(sample_vec)
            print(f"‚úÖ Vector Size detected: {vec_size}")

        client.create_collection(
            collection_name=Config.COLLECTION_NAME,
            vectors_config=models.VectorParams(
                size=vec_size, 
                distance=models.Distance.COSINE
            ),
            # T·ªëi ∆∞u HNSW config t·ª´ Config
            hnsw_config=models.HnswConfigDiff(
                m=Config.HNSW_M,
                ef_construct=Config.HNSW_EF_CONSTRUCT
            )
        )

    # 3. Setup Queue & Workers
    queue = asyncio.Queue()
    
    # N·∫°p data v√†o queue
    for record in records:
        queue.put_nowait(record)
    
    failed_log = []
    
    # Kh·ªüi t·∫°o Session
    # Set limit connection pool cao h∆°n s·ªë worker
    conn = aiohttp.TCPConnector(limit=NUM_WORKERS + 10) 
    async with aiohttp.ClientSession(connector=conn) as session:
        
        # Progress Bar
        pbar = tqdm(total=total_records, desc="Indexing", unit="chunk")
        
        # T·∫°o Workers
        workers = []
        for _ in range(NUM_WORKERS):
            w = asyncio.create_task(worker(queue, session, client, pbar, failed_log))
            workers.append(w)

        # Ch·ªù queue ƒë∆∞·ª£c x·ª≠ l√Ω h·∫øt
        await queue.join()

        # G·ª≠i t√≠n hi·ªáu d·ª´ng (None) cho t·ª´ng worker
        for _ in range(NUM_WORKERS):
            await queue.put(None)
        
        # Ch·ªù t·∫•t c·∫£ worker t·∫Øt h·∫≥n
        await asyncio.gather(*workers)
        
        pbar.close()

    # 4. K·∫øt th√∫c
    print("\n‚úÖ INDEXING COMPLETED!")
    
    if failed_log:
        print(f"‚ö†Ô∏è C√≥ {len(failed_log)} chunks b·ªã l·ªói. ƒêang l∆∞u log...")
        failed_file = Config.LOGS_DIR / "indexing_failed_ids.json"
        with open(failed_file, "w", encoding="utf-8") as f:
            json.dump(failed_log, f, ensure_ascii=False, indent=2)
        print(f"üìÑ Saved failed IDs to {failed_file}")

if __name__ == "__main__":
    # Fix l·ªói Event Loop tr√™n Windows
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        
    asyncio.run(main())