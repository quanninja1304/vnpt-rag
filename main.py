import asyncio
import aiohttp
import pandas as pd
import json
import re
import pickle
import sys
import os
from pathlib import Path
from aiolimiter import AsyncLimiter
from qdrant_client import QdrantClient
from underthesea import word_tokenize
from config import Config

# ==============================================================================
# 1. C·∫§U H√åNH & QUOTA MANAGER
# ==============================================================================
# Rate Limit (T·ªëc ƒë·ªô)
LIMITER_EMBED = AsyncLimiter(300, 60) # 300 req/ph√∫t
LIMITER_LLM = AsyncLimiter(100, 60)   # 100 req/ph√∫t

# Concurrency Limit (S·ªë lu·ªìng song song - Tr√°nh tr√†n RAM)
MAX_CONCURRENT_TASKS = 15 

# Quota Limit (T·ªïng s·ªë request/ng√†y - Tr√°nh h·∫øt ti·ªÅn)
# Tr·ª´ hao 20 request ƒë·ªÉ test ho·∫∑c l·ªói m·∫°ng
MAX_QUOTA_LARGE = 480 
MAX_QUOTA_SMALL = 980

class QuotaManager:
    """Qu·∫£n l√Ω s·ªë l∆∞·ª£ng request ƒë·ªÉ kh√¥ng b·ªã h·∫øt quota gi·ªØa ch·ª´ng"""
    def __init__(self):
        self.large_used = 0
        self.small_used = 0
        self.lock = asyncio.Lock()

    async def check_and_increment(self, model_type):
        async with self.lock:
            if model_type == Config.LLM_MODEL_LARGE:
                if self.large_used < MAX_QUOTA_LARGE:
                    self.large_used += 1
                    return True # Cho ph√©p d√πng Large
                return False # H·∫øt quota Large
            else:
                self.small_used += 1
                return True # Small c·ª© d√πng (ho·∫∑c check limit n·∫øu c·∫ßn)

QUOTA_MGR = QuotaManager()

TOP_K = 6
ALPHA_VECTOR = 0.7
BM25_FILE = Config.OUTPUT_DIR / "bm25_index.pkl"

# ==============================================================================
# 2. CLIENT G·ªåI API (ROBUST)
# ==============================================================================
async def call_llm_generic(session, messages, model_name, max_tokens=1024, retry=3):
    creds = Config.VNPT_CREDENTIALS.get(model_name)
    url = f"{Config.VNPT_API_URL}/{model_name.replace('_', '-')}"
    
    headers = {
        'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}',
        'Token-id': creds['token_id'],
        'Token-key': creds['token_key'],
        'Content-Type': 'application/json'
    }
    
    payload = {
        "model": model_name,
        "messages": messages,
        "temperature": 0.1,
        "top_p": 0.95,
        "max_completion_tokens": max_tokens
    }

    for attempt in range(retry):
        try:
            async with LIMITER_LLM:
                # [FIX CONNECTION] force_close=True ƒë·ªÉ tr√°nh l·ªói Server disconnected
                async with session.post(url, json=payload, headers=headers, timeout=45) as resp:
                    if resp.status == 200:
                        try:
                            data = await resp.json()
                            # [FIX CRASH] Ki·ªÉm tra k·ªπ xem c√≥ 'choices' kh√¥ng
                            if 'choices' in data and len(data['choices']) > 0:
                                return data['choices'][0]['message']['content']
                            else:
                                # API tr·∫£ 200 nh∆∞ng n·ªôi dung l·ªói
                                print(f"‚ö†Ô∏è API {model_name} Weird Response: {data}")
                                return None
                        except Exception as json_err:
                            print(f"‚ùå JSON Parse Error: {json_err}")
                            return None
                    
                    text_resp = await resp.text()
                    # Retry n·∫øu l·ªói Rate Limit ho·∫∑c Server
                    if resp.status in [429, 500, 502, 503] or (resp.status == 401 and "Rate limit" in text_resp):
                        wait = 2 * (attempt + 1)
                        print(f"‚ö†Ô∏è {model_name} Busy ({resp.status}). Wait {wait}s...")
                        await asyncio.sleep(wait)
                        continue
                    else:
                        print(f"‚ùå Error {model_name} ({resp.status}): {text_resp}")
                        return None
        except Exception as e:
            # L·ªói m·∫°ng thu·∫ßn t√∫y (disconnect)
            print(f"‚ö†Ô∏è Net Error {model_name}: {str(e)[:50]}...") # In ng·∫Øn g·ªçn
            await asyncio.sleep(1)
            
    return None

async def get_embedding_async(session, text):
    model = Config.MODEL_EMBEDDING_API
    creds = Config.VNPT_CREDENTIALS.get(model)
    headers = {'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}', 'Token-id': creds['token_id'], 'Token-key': creds['token_key'], 'Content-Type': 'application/json'}
    payload = {"model": model, "input": text, "encoding_format": "float"}

    for attempt in range(3):
        async with LIMITER_EMBED:
            try:
                async with session.post(Config.VNPT_EMBEDDING_URL, json=payload, headers=headers, timeout=30) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        if 'data' in data: return data['data'][0]['embedding']
                    
                    text_resp = await resp.text()
                    if resp.status in [429, 500] or (resp.status == 401 and "Rate limit" in text_resp):
                        await asyncio.sleep(2 * (attempt + 1))
                        continue
            except:
                await asyncio.sleep(1)
    return None

# ==============================================================================
# 3. HYBRID RETRIEVER
# ==============================================================================
class HybridRetriever:
    def __init__(self):
        self.client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)
        self.bm25_data = None
        if BM25_FILE.exists():
            with open(BM25_FILE, "rb") as f: self.bm25_data = pickle.load(f)

    async def search(self, session, query, top_k=TOP_K):
        # 1. Vector
        query_vec = await get_embedding_async(session, query)
        vec_hits = []
        if query_vec:
            try:
                vec_hits = self.client.search(collection_name=Config.COLLECTION_NAME, query_vector=query_vec, limit=top_k, with_payload=True)
            except: pass
        
        # 2. BM25
        bm25_hits = []
        if self.bm25_data:
            tokens = word_tokenize(query.lower())
            scores = self.bm25_data['bm25_obj'].get_scores(tokens)
            top_idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
            for idx in top_idxs:
                if scores[idx] > 0.8:
                    bm25_hits.append({"id": self.bm25_data['chunk_ids'][idx], "score": scores[idx], "text": self.bm25_data['texts'][idx], "title": self.bm25_data['titles'][idx]})

        # 3. Fusion
        fused = {}
        max_v = max([h.score for h in vec_hits]) if vec_hits else 1.0
        for h in vec_hits:
            fused[h.payload['chunk_id']] = {"text": h.payload['text'], "title": h.payload['title'], "score": (h.score/max_v)*ALPHA_VECTOR}
            
        max_b = max([h['score'] for h in bm25_hits]) if bm25_hits else 1.0
        for h in bm25_hits:
            norm = (h['score']/max_b)*(1-ALPHA_VECTOR)
            cid = h['id']
            if cid in fused: fused[cid]['score'] += norm
            else: fused[cid] = {"text": h['text'], "title": h['title'], "score": norm}
            
        return sorted(fused.values(), key=lambda x: x['score'], reverse=True)[:top_k]

# ==============================================================================
# 4. LOGIC TR·∫¢ L·ªúI & QUOTA ROUTING
# ==============================================================================
def is_sensitive(question):
    blacklist = ["sex", "khi√™u d√¢m", "b·∫°o ƒë·ªông", "l·∫≠t ƒë·ªï", "ph·∫£n ƒë·ªông", "gi·∫øt ng∆∞·ªùi", "kh·ªßng b·ªë"]
    return any(w in question.lower() for w in blacklist)

def build_advanced_prompt(question, options_text, docs):
    context_str = ""
    for i, doc in enumerate(docs):
        context_str += f"--- T√ÄI LI·ªÜU #{i+1} ({doc['title']}) ---\n{doc['text']}\n\n"

    system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n gia v·ªÅ Vi·ªát Nam. Nhi·ªám v·ª•:
1. ƒê·ªçc t√†i li·ªáu tham kh·∫£o.
2. Tr·∫£ l·ªùi c√¢u h·ªèi tr·∫Øc nghi·ªám.
3. N·∫øu t√†i li·ªáu m√¢u thu·∫´n th·ªùi gian, ∆ØU TI√äN TH√îNG TIN M·ªöI NH·∫§T (2024-2025)."""

    user_prompt = f"""
D·ªÆ LI·ªÜU:
{context_str}

C√ÇU H·ªéI: {question}
L·ª∞A CH·ªåN:
{options_text}

H∆Ø·ªöNG D·∫™N:
Suy lu·∫≠n t·ª´ng b∆∞·ªõc:
1. T√¨m t·ª´ kh√≥a & m·ªëc th·ªùi gian.
2. T√¨m th√¥ng tin trong t√†i li·ªáu.
3. So s√°nh l·ª±a ch·ªçn.
4. CH·ªà TR·∫¢ V·ªÄ K√ù T·ª∞ ƒê√ÅP √ÅN (A, B, C, D)."""
    return [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

async def generate_answer(session, question, options_text, docs):
    messages = build_advanced_prompt(question, options_text, docs)
    
    # ∆Ø·ªõc l∆∞·ª£ng ƒë·ªô d√†i Context
    context_len = sum([len(d['text']) for d in docs]) * 1.5
    total_tokens = context_len + 1000 
    
    # --- LOGIC CH·ªåN MODEL (SMART ROUTER) ---
    selected_model = Config.LLM_MODEL_SMALL # M·∫∑c ƒë·ªãnh d√πng Small (cho an to√†n quota)
    
    # 1. N·∫øu Context qu√° l·ªõn -> B·∫ÆT BU·ªòC d√πng Small
    if total_tokens > 18000:
        use_large = False
    else:
        # 2. N·∫øu Context v·ª´a ph·∫£i -> Check Quota Large xem c√≤n kh√¥ng?
        use_large = await QUOTA_MGR.check_and_increment(Config.LLM_MODEL_LARGE)
    
    if use_large:
        selected_model = Config.LLM_MODEL_LARGE
    
    # G·ªçi Model
    answer = await call_llm_generic(session, messages, selected_model)
    
    # Fallback: N·∫øu Large fail -> G·ªçi Small
    if answer is None and selected_model == Config.LLM_MODEL_LARGE:
        print("üîÑ Fallback to SMALL model.")
        await QUOTA_MGR.check_and_increment(Config.LLM_MODEL_SMALL) # Count usage small
        answer = await call_llm_generic(session, messages, Config.LLM_MODEL_SMALL)
        
    return answer if answer else "A"

def extract_key(text):
    match = re.search(r'(?:ƒë√°p √°n|ch·ªçn)[:\s]*([A-D])', text, re.IGNORECASE)
    if match: return match.group(1).upper()
    matches = re.findall(r'\b([A-D])\b', text)
    return matches[-1].upper() if matches else "A"

# ==============================================================================
# 5. MAIN PIPELINE (ƒê√É S·ª¨A CONCURRENCY & INPUT)
# ==============================================================================
async def process_row_safe(sem, session, retriever, row):
    """Wrapper c√≥ Semaphore ƒë·ªÉ gi·ªõi h·∫°n s·ªë lu·ªìng ch·∫°y c√πng l√∫c"""
    async with sem:
        try:
            # Parse input JSON (Key c√≥ th·ªÉ kh√°c nhau t√πy file, c·∫ßn linh ho·∫°t)
            qid = row.get('id', row.get('qid', 'unknown'))
            question = row.get('question', '')
            
            # [FIX INPUT] L·∫•y options t·ª´ JSON (th∆∞·ªùng l√† list ho·∫∑c c√°c field r·ªùi)
            # Gi·∫£ s·ª≠ format: "option_1": "...", "option_2": "..." HO·∫∂C "options": ["...", "..."]
            if 'options' in row and isinstance(row['options'], list):
                opts = row['options']
                options_text = f"A. {opts[0]}\nB. {opts[1]}\nC. {opts[2]}\nD. {opts[3]}"
            else:
                options_text = "\n".join([f"{k}. {row.get(f'option_{i}', '')}" for i, k in enumerate(['A','B','C','D'], 1)])

            if is_sensitive(question):
                return {"id": qid, "answer": "A"}

            docs = await retriever.search(session, question, top_k=TOP_K)
            raw_ans = await generate_answer(session, question, options_text, docs)
            final_key = extract_key(raw_ans)
            
            # Log ti·∫øn ƒë·ªô nh·∫π
            print(f"‚úÖ Q:{qid} | Ans:{final_key} | Docs:{len(docs)} | LargeUsed:{QUOTA_MGR.large_used}")
            return {"id": qid, "answer": final_key}
            
        except Exception as e:
            print(f"‚ùå Error processing QID {row.get('id')}: {e}")
            return {"id": row.get('id'), "answer": "A"}

async def main():
    input_file = Config.BASE_DIR / "data" / "test.json" 

    print(f"üìÇ Reading JSON: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # [FIX 2] Semaphore ƒë·ªÉ ki·ªÉm so√°t concurrency
    sem = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
    retriever = HybridRetriever()
    
    print(f"üî• Processing {len(data)} questions with {MAX_CONCURRENT_TASKS} concurrent tasks...")
    
    connector = aiohttp.TCPConnector(force_close=True, limit=MAX_CONCURRENT_TASKS)

    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [process_row_safe(sem, session, retriever, row) for row in data]
        results = await asyncio.gather(*tasks)
    
    # [OUTPUT] Format CSV theo y√™u c·∫ßu: qid, answer
    output_csv = Config.BASE_DIR / "output" / "pred.csv"
    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    out_df = pd.DataFrame(results)
    # Rename c·ªôt id -> qid n·∫øu c·∫ßn thi·∫øt cho ƒë√∫ng format n·ªôp
    if 'id' in out_df.columns:
        out_df.rename(columns={'id': 'qid'}, inplace=True)
        
    out_df = out_df[['qid', 'answer']] # Ch·ªâ l·∫•y 2 c·ªôt c·∫ßn thi·∫øt
    out_df.to_csv(output_csv, index=False)
    print(f"üíæ Done! Saved to {output_csv}")

if __name__ == "__main__":
    if sys.platform == 'win32': asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())

