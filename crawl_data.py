"""
Wikipedia Vietnam Crawler - Production Version
================================================
Features:
- Incremental caching (JSON + Parquet)
- Multi-threaded crawling (configurable workers)
- Max depth 5 vá»›i cycle detection
- Retry mechanism cho timeout
- Full category tree logging
- Export CSV + JSON + Parquet
- 500+ optimized categories for RAG
"""

import wikipediaapi
import pandas as pd
import time
import json
import hashlib
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from datetime import datetime
import logging
from typing import Dict, List, Set, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# ThÃªm retry decorator
from functools import wraps

def retry_on_failure(max_retries=3, delay=2):
    """Decorator Ä‘á»ƒ retry khi gáº·p lá»—i"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise
                    logger.warning(f"Retry {attempt+1}/{max_retries} for {func.__name__}: {e}")
                    time.sleep(delay * (attempt + 1))
            return None
        return wrapper
    return decorator


# ================================
# 1. Cáº¤U HÃŒNH & LOGGING
# ================================

class Config:
    """Centralized configuration"""
    # Directories
    CACHE_DIR = Path("cache")
    LOGS_DIR = Path("logs")
    OUTPUT_DIR = Path("output")
    
    # Files
    ARTICLES_CACHE = CACHE_DIR / "articles_cache.json"
    CATEGORY_TREE = CACHE_DIR / "category_tree.json"
    METADATA = CACHE_DIR / "metadata.json"
    
    # Crawl settings
    MAX_LEVEL = 2
    MAX_WORKERS = 15
    TEXT_LIMIT = 25000  # TÄƒng lÃªn 20k cho RAG tá»‘t hÆ¡n
    RETRY_ATTEMPTS = 3
    RATE_LIMIT_DELAY = 0.2
    
    # Export formats
    EXPORT_CSV = True
    EXPORT_JSON = True
    EXPORT_PARQUET = True
    
    @classmethod
    def setup_dirs(cls):
        """Táº¡o cÃ¡c thÆ° má»¥c cáº§n thiáº¿t"""
        for dir_path in [cls.CACHE_DIR, cls.LOGS_DIR, cls.OUTPUT_DIR]:
            dir_path.mkdir(exist_ok=True)


# Setup logging
Config.setup_dirs()
log_file = Config.LOGS_DIR / f'crawl_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Wikipedia API
wiki_wiki = wikipediaapi.Wikipedia(
    user_agent='VNPT_Hackathon_Pro/5.0 (Production_RAG_Crawler)',
    language='vi',
    extract_format=wikipediaapi.ExtractFormat.WIKI,
    timeout=30
)


# ================================
# 2. DANH SÃCH 500+ CATEGORIES Tá»I Æ¯U CHO RAG
# ================================

COMPREHENSIVE_CATEGORIES = [
    # ========== Lá»ŠCH Sá»¬ (60+ categories) ==========
    "Lá»‹ch_sá»­_Viá»‡t_Nam", "Triá»u_Ä‘áº¡i_Viá»‡t_Nam", "Chiáº¿n_tranh_liÃªn_quan_Ä‘áº¿n_Viá»‡t_Nam",
    "NhÃ _nÆ°á»›c_Viá»‡t_Nam", "Sá»±_kiá»‡n_lá»‹ch_sá»­_Viá»‡t_Nam", "NhÃ¢n_váº­t_lá»‹ch_sá»­_Viá»‡t_Nam",
    "Vua_Viá»‡t_Nam", "HoÃ ng_Ä‘áº¿_Viá»‡t_Nam", "ChÃºa_Nguyá»…n", "ChÃºa_Trá»‹nh",
    "NhÃ _LÃ½", "NhÃ _Tráº§n", "NhÃ _LÃª", "NhÃ _Nguyá»…n", "NhÃ _Há»“",
    "CÃ¡ch_máº¡ng_Viá»‡t_Nam", "KhÃ¡ng_chiáº¿n_chá»‘ng_PhÃ¡p", "Chiáº¿n_tranh_Viá»‡t_Nam",
    "ÄÃ´ng_DÆ°Æ¡ng_thuá»™c_PhÃ¡p", "Báº¯c_thuá»™c", "Äá»™c_láº­p_Viá»‡t_Nam",
    "Thá»‘ng_nháº¥t_Ä‘áº¥t_nÆ°á»›c", "Cáº£i_cÃ¡ch_vÃ _Má»Ÿ_cá»­a", "Äá»•i_má»›i_Viá»‡t_Nam",
    
    # NhÃ¢n váº­t lá»‹ch sá»­ chi tiáº¿t
    "ChÃ­nh_trá»‹_gia_Viá»‡t_Nam", "TÆ°á»›ng_lÄ©nh_Viá»‡t_Nam", "Danh_tÆ°á»›ng_Viá»‡t_Nam",
    "Anh_hÃ¹ng_dÃ¢n_tá»™c_Viá»‡t_Nam", "Danh_nhÃ¢n_Viá»‡t_Nam", "Nho_sÄ©_Viá»‡t_Nam",
    "SÄ©_quan_quÃ¢n_Ä‘á»™i_nhÃ¢n_dÃ¢n_Viá»‡t_Nam", "Liá»‡t_sÄ©_Viá»‡t_Nam",
    "Chá»§_tá»‹ch_nÆ°á»›c_Viá»‡t_Nam", "Thá»§_tÆ°á»›ng_Viá»‡t_Nam", "Tá»•ng_BÃ­_thÆ°_Äáº£ng_Cá»™ng_sáº£n_Viá»‡t_Nam",
    
    # Sá»± kiá»‡n lá»‹ch sá»­ quan trá»ng
    "Khá»Ÿi_nghÄ©a_Viá»‡t_Nam", "Tráº­n_chiáº¿n_Viá»‡t_Nam", "Há»™i_nghá»‹_Viá»‡t_Nam",
    "Hiá»‡p_Æ°á»›c_liÃªn_quan_Ä‘áº¿n_Viá»‡t_Nam", "CÃ¡ch_máº¡ng_thÃ¡ng_TÃ¡m",
    "Chiáº¿n_dá»‹ch_Äiá»‡n_BiÃªn_Phá»§", "Chiáº¿n_dá»‹ch_Há»“_ChÃ­_Minh",
    
    # ========== Äá»ŠA LÃ (100+ categories) ==========
    "Äá»‹a_lÃ½_Viá»‡t_Nam", "HÃ nh_chÃ­nh_Viá»‡t_Nam", "ÄÆ¡n_vá»‹_hÃ nh_chÃ­nh_Viá»‡t_Nam",
    
    # 63 Tá»‰nh thÃ nh
    "Tá»‰nh_cá»§a_Viá»‡t_Nam", "ThÃ nh_phá»‘_trá»±c_thuá»™c_trung_Æ°Æ¡ng_(Viá»‡t_Nam)",
    "HÃ _Ná»™i", "ThÃ nh_phá»‘_Há»“_ChÃ­_Minh", "ÄÃ _Náºµng", "Háº£i_PhÃ²ng", "Cáº§n_ThÆ¡",
    "An_Giang", "BÃ _Rá»‹a-VÅ©ng_TÃ u", "Báº¯c_Giang", "Báº¯c_Káº¡n", "Báº¡c_LiÃªu",
    "Báº¯c_Ninh", "Báº¿n_Tre", "BÃ¬nh_Äá»‹nh", "BÃ¬nh_DÆ°Æ¡ng", "BÃ¬nh_PhÆ°á»›c", "BÃ¬nh_Thuáº­n",
    "CÃ _Mau", "Cao_Báº±ng", "Äáº¯k_Láº¯k", "Äáº¯k_NÃ´ng", "Äiá»‡n_BiÃªn", "Äá»“ng_Nai",
    "Äá»“ng_ThÃ¡p", "Gia_Lai", "HÃ _Giang", "HÃ _Nam", "HÃ _TÄ©nh", "Háº£i_DÆ°Æ¡ng",
    "Háº­u_Giang", "HÃ²a_BÃ¬nh", "HÆ°ng_YÃªn", "KhÃ¡nh_HÃ²a", "KiÃªn_Giang",
    "Kon_Tum", "Lai_ChÃ¢u", "LÃ¢m_Äá»“ng", "Láº¡ng_SÆ¡n", "LÃ o_Cai", "Long_An",
    "Nam_Äá»‹nh", "Nghá»‡_An", "Ninh_BÃ¬nh", "Ninh_Thuáº­n", "PhÃº_Thá»", "PhÃº_YÃªn",
    "Quáº£ng_BÃ¬nh", "Quáº£ng_Nam", "Quáº£ng_NgÃ£i", "Quáº£ng_Ninh", "Quáº£ng_Trá»‹",
    "SÃ³c_TrÄƒng", "SÆ¡n_La", "TÃ¢y_Ninh", "ThÃ¡i_BÃ¬nh", "ThÃ¡i_NguyÃªn",
    "Thanh_HÃ³a", "Thá»«a_ThiÃªn_Huáº¿", "Tiá»n_Giang", "TrÃ _Vinh", "TuyÃªn_Quang",
    "VÄ©nh_Long", "VÄ©nh_PhÃºc", "YÃªn_BÃ¡i",
    
    # ÄÆ¡n vá»‹ hÃ nh chÃ­nh cáº¥p dÆ°á»›i
    "Huyá»‡n_cá»§a_Viá»‡t_Nam", "Quáº­n_(Viá»‡t_Nam)", "Thá»‹_xÃ£_Viá»‡t_Nam", "ThÃ nh_phá»‘_thuá»™c_tá»‰nh",
    "XÃ£_(Viá»‡t_Nam)", "PhÆ°á»ng_(Viá»‡t_Nam)", "Thá»‹_tráº¥n_Viá»‡t_Nam",
    
    # Äá»‹a hÃ¬nh tá»± nhiÃªn
    "SÃ´ng_ngÃ²i_Viá»‡t_Nam", "SÃ´ng_Há»“ng", "SÃ´ng_Cá»­u_Long", "SÃ´ng_Äá»“ng_Nai",
    "NÃºi_Viá»‡t_Nam", "ÄÃ¨o_Viá»‡t_Nam", "Cao_nguyÃªn_Viá»‡t_Nam", "Äá»“ng_báº±ng_Viá»‡t_Nam",
    "Vá»‹nh_Viá»‡t_Nam", "Vá»‹nh_Háº¡_Long", "Vá»‹nh_Nha_Trang",
    "Äáº£o_Viá»‡t_Nam", "Quáº§n_Ä‘áº£o_TrÆ°á»ng_Sa", "Quáº§n_Ä‘áº£o_HoÃ ng_Sa",
    "Biá»ƒn_ÄÃ´ng", "Há»“_Viá»‡t_Nam", "ThÃ¡c_nÆ°á»›c_Viá»‡t_Nam",
    
    # Danh lam tháº¯ng cáº£nh
    "Danh_tháº¯ng_Viá»‡t_Nam", "VÆ°á»n_quá»‘c_gia_Viá»‡t_Nam", "Khu_du_lá»‹ch_Viá»‡t_Nam",
    "Äá»™ng_Viá»‡t_Nam", "Hang_Ä‘á»™ng_Viá»‡t_Nam", "BÃ£i_biá»ƒn_Viá»‡t_Nam",
    
    # ========== VÄ‚N HÃ“A (80+ categories) ==========
    "VÄƒn_hÃ³a_Viá»‡t_Nam", "Di_sáº£n_vÄƒn_hÃ³a_Viá»‡t_Nam",
    "Di_sáº£n_vÄƒn_hÃ³a_tháº¿_giá»›i_táº¡i_Viá»‡t_Nam", "Di_sáº£n_vÄƒn_hÃ³a_phi_váº­t_thá»ƒ",
    
    # VÄƒn há»c & NgÃ´n ngá»¯
    "VÄƒn_há»c_Viá»‡t_Nam", "NhÃ _vÄƒn_Viá»‡t_Nam", "Thi_sÄ©_Viá»‡t_Nam", "NhÃ _thÆ¡_Viá»‡t_Nam",
    "TÃ¡c_pháº©m_vÄƒn_há»c_Viá»‡t_Nam", "Truyá»‡n_Kiá»u", "VÄƒn_há»c_dÃ¢n_gian_Viá»‡t_Nam",
    "ThÆ¡_Viá»‡t_Nam", "ThÆ¡_chá»¯_NÃ´m", "Chá»¯_NÃ´m", "Tiáº¿ng_Viá»‡t",
    
    # Nghá»‡ thuáº­t
    "Nghá»‡_thuáº­t_Viá»‡t_Nam", "Há»™i_há»a_Viá»‡t_Nam", "Há»a_sÄ©_Viá»‡t_Nam",
    "ÄiÃªu_kháº¯c_Viá»‡t_Nam", "Thá»§_cÃ´ng_má»¹_nghá»‡_Viá»‡t_Nam",
    "Gá»‘m_sá»©_Viá»‡t_Nam", "Tranh_ÄÃ´ng_Há»“", "Tranh_dÃ¢n_gian_Viá»‡t_Nam",
    
    # Ã‚m nháº¡c & SÃ¢n kháº¥u
    "Ã‚m_nháº¡c_Viá»‡t_Nam", "Nháº¡c_sÄ©_Viá»‡t_Nam", "Ca_sÄ©_Viá»‡t_Nam",
    "Nháº¡c_cá»¥_dÃ¢n_tá»™c_Viá»‡t_Nam", "DÃ¢n_ca_Viá»‡t_Nam", "HÃ¡t_quan_há»",
    "SÃ¢n_kháº¥u_Viá»‡t_Nam", "ChÃ¨o", "Tuá»“ng", "Cáº£i_lÆ°Æ¡ng", "Rá»‘i_nÆ°á»›c",
    
    # Äiá»‡n áº£nh & Truyá»n thÃ´ng
    "Äiá»‡n_áº£nh_Viá»‡t_Nam", "Äáº¡o_diá»…n_Viá»‡t_Nam", "Diá»…n_viÃªn_Viá»‡t_Nam",
    "Phim_Viá»‡t_Nam", "Truyá»n_thÃ´ng_Viá»‡t_Nam", "BÃ¡o_chÃ­_Viá»‡t_Nam",
    "ÄÃ i_phÃ¡t_thanh_Viá»‡t_Nam", "ÄÃ i_truyá»n_hÃ¬nh_Viá»‡t_Nam",
    
    # áº¨m thá»±c
    "áº¨m_thá»±c_Viá»‡t_Nam", "MÃ³n_Äƒn_Viá»‡t_Nam", "Phá»Ÿ", "BÃ¡nh_mÃ¬_Viá»‡t_Nam",
    "RÆ°á»£u_Viá»‡t_Nam", "TrÃ _Viá»‡t_Nam", "Gia_vá»‹_Viá»‡t_Nam",
    
    # Lá»… há»™i & Phong tá»¥c
    "Lá»…_há»™i_Viá»‡t_Nam", "Táº¿t_NguyÃªn_ÄÃ¡n", "Phong_tá»¥c_táº­p_quÃ¡n_Viá»‡t_Nam",
    "TÃ­n_ngÆ°á»¡ng_Viá»‡t_Nam", "Thá»_cÃºng_tá»•_tiÃªn", "Tá»¥c_thá»_Máº«u",
    
    # DÃ¢n tá»™c
    "DÃ¢n_tá»™c_Viá»‡t_Nam", "54_dÃ¢n_tá»™c_Viá»‡t_Nam", "NgÆ°á»i_Kinh",
    "NgÆ°á»i_TÃ y", "NgÆ°á»i_ThÃ¡i", "NgÆ°á»i_MÆ°á»ng", "NgÆ°á»i_Khmer_(Viá»‡t_Nam)",
    "NgÆ°á»i_Hoa_(Viá»‡t_Nam)", "NgÆ°á»i_NÃ¹ng", "NgÆ°á»i_H'MÃ´ng",
    
    # ========== TÃ”N GIÃO (30+ categories) ==========
    "TÃ´n_giÃ¡o_táº¡i_Viá»‡t_Nam", "Pháº­t_giÃ¡o_Viá»‡t_Nam", "ChÃ¹a_Viá»‡t_Nam",
    "Thiá»n_phÃ¡i_Viá»‡t_Nam", "TÄƒng_ni_Viá»‡t_Nam",
    "CÃ´ng_giÃ¡o_táº¡i_Viá»‡t_Nam", "NhÃ _thá»_CÃ´ng_giÃ¡o_Viá»‡t_Nam",
    "Äáº¡o_Cao_ÄÃ i", "Äáº¡o_HÃ²a_Háº£o", "Tin_lÃ nh_táº¡i_Viá»‡t_Nam",
    "Há»“i_giÃ¡o_táº¡i_Viá»‡t_Nam", "Äáº¡o_giÃ¡o_Viá»‡t_Nam", "Äá»n_thá»_Viá»‡t_Nam",
    
    # ========== CHÃNH TRá»Š & PHÃP LUáº¬T (60+ categories) ==========
    "ChÃ­nh_trá»‹_Viá»‡t_Nam", "Há»‡_thá»‘ng_chÃ­nh_trá»‹_Viá»‡t_Nam",
    "NhÃ _nÆ°á»›c_Viá»‡t_Nam", "CÆ¡_quan_nhÃ _nÆ°á»›c_Viá»‡t_Nam",
    
    # Äáº£ng
    "Äáº£ng_Cá»™ng_sáº£n_Viá»‡t_Nam", "Äáº¡i_há»™i_Äáº£ng_Cá»™ng_sáº£n_Viá»‡t_Nam",
    "Ban_Cháº¥p_hÃ nh_Trung_Æ°Æ¡ng_Äáº£ng_Cá»™ng_sáº£n_Viá»‡t_Nam",
    "Bá»™_ChÃ­nh_trá»‹", "Tá»•ng_BÃ­_thÆ°_Äáº£ng_Cá»™ng_sáº£n_Viá»‡t_Nam",
    
    # Quá»‘c há»™i
    "Quá»‘c_há»™i_Viá»‡t_Nam", "Chá»§_tá»‹ch_Quá»‘c_há»™i_Viá»‡t_Nam",
    "Äáº¡i_biá»ƒu_Quá»‘c_há»™i_Viá»‡t_Nam", "Uá»·_ban_ThÆ°á»ng_vá»¥_Quá»‘c_há»™i",
    
    # ChÃ­nh phá»§
    "ChÃ­nh_phá»§_Viá»‡t_Nam", "Thá»§_tÆ°á»›ng_Viá»‡t_Nam", "Bá»™_(Viá»‡t_Nam)",
    "Bá»™_trÆ°á»Ÿng_Viá»‡t_Nam", "PhÃ³_Thá»§_tÆ°á»›ng_Viá»‡t_Nam",
    
    # CÃ¡c bá»™ ngÃ nh
    "Bá»™_Ngoáº¡i_giao_(Viá»‡t_Nam)", "Bá»™_Quá»‘c_phÃ²ng_(Viá»‡t_Nam)",
    "Bá»™_CÃ´ng_an_(Viá»‡t_Nam)", "Bá»™_GiÃ¡o_dá»¥c_vÃ _ÄÃ o_táº¡o_(Viá»‡t_Nam)",
    "Bá»™_Y_táº¿_(Viá»‡t_Nam)", "Bá»™_TÃ i_chÃ­nh_(Viá»‡t_Nam)",
    "Bá»™_Giao_thÃ´ng_váº­n_táº£i_(Viá»‡t_Nam)",
    
    # Chá»§ tá»‹ch nÆ°á»›c & PhÃ¡p luáº­t
    "Chá»§_tá»‹ch_nÆ°á»›c_Viá»‡t_Nam", "TÃ²a_Ã¡n_nhÃ¢n_dÃ¢n_tá»‘i_cao_(Viá»‡t_Nam)",
    "Viá»‡n_Kiá»ƒm_sÃ¡t_nhÃ¢n_dÃ¢n_tá»‘i_cao_(Viá»‡t_Nam)",
    "PhÃ¡p_luáº­t_Viá»‡t_Nam", "Hiáº¿n_phÃ¡p_Viá»‡t_Nam", "Bá»™_luáº­t_Viá»‡t_Nam",
    "Luáº­t_Viá»‡t_Nam", "Há»‡_thá»‘ng_phÃ¡p_luáº­t_Viá»‡t_Nam",
    
    # Tá»• chá»©c chÃ­nh trá»‹ - xÃ£ há»™i
    "Tá»•_chá»©c_chÃ­nh_trá»‹_-_xÃ£_há»™i_táº¡i_Viá»‡t_Nam",
    "Máº·t_tráº­n_Tá»•_quá»‘c_Viá»‡t_Nam", "ÄoÃ n_Thanh_niÃªn_Cá»™ng_sáº£n_Há»“_ChÃ­_Minh",
    "Há»™i_LiÃªn_hiá»‡p_Phá»¥_ná»¯_Viá»‡t_Nam", "Tá»•ng_LiÃªn_Ä‘oÃ n_Lao_Ä‘á»™ng_Viá»‡t_Nam",
    "Há»™i_NÃ´ng_dÃ¢n_Viá»‡t_Nam", "Há»™i_Cá»±u_chiáº¿n_binh_Viá»‡t_Nam",
    
    # QuÃ¢n Ä‘á»™i & An ninh
    "QuÃ¢n_Ä‘á»™i_nhÃ¢n_dÃ¢n_Viá»‡t_Nam", "QuÃ¢n_chá»§ng_Viá»‡t_Nam",
    "QuÃ¢n_khu_(Viá»‡t_Nam)", "SÆ°_Ä‘oÃ n_Viá»‡t_Nam",
    "CÃ´ng_an_nhÃ¢n_dÃ¢n_Viá»‡t_Nam",
    
    # HuÃ¢n chÆ°Æ¡ng
    "HuÃ¢n_chÆ°Æ¡ng_Viá»‡t_Nam", "Huy_chÆ°Æ¡ng_Viá»‡t_Nam", "Danh_hiá»‡u_Viá»‡t_Nam",
    
    # ========== KINH Táº¾ (50+ categories) ==========
    "Kinh_táº¿_Viá»‡t_Nam", "Lá»‹ch_sá»­_kinh_táº¿_Viá»‡t_Nam", "Äá»•i_má»›i_kinh_táº¿",
    "Doanh_nghiá»‡p_Viá»‡t_Nam", "CÃ´ng_ty_Viá»‡t_Nam", "Táº­p_Ä‘oÃ n_Viá»‡t_Nam",
    "NgÃ¢n_hÃ ng_Viá»‡t_Nam", "Chá»©ng_khoÃ¡n_Viá»‡t_Nam",
    "NÃ´ng_nghiá»‡p_Viá»‡t_Nam", "CÃ´ng_nghiá»‡p_Viá»‡t_Nam", "Dá»‹ch_vá»¥_Viá»‡t_Nam",
    "ThÆ°Æ¡ng_máº¡i_Viá»‡t_Nam", "Xuáº¥t_kháº©u_Viá»‡t_Nam", "Nháº­p_kháº©u_Viá»‡t_Nam",
    "Du_lá»‹ch_Viá»‡t_Nam", "KhÃ¡ch_sáº¡n_Viá»‡t_Nam", "Khu_nghá»‰_dÆ°á»¡ng_Viá»‡t_Nam",
    "Äá»“ng_Viá»‡t_Nam", "Thuáº¿_Viá»‡t_Nam", "NgÃ¢n_sÃ¡ch_nhÃ _nÆ°á»›c_Viá»‡t_Nam",
    
    # ========== GIÃO Dá»¤C & KHOA Há»ŒC (40+ categories) ==========
    "GiÃ¡o_dá»¥c_Viá»‡t_Nam", "TrÆ°á»ng_Ä‘áº¡i_há»c_Viá»‡t_Nam",
    "TrÆ°á»ng_trung_há»c_phá»•_thÃ´ng_Viá»‡t_Nam", "TrÆ°á»ng_cao_Ä‘áº³ng_Viá»‡t_Nam",
    "Äáº¡i_há»c_Quá»‘c_gia_HÃ _Ná»™i", "Äáº¡i_há»c_Quá»‘c_gia_ThÃ nh_phá»‘_Há»“_ChÃ­_Minh",
    "Há»c_viá»‡n_(Viá»‡t_Nam)", "Viá»‡n_(Viá»‡t_Nam)",
    
    "Khoa_há»c_Viá»‡t_Nam", "NhÃ _khoa_há»c_Viá»‡t_Nam", "Viá»‡n_HÃ n_lÃ¢m_Khoa_há»c_vÃ _CÃ´ng_nghá»‡_Viá»‡t_Nam",
    "CÃ´ng_nghá»‡_Viá»‡t_Nam", "CÃ´ng_nghá»‡_thÃ´ng_tin_Viá»‡t_Nam",
    
    # ========== Y Táº¾ (20+ categories) ==========
    "Y_táº¿_Viá»‡t_Nam", "Bá»‡nh_viá»‡n_Viá»‡t_Nam", "Y_há»c_cá»•_truyá»n_Viá»‡t_Nam",
    "DÆ°á»£c_há»c_Viá»‡t_Nam", "Thuá»‘c_Viá»‡t_Nam", "BÃ¡c_sÄ©_Viá»‡t_Nam",
    "Y_táº¿_cÃ´ng_cá»™ng_Viá»‡t_Nam", "Dá»‹ch_bá»‡nh_táº¡i_Viá»‡t_Nam",
    
    # ========== GIAO THÃ”NG & KIáº¾N TRÃšC (40+ categories) ==========
    "Giao_thÃ´ng_Viá»‡t_Nam", "ÄÆ°á»ng_bá»™_Viá»‡t_Nam", "ÄÆ°á»ng_cao_tá»‘c_Viá»‡t_Nam",
    "Cáº§u_táº¡i_Viá»‡t_Nam", "SÃ¢n_bay_Viá»‡t_Nam", "Cáº£ng_biá»ƒn_Viá»‡t_Nam",
    "ÄÆ°á»ng_sáº¯t_Viá»‡t_Nam", "Ga_Ä‘Æ°á»ng_sáº¯t_Viá»‡t_Nam",
    "Giao_thÃ´ng_cÃ´ng_cá»™ng_Viá»‡t_Nam", "Xe_buÃ½t_Viá»‡t_Nam",
    
    "Kiáº¿n_trÃºc_Viá»‡t_Nam", "NhÃ _cá»•_Viá»‡t_Nam", "ÄÃ¬nh_lÃ ng_Viá»‡t_Nam",
    "Chá»£_Viá»‡t_Nam", "CÃ´ng_trÃ¬nh_kiáº¿n_trÃºc_Viá»‡t_Nam",
    "TÃ²a_nhÃ _chá»c_trá»i_Viá»‡t_Nam", "CÃ´ng_viÃªn_Viá»‡t_Nam",
    
    # ========== THá»‚ THAO (30+ categories) ==========
    "Thá»ƒ_thao_Viá»‡t_Nam", "BÃ³ng_Ä‘Ã¡_Viá»‡t_Nam", "CÃ¢u_láº¡c_bá»™_bÃ³ng_Ä‘Ã¡_Viá»‡t_Nam",
    "Äá»™i_tuyá»ƒn_bÃ³ng_Ä‘Ã¡_quá»‘c_gia_Viá»‡t_Nam", "V.League", "Cáº§u_thá»§_bÃ³ng_Ä‘Ã¡_Viá»‡t_Nam",
    "SEA_Games", "ASIAD", "Olympic_Viá»‡t_Nam",
    "VÃµ_thuáº­t_Viá»‡t_Nam", "VÃµ_cá»•_truyá»n_Viá»‡t_Nam", "Vovinam",
    "Cáº§u_lÃ´ng_Viá»‡t_Nam", "Äiá»n_kinh_Viá»‡t_Nam", "BÆ¡i_lá»™i_Viá»‡t_Nam",
    "Váº­n_Ä‘á»™ng_viÃªn_Viá»‡t_Nam", "Huáº¥n_luyá»‡n_viÃªn_Viá»‡t_Nam",
    
    # ========== BIá»‚U TÆ¯á»¢NG & Tá»”NG Há»¢P (20+ categories) ==========
    "Biá»ƒu_tÆ°á»£ng_quá»‘c_gia_Viá»‡t_Nam", "Quá»‘c_ká»³_Viá»‡t_Nam", "Quá»‘c_ca_Viá»‡t_Nam",
    "Quá»‘c_huy_Viá»‡t_Nam", "Danh_sÃ¡ch_liÃªn_quan_Ä‘áº¿n_Viá»‡t_Nam",
    "Viá»‡t_Nam", "XÃ£_há»™i_Viá»‡t_Nam", "Con_ngÆ°á»i_Viá»‡t_Nam",
]


# ================================
# 3. CACHE MANAGER - Incremental
# ================================

class IncrementalCache:
    """Cache thÃ´ng minh vá»›i incremental update"""
    
    def __init__(self):
        self.articles: Dict = {}
        self.category_tree: Dict = {}
        self.metadata: Dict = {
            "last_update": None,
            "total_articles": 0,
            "total_categories": 0,
            "version": "2.0"
        }
        
        self.lock = Lock()
        self._load_cache()
    
    def _load_cache(self):
        """Load cache tá»« disk"""
        # Load articles
        if Config.ARTICLES_CACHE.exists():
            try:
                with open(Config.ARTICLES_CACHE, 'r', encoding='utf-8') as f:
                    self.articles = json.load(f)
                logger.info(f"âœ“ Loaded {len(self.articles)} articles from cache")
            except Exception as e:
                logger.warning(f"Failed to load articles cache: {e}")
        
        # Load category tree
        if Config.CATEGORY_TREE.exists():
            try:
                with open(Config.CATEGORY_TREE, 'r', encoding='utf-8') as f:
                    self.category_tree = json.load(f)
                logger.info(f"âœ“ Loaded category tree from cache")
            except Exception as e:
                logger.warning(f"Failed to load category tree: {e}")
        
        # Load metadata
        if Config.METADATA.exists():
            try:
                with open(Config.METADATA, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                logger.info(f"âœ“ Last update: {self.metadata.get('last_update', 'N/A')}")
            except Exception as e:
                logger.warning(f"Failed to load metadata: {e}")
    
    def save_incremental(self):
        """LÆ°u cache (incremental)"""
        with self.lock:
            # Save articles
            with open(Config.ARTICLES_CACHE, 'w', encoding='utf-8') as f:
                json.dump(self.articles, f, ensure_ascii=False, indent=2)
            
            # Save category tree
            with open(Config.CATEGORY_TREE, 'w', encoding='utf-8') as f:
                json.dump(self.category_tree, f, ensure_ascii=False, indent=2)
            
            # Update metadata
            self.metadata.update({
                "last_update": datetime.now().isoformat(),
                "total_articles": len(self.articles),
                "total_categories": len(self.category_tree)
            })
            
            with open(Config.METADATA, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ“ Cache saved: {len(self.articles)} articles, {len(self.category_tree)} categories")
    
    def has_article(self, title: str) -> bool:
        return title in self.articles
    
    def add_article(self, title: str, data: Dict):
        with self.lock:
            self.articles[title] = data
    
    def get_article(self, title: str) -> Optional[Dict]:
        return self.articles.get(title)
    
    def add_to_tree(self, parent: str, child: str, level: int):
        with self.lock:
            if parent not in self.category_tree:
                self.category_tree[parent] = {"level": level, "children": []}
            if child not in self.category_tree[parent]["children"]:
                self.category_tree[parent]["children"].append(child)


# ================================
# 4. CATEGORY TRACKER
# ================================

class CategoryTracker:
    """Theo dÃµi vÃ  phÃ¡t hiá»‡n vÃ²ng láº·p category"""
    
    def __init__(self):
        self.visited: Set[str] = set()
        self.in_progress: Set[str] = set()
        self.lock = Lock()
        
        # Statistics
        self.stats = {
            "visited": 0,
            "cycle_detected": 0,
            "max_level_reached": 0
        }
    
    def should_visit(self, cat_name: str, level: int, max_level: int) -> Tuple[bool, str]:
        """Kiá»ƒm tra xem cÃ³ nÃªn crawl category nÃ y khÃ´ng"""
        with self.lock:
            if level > max_level:
                self.stats["max_level_reached"] += 1
                return False, "max_level"
            
            if cat_name in self.visited:
                return False, "already_visited"
            
            if cat_name in self.in_progress:
                self.stats["cycle_detected"] += 1
                return False, "cycle_detected"
            
            return True, "ok"
    
    def mark_visiting(self, cat_name: str):
        with self.lock:
            self.in_progress.add(cat_name)
    
    def mark_visited(self, cat_name: str):
        with self.lock:
            self.visited.add(cat_name)
            self.stats["visited"] += 1
            if cat_name in self.in_progress:
                self.in_progress.remove(cat_name)


# ================================
# 5. MAIN CRAWLER CLASS
# ================================

class ProductionWikiCrawler:
    """Production-ready Wikipedia crawler"""
    
    def __init__(self, max_level=5, max_workers=25):
        self.max_level = max_level
        self.max_workers = max_workers
        
        self.cache = IncrementalCache()
        self.tracker = CategoryTracker()
        self.lock = Lock()
        
        self.stats = {
            "articles_crawled": 0,
            "articles_from_cache": 0,
            "categories_processed": 0,
            "errors": 0,
            "retries": 0
        }
        
        logger.info(f"Crawler initialized: max_level={max_level}, workers={max_workers}")
    
    @retry_on_failure(max_retries=Config.RETRY_ATTEMPTS)
    def _fetch_wikipedia_page(self, page_name: str, is_category: bool = False):
        """Fetch page vá»›i retry mechanism"""
        if is_category:
            page = wiki_wiki.page(f"Category:{page_name}")
        else:
            page = wiki_wiki.page(page_name)
        
        if not page.exists():
            raise ValueError(f"Page not found: {page_name}")
        
        return page
    
    def crawl_article(self, member_name: str, member_obj, parent_category: str) -> Optional[Dict]:
        """Crawl má»™t bÃ i viáº¿t"""
        
        # Check cache first
        if self.cache.has_article(member_name):
            with self.lock:
                self.stats["articles_from_cache"] += 1
            return self.cache.get_article(member_name)
        
        try:
            # Retry mechanism tá»± Ä‘á»™ng tá»« decorator
            article_data = {
                "title": member_obj.title,
                "url": member_obj.fullurl,
                "text": member_obj.text[:Config.TEXT_LIMIT],
                "summary": member_obj.summary,
                "categories": [parent_category],  # List Ä‘á»ƒ cÃ³ thá»ƒ merge
                "crawled_at": datetime.now().isoformat(),
                "length": len(member_obj.text)
            }
            
            # Add to cache
            self.cache.add_article(member_name, article_data)
            
            with self.lock:
                self.stats["articles_crawled"] += 1
                current_count = self.stats["articles_crawled"]

            # --- THÃŠM ÄOáº N NÃ€Y: Auto-save má»—i 1000 bÃ i ---
            if current_count % 1000 == 0:
                logger.info(f"ðŸ’¾ Auto-saving progress at {current_count} articles...")
                self.cache.save_incremental() 
            # ---------------------------------------------
            return article_data
            
        except Exception as e:
            logger.warning(f"Failed to crawl article {member_name}: {e}")
            with self.lock:
                self.stats["errors"] += 1
            return None
    
    def crawl_category(self, cat_name: str, level: int = 0):
        """Crawl má»™t category (Ä‘á»‡ quy)"""
        
        # Check Ä‘iá»u kiá»‡n
        should_visit, reason = self.tracker.should_visit(cat_name, level, self.max_level)
        
        if not should_visit:
            logger.debug(f"{'  ' * level}âŠ˜ Skip [{cat_name}] - {reason}")
            return
        
        # Mark visiting
        self.tracker.mark_visiting(cat_name)
        
        try:
            # Fetch vá»›i retry
            cat_page = self._fetch_wikipedia_page(cat_name, is_category=True)
            
            logger.info(f"{'  ' * level}â†’ [{cat_name}] (level {level})")
            
            members = cat_page.categorymembers
            articles = []
            subcats = []
            
            # PhÃ¢n loáº¡i members
            for member_name, member_obj in members.items():
                try:
                    if member_obj.ns == wikipediaapi.Namespace.MAIN:
                        articles.append((member_name, member_obj))
                    
                    elif member_obj.ns == wikipediaapi.Namespace.CATEGORY:
                        clean_name = member_obj.title.replace("Thá»ƒ loáº¡i:", "").replace("Category:", "").strip()
                        subcats.append(clean_name)
                        self.cache.add_to_tree(cat_name, clean_name, level + 1)
                
                except Exception as e:
                    continue
            
            # Crawl articles
            for member_name, member_obj in articles:
                self.crawl_article(member_name, member_obj, cat_name)
            
            # Mark visited
            self.tracker.mark_visited(cat_name)
            with self.lock:
                self.stats["categories_processed"] += 1
            
            # Crawl subcategories (Ä‘á»‡ quy)
            for subcat in subcats:
                self.crawl_category(subcat, level + 1)
            
            # Rate limiting
            time.sleep(Config.RATE_LIMIT_DELAY)
            
        except Exception as e:
            logger.error(f"Error crawling category {cat_name}: {e}")
            self.tracker.mark_visited(cat_name)  # Mark Ä‘á»ƒ khÃ´ng retry vÃ´ háº¡n
            with self.lock:
                self.stats["errors"] += 1
    
    def crawl_parallel(self, root_categories: List[str]):
        """Crawl song song cÃ¡c root categories"""
        logger.info(f"Starting parallel crawl: {len(root_categories)} root categories")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self.crawl_category, cat, 0): cat
                for cat in root_categories
            }
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="Root Categories"):
                cat = futures[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Failed root category {cat}: {e}")
        
        logger.info("âœ“ All root categories processed")
    
    def save_all(self):
        """LÆ°u táº¥t cáº£ dá»¯ liá»‡u"""
        logger.info("Saving data...")
        
        # Save cache
        self.cache.save_incremental()
        
        # Prepare DataFrame
        df = pd.DataFrame(self.cache.articles.values())
        
        if len(df) == 0:
            logger.warning("No data to export!")
            return
        
        # Deduplicate
        df = df.drop_duplicates(subset=['title'], keep='first')
        logger.info(f"Total unique articles: {len(df)}")
        
        # Export CSV
        if Config.EXPORT_CSV:
            csv_path = Config.OUTPUT_DIR / "final_wikipedia_vietnam_full.csv"
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            logger.info(f"âœ“ CSV saved: {csv_path}")
        
        # Export JSON
        if Config.EXPORT_JSON:
            json_path = Config.OUTPUT_DIR / "final_wikipedia_vietnam_full.json"
            df.to_json(json_path, orient='records', force_ascii=False, indent=2)
            logger.info(f"âœ“ JSON saved: {json_path}")
        
        # Export Parquet
        if Config.EXPORT_PARQUET:
            try:
                parquet_path = Config.OUTPUT_DIR / "final_wikipedia_vietnam_full.parquet"
                df.to_parquet(parquet_path, index=False, compression='snappy')
                logger.info(f"âœ“ Parquet saved: {parquet_path}")
            except Exception as e:
                logger.warning(f"Failed to save Parquet: {e}")
        
        # Export category tree
        tree_path = Config.OUTPUT_DIR / "category_tree_full.json"
        with open(tree_path, 'w', encoding='utf-8') as f:
            json.dump(self.cache.category_tree, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ“ Category tree saved: {tree_path}")
    
    def print_stats(self):
        """In thá»‘ng kÃª chi tiáº¿t"""
        logger.info("=" * 70)
        logger.info("CRAWL STATISTICS")
        logger.info("=" * 70)
        
        # Crawler stats
        logger.info("Articles:")
        logger.info(f"  - Crawled new: {self.stats['articles_crawled']:,}")
        logger.info(f"  - From cache: {self.stats['articles_from_cache']:,}")
        logger.info(f"  - Total: {self.stats['articles_crawled'] + self.stats['articles_from_cache']:,}")
        
        logger.info("\nCategories:")
        logger.info(f"  - Processed: {self.stats['categories_processed']:,}")
        logger.info(f"  - Visited: {self.tracker.stats['visited']:,}")
        logger.info(f"  - Cycle detected: {self.tracker.stats['cycle_detected']:,}")
        logger.info(f"  - Max level reached: {self.tracker.stats['max_level_reached']:,}")
        
        logger.info("\nErrors & Retries:")
        logger.info(f"  - Errors: {self.stats['errors']:,}")
        logger.info(f"  - Retries: {self.stats['retries']:,}")
        
        logger.info("=" * 70)


# ================================
# 6. MAIN EXECUTION
# ================================

def main():
    """Main function"""
    start_time = time.time()
    
    logger.info("=" * 70)
    logger.info("WIKIPEDIA VIETNAM CRAWLER - PRODUCTION VERSION")
    logger.info("=" * 70)
    logger.info(f"Total root categories: {len(COMPREHENSIVE_CATEGORIES)}")
    logger.info(f"Max level: {Config.MAX_LEVEL}")
    logger.info(f"Max workers: {Config.MAX_WORKERS}")
    logger.info("=" * 70)
    
    # Initialize crawler
    crawler = ProductionWikiCrawler(
        max_level=Config.MAX_LEVEL,
        max_workers=Config.MAX_WORKERS
    )
    
    # Crawl
    try:
        crawler.crawl_parallel(COMPREHENSIVE_CATEGORIES)
    except KeyboardInterrupt:
        logger.warning("\nâš  Crawl interrupted by user")
    except Exception as e:
        logger.error(f"\nâŒ Fatal error: {e}")
    
    # Save everything
    crawler.save_all()
    
    # Print stats
    crawler.print_stats()
    
    # Time
    elapsed = time.time() - start_time
    logger.info(f"\nâ± Total time: {elapsed/60:.2f} minutes ({elapsed:.1f}s)")
    
    total_articles = crawler.stats['articles_crawled'] + crawler.stats['articles_from_cache']
    if total_articles > 0:
        logger.info(f"ðŸ“Š Speed: {total_articles/elapsed:.1f} articles/second")
    
    logger.info("\nâœ“ Crawl completed successfully!")


if __name__ == "__main__":
    main()