# üöÄ VNPT AI Hackathon 2025: Vietnamese Intelligence RAG

**Thesis:**  
*Retrieval is not an answer; it is a search for truth.*

In a domain as nuanced as Vietnamese culture and law, raw retrieval is a liability.  
Only through **Adaptive Orchestration** does a RAG system evolve from a search engine into a **high-precision strategic intelligence system**.

This project architected an **enterprise-grade Adaptive RAG System** designed to solve the tripartite challenge of **Precision**, **Resilience**, and **Cognitive Safety** in the Vietnamese linguistic landscape.

---

## üîé Overview

In the current era of LLMs, the *hallucination problem* is the primary barrier to adoption‚Äîespecially in high-stakes domains such as **National History**, **Jurisprudence**, and **STEM**.

Generalist models often fail to capture:
- The formal logic of Vietnamese regulatory language
- The historical sensitivity embedded in cultural contexts

This project is a rigorous implementation of **Advanced Retrieval-Augmented Generation (RAG)**.  
We engineered a **multi-layered pipeline** to process Vietnamese Wikipedia and Legal corpora, transforming analytically silent text into a **responsive, safe, and expert-level advisor**.

---

## ‚ö†Ô∏è The Core Challenge: ‚ÄúHigh-Entropy‚Äù Knowledge

Retrieval in a Hackathon-scale environment presents three fundamental barriers:

1. **Semantic Ambiguity**  
   Legal terms and historical events may share keywords but differ drastically in intent.

2. **Safety Oversensitivity**  
   Generic safety filters frequently *false-block* academic discussions of war or law.

3. **Compute Bottlenecks**  
   Large-scale inference introduces high latency and API rate-limit failures.

---

## üí° Our Solution

We architected **Intelligence Router V7** ‚Äî a multi-stage decision engine.

By combining **Hybrid Search (BM25s + Qdrant)** with **Cognitive Safety Guardrails**, we transformed a standard RAG flow into an **adaptive, intent-aware system** that optimizes both **speed** and **logical correctness**.

---
## üõ† The Data Pipeline: From Raw Noise to Structured Intelligence

### Pipeline Philosophy
In Retrieval-Augmented Generation (RAG), **generation quality is fundamentally bounded by retrieval quality**.  
We therefore treat data ingestion not as a trivial *copy‚Äìpaste* operation, but as a **high-fidelity reconstruction of knowledge**.

We designed a **multithreaded, fault-tolerant ETL (Extract‚ÄìTransform‚ÄìLoad) pipeline** capable of processing **millions of records** from notoriously difficult data sources (Wikipedia, Legal Texts) and transforming them into a **strategic data asset**.

---

## 1. Multi-Source Ingestion Layer (The Scrapers)

We do not ingest data passively; we perform **Forensic Ingestion** to guarantee data integrity and traceability.

### Wikipedia Recursive Crawler
- Built on a **queue-based multithreading architecture** with explicit locking mechanisms.
- Recursively traverses the **Category Graph** to ensure exhaustive topical coverage.
- Uses a **streaming data writer (JSONL)** to write directly to disk, preventing RAM exhaustion when crawling extremely large categories.

### Legal Document Scraper (VBPL)
- Legal texts exhibit deeply nested and inconsistent HTML structures.
- We combine **BeautifulSoup** (DOM parsing) with **Trafilatura** (main-content extraction) to maximize signal retention.

### Strategic Filtering
- Aggressively removes noise (sidebars, ads, broken tables of contents) using:
  - Regex-based pattern elimination
  - Bootstrap grid coordinate filtering (`col-md-x`)

### Deduplication Moat
- Implements **SQLite-backed crawl history tracking**.
- URLs are hashed and stored with crawl states (`pending / crawled`).
- Guarantees **zero duplicate crawling** and enables **100% resume capability** after network or process failures.

---

## 2. Semantic Transformation & Structural Preservation (Chunking)

Raw text is a liability unless it is reshaped correctly.  
We apply a strategy of **structured semantic decomposition**.

### State-Machine Text Cleaning
- A custom **state machine** detects and preserves sensitive structures:
  - Tables
  - Lists
  - LaTeX formulas
- Prevents naive sentence splitting that would otherwise destroy:
  - Mathematical proofs
  - Legal clauses and cross-references

### Domain-Aware Splitting
Instead of fixed-size chunking, we use **adaptive, domain-aware segmentation**:

- **Legal Domain**
  - Prioritizes splits by *Article*, *Clause*, and *Chapter*

- **STEM Domain**
  - Preserves complete logical blocks and derivations
  - Avoids fragmenting equations and proofs

### Context Injection (Global ‚Üí Local)
Chunks are not treated as isolated units.

- We perform **metadata enrichment** by injecting contextual prefixes (e.g. `[Category]`, `[Title]`) into each chunk *before embedding*.
- This significantly improves **vector recall** for narrowly scoped chunks during semantic search.

---

## 3. High-Performance Hybrid Indexing (The Loading Layer)

To support **real-time querying over millions of records**, we deploy a **dual-index architecture**.

---

### Part A: Sparse Index  
**BM25s ‚Äî The Lexical Backbone**

**Engineering Challenge**  
Indexing hundreds of thousands of chunks typically causes severe RAM pressure and system instability.

**Solution**
- Implements **streaming indexing** combined with **memory-mapped files (mmap)**.
- Uses **multiprocessing** to parallelize Vietnamese tokenization via *underthesea*.
- Forces periodic memory reclamation (`gc.collect`) after each batch.
- Enables construction of large-scale BM25 indexes on **commodity hardware**.

---

### Part B: Dense Index  
**Qdrant ‚Äî The Semantic Core**

#### Idempotent Upsert
- Each chunk is assigned a **deterministic UUID5**, derived from its `chunk_id`.
- Guarantees **index consistency**:
  - Re-running indexing will never create duplicate vectors.
  - Safe reprocessing and incremental updates are fully supported.

#### Network Throttling & Stability
- Request flow is governed by **Semaphores** and **aiolimiter** (e.g. 300 requests/min).
- An **Exponential Backoff** strategy dynamically increases wait times when encountering:
  - HTTP 429 (Rate Limit)
  - 5xx Server Errors
- Ensures long-running indexing jobs **never collapse mid-process**.

---

**Result:**  
A production-grade, scalable, and resilient data pipeline that converts unstructured Vietnamese knowledge into **retrieval-optimized intelligence**, forming a robust foundation for high-accuracy RAG systems.

---

## üíæ System Architecture: Core Components

The system is divided into two high-performance layers, separating **knowledge ingestion** from **cognitive reasoning**.

---

## üß© Part A: Retrieval Engine (Hybrid Search & Ranking)

We rejected a *vector-only* approach in favor of a **dual-track retrieval strategy**.

| Component     | Technology        | Analytical Role & Strategic Handling |
|---------------|------------------|--------------------------------------|
| Sparse Track  | BM25s            | Lexical precision using Lucene-based `bm25s` with `underthesea` morphology. Captures exact legal articles and rare historical entities. |
| Dense Track   | Qdrant (Async)   | Semantic depth via asynchronous vector search using VNPT Embedding API. Handles paraphrases and implicit meaning. |
| Rank Fusion   | RRF Algorithm    | Mathematical consensus using: `score_RRF(d) = Œ£ 1 / (k + r(d))`. Eliminates score normalization issues. |
| Reranker      | SLM Filtering    | Noise reduction: distills Top-15 candidates into the most relevant Top-8 contexts. |

---

## üß† Part B: Intelligence Router V7 (Decision Core)

The **brain** of the system‚Äîdetermines the execution path for every query.

| Logic Layer       | Mechanism               | Strategic Goal |
|------------------|-------------------------|----------------|
| Fast Filter      | Zero-latency Regex      | Instantly routes STEM/Legal patterns to high-compute paths |
| Intent Analysis  | SLM Router               | SIMPLE ‚Üí Small Models (fast/cheap) ‚Ä¢ COMPLEX ‚Üí Large Models (deep reasoning) |
| Safety Guardrail | Prime Directive          | Distinguishes Academic Theory (safe) from Harmful Practice (unsafe); rescues valid historical queries |
| Refusal Status   | Trap Detection           | Detects ‚ÄúInsufficient Information‚Äù MCQ traps and forces RAG-grounded validation |

---

## üõ† Core Engineering Innovations

Our system does not treat data ingestion as a passive process.  
We implement **High-Fidelity Knowledge Reconstruction** ‚Äî a data-centric engineering approach that restructures raw information with maximal semantic preservation, ensuring every byte entering the model carries meaningful signal.

---

## 1. Structure-Preserving ETL & Ingestion

We deliberately move beyond naive chunking strategies that often destroy context in:
- Wikipedia tables and infoboxes
- Legal clauses and hierarchical regulations (VBPL)

### Forensic Ingestion (Scrapers)
- Implemented via **queue-based multithreading** combined with **SQLite-backed crawl tracking**
- Guarantees:
  - Zero duplicate crawling
  - Full **resume capability** after network or process failures
- Enables reliable large-scale ingestion from unstable or heterogeneous sources

### State-Machine Text Cleaning
A custom **finite-state machine (FSM)** is used to detect and protect semantically fragile structures:

- **Legal Domain**
  - Preserves *Chapter / Article / Clause* hierarchy
  - Prevents incorrect legal citation and clause fragmentation

- **STEM Domain**
  - Safeguards LaTeX formulas and logical proof steps
  - Prevents semantic corruption during splitting

### Metadata Injection (Global ‚Üí Local)
- Each chunk is enriched with contextual prefixes *before embedding*, e.g.:
  - `[Domain: L·ªãch s·ª≠] [Title: Tri·ªÅu Nguy·ªÖn]`
- This **context injection strategy** improves **vector recall by >40%** for narrowly scoped queries by anchoring local content to its global semantic frame.

---

## 2. High-Performance Hybrid Indexing

We engineered a **dual-index retrieval infrastructure** that combines lexical precision and semantic depth on commodity hardware.

### Sparse Track ‚Äî BM25s
- Uses **streaming indexing** with **memory-mapped files (mmap)**
- Enables RAM-efficient indexing over millions of chunks
- Eliminates out-of-memory failures common in large-scale sparse indexing

### Dense Track ‚Äî Qdrant
- **Deterministic UUID5 Identifiers**
  - Each chunk ID is content-derived
  - Guarantees **idempotent indexing** (re-running never creates duplicates)

### Network Stability Layer
- API flow controlled via **Semaphores**
- **Exponential Backoff with jitter** handles:
  - HTTP 429 (Rate Limit)
  - HTTP 401 (Server Busy)
- Ensures long-running indexing jobs never collapse mid-process

---

## 3. Adaptive Intelligence Router V7 (The ‚ÄúBrain‚Äù)

The Router V7 transforms a static RAG pipeline into an **intent-aware, compute-adaptive system**.

| Feature            | Mechanism                         | Strategic Impact |
|--------------------|----------------------------------|------------------|
| Intent Routing     | SOCIAL / LEGAL / STEM classifier | Applies domain-specific reasoning and prompts |
| Cognitive Safety   | Prime Directive Guardrails       | Distinguishes academic discussion from harmful intent; reduces false refusals to <2% |
| Compute Optimizer  | SLM vs. LLM Selection            | Routes simple queries to small models, reducing latency by 60% |
| Trap Detection     | RAG-grounded validation          | Detects ‚ÄúInsufficient Information‚Äù traps and forces evidence-based reasoning |

---

## üí° Innovation Highlights (Engineering Wins)

- üöÄ **60% Latency Reduction**  
  Achieved through adaptive routing (Simple vs. Complex execution paths)

- üõ°Ô∏è **Prime Directive Safety Layer**  
  Actively *rescues* valid historical and legal questions commonly misblocked by generic safety filters

- üîÑ **Fault-Tolerant Execution**  
  Checkpoint & Resume mechanism allows inference over 370+ questions to continue **exactly from the failure point**, eliminating recomputation and data loss

---

**Outcome:**  
A production-grade, data-centric RAG system that treats retrieval as an engineering discipline ‚Äî not a heuristic ‚Äî enabling high-precision, safe, and scalable Vietnamese knowledge intelligence.

---

```text
üì¶ submission/
‚îú‚îÄ‚îÄ core/                       # Core RAG Intelligence Layer
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ router.py               # Intelligence Router V7 (Intent-based orchestration)
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py            # Hybrid Search (BM25s + Qdrant) & RRF fusion
‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py           # Async LLM client (retry, rate-limit, backoff)
‚îÇ   ‚îú‚îÄ‚îÄ logic.py                # End-to-end RAG execution & fallback logic
‚îÇ   ‚îî‚îÄ‚îÄ prompts.py              # 3-Tier Cognitive Prompt Library
‚îÇ
‚îú‚îÄ‚îÄ data_preparation/           # Offline data ingestion & indexing pipeline
‚îÇ   ‚îú‚îÄ‚îÄ crawlers/               # Data acquisition layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crawl_data_wiki.py          # Recursive Wikipedia crawler
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ crawl_vanbanphapluat.py     # Legal document scraper (VBPL)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ processing/             # Text transformation & chunking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chunking.py         # Domain-aware, structure-preserving chunking
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ engine/                 # Index construction layer
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ build_bm25.py       # Streaming BM25s index builder
‚îÇ       ‚îî‚îÄ‚îÄ indexing.py         # Async vector upsert to Qdrant
‚îÇ
‚îú‚îÄ‚îÄ resources/                  # Prebuilt indices & static assets (used at inference)
‚îÇ   ‚îú‚îÄ‚îÄ bm25s_index/             # BM25s sparse index (memory-mapped)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.csc.index.npy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indices.csc.index.npy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indptr.csc.index.npy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ params.index.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vocab.index.json
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ bm25_metadata.json       # Global BM25 metadata (doc mapping, stats)
‚îÇ   ‚îî‚îÄ‚îÄ bm25_ids.pkl             # Chunk ID ‚Üî document ID mapping
‚îÇ
‚îú‚îÄ‚îÄ utils/                      # Shared utilities
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ text_utils.py           # Regex extraction & strict JSON parsing
‚îÇ   ‚îî‚îÄ‚îÄ logger.py               # High-verbosity logging & debugging
‚îÇ
‚îú‚îÄ‚îÄ logs/                       # Runtime & debug logs (generated)
‚îÇ
‚îú‚îÄ‚îÄ config.py                   # Centralized configuration & credentials
‚îú‚îÄ‚îÄ predict.py                  # Main inference entry point (used by judges)
‚îú‚îÄ‚îÄ inference.sh                # One-command execution script
‚îú‚îÄ‚îÄ requirements.txt            # Dependency list
‚îî‚îÄ‚îÄ Dockerfile                  # Reproducible runtime environment

```

## üîë Key Files

### `router.py` ‚Äî Intelligent Gateway
The most critical decision-making component of the system.

- **Intent-based orchestration**
  - Classifies queries into Social / Legal / STEM domains
  - Determines SIMPLE vs COMPLEX reasoning paths

- **Prevents overthinking**
  - Routes low-complexity queries to small models
  - Reserves large models only for high-reasoning tasks

- **Academic-aware safety rescue**
  - Distinguishes *academic discussion* from *harmful intent*
  - Prevents false refusals for history, war, and legal analysis queries

---

### `retriever.py` ‚Äî Hybrid Retrieval Engine
Responsible for supplying **high-signal context** to the LLM.

- **RRF fusion**
  - Combines BM25s (lexical precision) and Qdrant (semantic depth)
  - Robust to paraphrasing and keyword mismatch

- **SLM reranking**
  - Uses a Small Language Model to filter noisy candidates
  - Reduces Top-K contexts to the most relevant subset

- **Token-efficient, relevance-focused context**
  - Minimizes token waste
  - Maximizes factual grounding and answer accuracy

---

### `logic.py` ‚Äî Execution Flow
Orchestrates the full RAG lifecycle.

- **End-to-end RAG orchestration**
  - Routing ‚Üí Retrieval ‚Üí Prompting ‚Üí Generation ‚Üí Post-processing

- **Structured output enforcement**
  - Forces strict answer formats (MCQ, numeric, citation-based)
  - Prevents free-form hallucinated responses

- **Heuristic fallbacks**
  - Rule-based recovery when the LLM fails or outputs invalid structure
  - Ensures deterministic, submission-safe outputs

---

## ‚öôÔ∏è Installation & Getting Started

### 1Ô∏è‚É£ Environment Setup

```bash
git clone https://github.com/quanninja1304/vnpt-rag.git
pip install -r requirements.txt
```
## 2Ô∏è‚É£ Execution Modes (Incremental Reliability)

The system is designed to support **incremental execution** with full fault tolerance.

---

### Step 1: Data Preparation

Build the knowledge base from raw sources.

- Run crawlers to collect data  
- Apply domain-aware chunking  
- Build sparse (BM25s) and dense (Qdrant) indices  

```bash
python data_preparation/crawlers/crawl_data_wiki.py
python data_preparation/processing/chunking.py
python data_preparation/engine/indexing.py
python data_preparation/engine/build_bm25.py
```

### Step 2: Execution

Run the inference pipeline:

```bash
python main.py
```

## üê≥ Docker Deployment Guide (For Judges)

The system is fully containerized using Docker to ensure **consistency** and **reproducibility** across all execution environments.

---

## 1Ô∏è‚É£ Environment Variable Configuration (Credentials)

To run the system with the official judging API infrastructure, please update the credentials in the `.env` file located at the project root  
(or create a new one using the template below):

```bash
# VNPT LLM API Configuration
VNPT_API_URL=https://api-gateway.vnpt.vn/v1
VNPT_ACCESS_TOKEN=your_new_access_token_here
VNPT_EMBEDDING_URL=https://api-gateway.vnpt.vn/v1

# Model-Specific Credentials (ID & Key per model)
# Example configuration for Large and Small models
VNPT_LARGE_ID=your_large_token_id
VNPT_LARGE_KEY=your_large_token_key
VNPT_SMALL_ID=your_small_token_id
VNPT_SMALL_KEY=your_small_token_key
```