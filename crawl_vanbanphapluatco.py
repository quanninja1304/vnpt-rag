"""
PRODUCTION CRAWLER - VANBANPHAPLUAT.CO
Output Schema kh·ªõp v·ªõi quy tr√¨nh Chunking/RAG
"""
import requests
from bs4 import BeautifulSoup
import trafilatura
import pandas as pd
import time
import random
import hashlib
import os
import re
import sqlite3
import logging
from urllib.parse import urljoin
from tqdm import tqdm
from datetime import datetime
import urllib3
from config import Config

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH CH·∫†Y TH·∫¨T (PRODUCTION) ---
DB_FILE = "vbpl_production.db"
OUTPUT_FILE = Config.OUTPUT_DIR / "vbpl_full_dataset.parquet"
TEMP_BATCH_DIR = "vbpl_batches_prod"

# TƒÉng k√≠ch th∆∞·ªõc batch ƒë·ªÉ gi·∫£m s·ªë l∆∞·ª£ng file nh·ªè (ghi ƒëƒ©a m·ªói 100 b√†i)
CHECKPOINT_SIZE = 100       

# B·ªô l·ªçc n·ªôi dung r√°c (VƒÉn b·∫£n lu·∫≠t th∆∞·ªùng d√†i, <800 k√Ω t·ª± th∆∞·ªùng l√† l·ªói ho·∫∑c m·ª•c l·ª•c)
MIN_CONTENT_LENGTH = 800  

# Logging
logging.basicConfig(
    filename="vbpl_production.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

os.makedirs(TEMP_BATCH_DIR, exist_ok=True)

# --- DANH M·ª§C CRAWL ƒê·∫¶Y ƒê·ª¶ (FULL SCOPE) ---
SEED_CATEGORIES = {
    # === LO·∫†I VƒÇN B·∫¢N ===
    "Lo·∫°i vƒÉn b·∫£n": [
        "/loai-van-ban/luat", "/loai-van-ban/nghi-dinh", "/loai-van-ban/thong-tu",
        "/loai-van-ban/quyet-dinh", "/loai-van-ban/chi-thi", "/loai-van-ban/nghi-quyet",
        "/loai-van-ban/phap-lenh", "/loai-van-ban/lenh", "/loai-van-ban/thong-tu-lien-tich",
        "/loai-van-ban/cong-dien", "/loai-van-ban/cong-van", "/loai-van-ban/quy-che",
        "/loai-van-ban/quy-dinh", "/loai-van-ban/huong-dan", "/loai-van-ban/tieu-chuan-viet-nam",
        "/loai-van-ban/quy-chuan"
    ],
    
    # === Lƒ®NH V·ª∞C ===
    "Lƒ©nh v·ª±c": [
        "/linh-vuc/doanh-nghiep", "/linh-vuc/lao-dong-tien-luong", "/linh-vuc/thue-phi-le-phi",
        "/linh-vuc/bao-hiem", "/linh-vuc/giao-thong-van-tai", "/linh-vuc/xay-dung-do-thi",
        "/linh-vuc/tai-chinh-nha-nuoc", "/linh-vuc/nong-nghiep", "/linh-vuc/the-thao-y-te",
        "/linh-vuc/giao-duc", "/linh-vuc/van-hoa-xa-hoi", "/linh-vuc/tai-nguyen-moi-truong",
        "/linh-vuc/bat-dong-san", "/linh-vuc/thuong-mai", "/linh-vuc/dau-tu",
        "/linh-vuc/chung-khoan", "/linh-vuc/tien-te-ngan-hang", "/linh-vuc/so-huu-tri-tue",
        "/linh-vuc/cong-nghe-thong-tin", "/linh-vuc/quyen-dan-su", "/linh-vuc/trach-nhiem-hinh-su",
        "/linh-vuc/vi-pham-hanh-chinh", "/linh-vuc/thu-tuc-to-tung", "/linh-vuc/bo-may-hanh-chinh",
        "/linh-vuc/ke-toan-kiem-toan", "/linh-vuc/cong-nghiep", "/linh-vuc/dien-dien-tu",
        "/linh-vuc/hoa-chat", "/linh-vuc/xuat-nhap-khau"
    ],
    
    # === VƒÇN B·∫¢N M·ªöI ===
    "VƒÉn b·∫£n m·ªõi": ["/van-ban-moi"]
}

# --- DATABASE MANAGER ---
class HistoryDB:
    def __init__(self, db_path):
        self.db_path = db_path
        self.conn = None
        self.cursor = None
        try:
            self.conn = sqlite3.connect(db_path, check_same_thread=False)
            self.cursor = self.conn.cursor()
            self.cursor.execute('''
                CREATE TABLE IF NOT EXISTS visited_urls (
                    url_hash TEXT PRIMARY KEY,
                    url TEXT UNIQUE,
                    category TEXT,
                    status TEXT DEFAULT 'pending',
                    crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            self.conn.commit()
        except Exception as e:
            logging.critical(f"‚ùå Cannot connect to DB: {e}")
            raise

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.conn: self.conn.close()

    def exists(self, url):
        try:
            h = hashlib.md5(url.encode()).hexdigest()
            self.cursor.execute("SELECT 1 FROM visited_urls WHERE url_hash = ?", (h,))
            return self.cursor.fetchone() is not None
        except:
            return False

    def add(self, url, category, status='success'):
        h = hashlib.md5(url.encode()).hexdigest()
        try:
            self.cursor.execute(
                "INSERT OR IGNORE INTO visited_urls (url_hash, url, category, status) VALUES (?, ?, ?, ?)", 
                (h, url, category, status)
            )
            self.conn.commit()
        except Exception as e:
            logging.error(f"DB insert error: {e}")

# --- NETWORK & EXTRACTION ---
def get_session():
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'vi-VN,vi;q=0.9'
    })
    return session

def extract_legal_document(session, url):
    try:
        resp = session.get(url, timeout=15, verify=False) 
        if len(resp.text) < 1000: return None

        # D√πng trafilatura ƒë·ªÉ l·∫•y text s·∫°ch
        data = trafilatura.extract(
            resp.text,
            output_format="json",
            include_comments=False,
            include_tables=True, # Gi·ªØ b·∫£ng bi·ªÉu v√¨ lu·∫≠t hay c√≥ b·∫£ng
            favor_precision=True
        )
        
        if data:
            import json
            j = json.loads(data)
            text = j.get('text', '').strip()
            title = j.get('title', '').strip()
            
            if len(text) < MIN_CONTENT_LENGTH: return None
            
            return {
                "title": title,
                "text": text,
                "url": url
            }
    except Exception as e:
        logging.error(f"Extract error {url}: {e}")
    return None

def find_document_links(session, url):
    links = set()
    base_url = "https://vanbanphapluat.co"
    try:
        resp = session.get(url, timeout=15, verify=False)
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        for a in soup.find_all('a', href=True):
            href = a['href']
            full_url = urljoin(base_url, href)
            
            # Logic l·ªçc link b√†i vi·∫øt (lo·∫°i b·ªè link danh m·ª•c/qu·∫£ng c√°o)
            if (base_url in full_url and 
                '/loai-van-ban/' not in full_url and
                '/linh-vuc/' not in full_url and
                '/van-ban-moi' not in full_url and
                len(full_url) > len(base_url) + 15):
                links.add(full_url)
    except Exception as e:
        logging.error(f"List page error {url}: {e}")
    return list(links)

def save_batch(batch_data, batch_id):
    if not batch_data: return
    try:
        df = pd.DataFrame(batch_data)
        batch_file = os.path.join(TEMP_BATCH_DIR, f"prod_batch_{batch_id:04d}.parquet")
        df.to_parquet(batch_file, index=False)
        logging.info(f"üíæ Saved batch {batch_id}: {len(df)} docs")
    except Exception as e:
        logging.error(f"Save batch error: {e}")

# --- CORE LOGIC (INFINITE SCROLL) ---
def crawl_category_full(session, db, category_group, seed_urls, batch_buffer, batch_counter):
    total_collected = 0
    
    for seed_url in seed_urls:
        # X√°c ƒë·ªãnh t√™n category c·ª• th·ªÉ t·ª´ URL ƒë·ªÉ l∆∞u v√†o c·ªôt doc_category
        # Vd: /linh-vuc/thue-phi -> "Thu·∫ø ph√≠"
        specific_cat_name = "VƒÉn b·∫£n ph√°p lu·∫≠t"
        if '/linh-vuc/' in seed_url:
            specific_cat_name = seed_url.split('/linh-vuc/')[-1].replace('-', ' ').title()
        elif '/loai-van-ban/' in seed_url:
            specific_cat_name = seed_url.split('/loai-van-ban/')[-1].replace('-', ' ').title()
        
        page = 1
        empty_pages_count = 0 
        
        # V√≤ng l·∫∑p v√¥ t·∫≠n, ch·ªâ d·ª´ng khi kh√¥ng c√≤n b√†i
        while True:
            # D·ª´ng n·∫øu 3 trang li√™n ti·∫øp kh√¥ng t√¨m th·∫•y b√†i m·ªõi
            if empty_pages_count >= 3:
                logging.info(f"D·ª´ng qu√©t {specific_cat_name} t·∫°i trang {page} (H·∫øt d·ªØ li·ªáu)")
                break

            # URL ph√¢n trang
            current_url = seed_url if page == 1 else f"{seed_url}?page={page}"
            
            # L·∫•y links
            doc_links = find_document_links(session, current_url)
            
            if not doc_links:
                logging.warning(f"{specific_cat_name} - Trang {page}: Kh√¥ng c√≥ link n√†o.")
                empty_pages_count += 1
                page += 1
                continue

            # Duy·ªát b√†i
            new_in_page = 0
            # D√πng tqdm nh∆∞ng ·∫©n b·ªõt ƒë·ªÉ ƒë·ª° spam console khi ch·∫°y l√¢u
            for doc_url in doc_links:
                if db.exists(doc_url):
                    continue
                
                doc = extract_legal_document(session, doc_url)
                if doc:
                    # --- [QUAN TR·ªåNG] ƒê·ªîI T√äN C·ªòT CHO KH·ªöP HEADER ---
                    batch_buffer.append({
                        "doc_title": doc['title'],       # Kh·ªõp v·ªõi y√™u c·∫ßu
                        "doc_category": specific_cat_name, # Kh·ªõp v·ªõi y√™u c·∫ßu
                        "doc_url": doc['url'],           # Kh·ªõp v·ªõi y√™u c·∫ßu
                        "doc_content": doc['text'],      # D·ªØ li·ªáu g·ªëc ƒë·ªÉ chunking
                        "crawled_at": datetime.now().isoformat()
                    })
                    
                    db.add(doc_url, specific_cat_name, 'success')
                    new_in_page += 1
                    total_collected += 1
                    
                    if len(batch_buffer) >= CHECKPOINT_SIZE:
                        save_batch(batch_buffer, batch_counter[0])
                        batch_counter[0] += 1
                        batch_buffer.clear()
                
                # Delay ng·∫´u nhi√™n ƒë·ªÉ kh√¥ng b·ªã block IP
                time.sleep(random.uniform(0.5, 1.2))

            if new_in_page > 0:
                print(f"[{specific_cat_name}] Page {page}: +{new_in_page} b√†i m·ªõi.")
                empty_pages_count = 0 # Reset bi·∫øn ƒë·∫øm n·∫øu t√¨m th·∫•y b√†i
            else:
                empty_pages_count += 1
                
            page += 1
            
    return total_collected

# --- MAIN RUN ---
def main():
    print("üöÄ B·∫ÆT ƒê·∫¶U CRAWL PRODUCTION (FULL D·ªÆ LI·ªÜU)...")
    print(f"üì¶ Output s·∫Ω l∆∞u t·∫°i: {TEMP_BATCH_DIR}")
    print("‚ö†Ô∏è  L∆∞u √Ω: Qu√° tr√¨nh n√†y c√≥ th·ªÉ k√©o d√†i nhi·ªÅu gi·ªù.")
    
    session = get_session()
    batch_buffer = []
    batch_counter = [0]
    
    with HistoryDB(DB_FILE) as db:
        for group_name, seeds in SEED_CATEGORIES.items():
            print(f"\nüìÇ ƒêang x·ª≠ l√Ω nh√≥m: {group_name.upper()}")
            # T·∫°o full URLs
            full_seeds = [urljoin("https://vanbanphapluat.co", s) for s in seeds]
            
            crawl_category_full(session, db, group_name, full_seeds, batch_buffer, batch_counter)
    
    # Save n·ªët batch cu·ªëi
    if batch_buffer:
        save_batch(batch_buffer, batch_counter[0])

    # Merge file cu·ªëi c√πng
    print("\nüì¶ ƒêang g·ªôp to√†n b·ªô d·ªØ li·ªáu...")
    all_files = [os.path.join(TEMP_BATCH_DIR, f) for f in os.listdir(TEMP_BATCH_DIR) if f.endswith('.parquet')]
    
    if all_files:
        combined_df = pd.concat([pd.read_parquet(f) for f in tqdm(all_files, desc="Merging")], ignore_index=True)
        
        # Deduplicate l·∫ßn cu·ªëi (tr√°nh tr√πng l·∫∑p do 1 b√†i thu·ªôc nhi·ªÅu danh m·ª•c)
        combined_df.drop_duplicates(subset=['doc_url'], keep='first', inplace=True)
        
        combined_df.to_parquet(OUTPUT_FILE, index=False)
        print(f"‚úÖ HO√ÄN T·∫§T! T·ªïng s·ªë vƒÉn b·∫£n: {len(combined_df):,}")
        print(f"üìÅ File k·∫øt qu·∫£: {OUTPUT_FILE}")
        
        # (Tu·ª≥ ch·ªçn) X√≥a file t·∫°m ƒë·ªÉ gi·∫£i ph√≥ng ·ªï c·ª©ng
        for f in all_files: os.remove(f)
        os.rmdir(TEMP_BATCH_DIR)
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c thu th·∫≠p.")

if __name__ == "__main__":
    main()