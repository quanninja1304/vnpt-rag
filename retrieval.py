import asyncio
from qdrant_client import QdrantClient
from config import Config
from vnpt_client import get_vnpt_embedding, call_vnpt_llm
import json

# --- Cáº¤U HÃŒNH ---
TOP_K = 5 # Láº¥y 5 Ä‘oáº¡n vÄƒn báº£n liÃªn quan nháº¥t (Vá»›i chunk to, 5 Ä‘oáº¡n lÃ  ráº¥t nhiá»u thÃ´ng tin)

async def search_qdrant(query_text):
    """TÃ¬m kiáº¿m semantic search trÃªn Qdrant"""
    # 1. Embed cÃ¢u há»i
    query_vector = get_vnpt_embedding(query_text)
    if not query_vector:
        print("âŒ Lá»—i embedding cÃ¢u há»i")
        return []

    # 2. Search Qdrant
    client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)
    
    search_result = client.search(
        collection_name=Config.COLLECTION_NAME,
        query_vector=query_vector,
        limit=TOP_K,
        with_payload=True
    )
    
    return search_result

def build_prompt(query, retrieved_chunks):
    """GhÃ©p context vÃ o prompt"""
    context_text = ""
    for i, hit in enumerate(retrieved_chunks):
        # Format: [Document Title] Content
        context_text += f"\n--- TÃ€I LIá»†U {i+1} (Nguá»“n: {hit.payload.get('title', 'Unknown')}) ---\n"
        context_text += hit.payload.get('text', '') + "\n"

    # Prompt Template (Tá»‘i Æ°u cho Tiáº¿ng Viá»‡t & Tráº¯c nghiá»‡m)
    prompt = [
                {"role": "system", "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh tham gia cuá»™c thi há»i Ä‘Ã¡p vá» Viá»‡t Nam.
        Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i dá»±a CHÃNH XÃC vÃ  DUY NHáº¤T trÃªn cÃ¡c Ä‘oáº¡n vÄƒn báº£n Ä‘Æ°á»£c cung cáº¥p bÃªn dÆ°á»›i.
        Náº¿u thÃ´ng tin khÃ´ng cÃ³ trong vÄƒn báº£n, hÃ£y tráº£ lá»i lÃ  khÃ´ng biáº¿t, Ä‘á»«ng bá»‹a ra.
        Äá»‘i vá»›i cÃ¢u há»i tráº¯c nghiá»‡m, hÃ£y suy luáº­n vÃ  chá»n Ä‘Ã¡p Ã¡n Ä‘Ãºng nháº¥t (A, B, C, hoáº·c D)."""},
                
                {"role": "user", "content": f"""
        DÆ°á»›i Ä‘Ã¢y lÃ  thÃ´ng tin tham kháº£o:
        {context_text}

        ----------------
        CÃ‚U Há»I: {query}
        ----------------
        HÃ£y Ä‘Æ°a ra cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng:"""}
            ]
    return prompt

async def run_test(question):
    print(f"â“ Äang há»i: {question}")
    
    # 1. Retrieval
    results = await search_qdrant(question)
    if not results:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u liÃªn quan.")
        return

    print(f"âœ… TÃ¬m tháº¥y {len(results)} chunks. Top 1 score: {results[0].score:.4f}")
    # In thá»­ tiÃªu Ä‘á» top 1 xem cÃ³ Ä‘Ãºng chá»§ Ä‘á» khÃ´ng
    print(f"   -> Top 1 Document: {results[0].payload['title']}")

    # 2. Generation
    messages = build_prompt(question, results)
    
    # 3. Call LLM
    print("ğŸ¤– Äang suy nghÄ©...")
    answer = call_vnpt_llm(messages, model=Config.LLM_MODEL_LARGE) # DÃ¹ng Large cho cháº¯c
    
    print("\n" + "="*50)
    print("CÃ‚U TRáº¢ Lá»œI Cá»¦A MODEL:")
    print(answer)
    print("="*50)

if __name__ == "__main__":
    # Test thá»­ vá»›i dá»¯ liá»‡u CÆ° Bao báº¡n vá»«a index
    # test_question = "Theo nghá»‹ quyáº¿t nÄƒm 2025, diá»‡n tÃ­ch tá»± nhiÃªn cá»§a phÆ°á»ng CÆ° Bao má»›i lÃ  bao nhiÃªu?"
    # Hoáº·c test cÃ¢u tráº¯c nghiá»‡m
    # test_question = "Äáº¿n nÄƒm 2025, phÆ°á»ng CÆ° Bao thuá»™c Ä‘Æ¡n vá»‹ hÃ nh chÃ­nh nÃ o? \nA. Huyá»‡n KrÃ´ng BÃºk\nB. Thá»‹ xÃ£ BuÃ´n Há»“\nC. ThÃ nh phá»‘ BuÃ´n Ma Thuá»™t\nD. Tá»‰nh Äáº¯k NÃ´ng"
    test_question = "Há»“ ChÃ­ Minh lÃ  ai?"
    asyncio.run(run_test(test_question))