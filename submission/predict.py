import asyncio
import sys
import json
import logging
import pandas as pd
import aiohttp
from pathlib import Path
import random

# Qdrant
from qdrant_client import AsyncQdrantClient

# modules
from config import Config, TIMEOUT_PER_QUESTION, LIMITER_EMBED, LIMITER_LARGE, LIMITER_SMALL
from utils.logger import logger
from core.retriever import HybridRetriever
from core.logic import process_row_logic


Config.LOGS_DIR.mkdir(parents=True, exist_ok=True)

# File l∆∞u k·∫øt qu·∫£ (D√πng ƒë·ªÉ Resume)
OUTPUT_FILE = Config.OUTPUT_FILE
DEBUG_LOG_FILE = Config.DEBUG_LOG_FILE

# Constants
BM25_INDEX_DIR = Config.BM25_INDEX_DIR
BM25_IDS_FILE = Config.BM25_IDS_FILE
BM25_META_FILE = Config.BM25_META_FILE

async def main():
    # 1. Load Data
    # files = [Config.BASE_DIR / "data" / "val.json", Config.BASE_DIR / "data" / "test.json"]
    # files = [Config.BASE_DIR / "data" / "test.json"]
    input_file = Config.INPUT_FILE
    if not input_file: 
        logger.error("‚ùå Input file not found!")
        return
    
    with open(input_file, 'r', encoding='utf-8') as f: data = json.load(f)

    # 2. Check Resume (ƒê·ªçc file ƒë√£ l∆∞u ƒë·ªÉ ch·∫°y ti·∫øp)
    processed_ids = set()
    if OUTPUT_FILE.exists():
        try:
            df_done = pd.read_csv(OUTPUT_FILE)
            processed_ids = set(df_done['qid'].astype(str))
            logger.info(f"RESUMING... Found {len(processed_ids)} processed questions.")
        except: pass
    
    # L·ªçc ra nh·ªØng c√¢u ch∆∞a l√†m
    data_to_process = [r for r in data if str(r.get('qid', r.get('id'))) not in processed_ids]
    
    if not data_to_process:
        logger.info("‚úÖ ALL DONE! Nothing to process.")
        return

    logger.info(f"üöÄ REMAINING: {len(data_to_process)}/{len(data)} questions")

    # 3. Setup Qdrant & Retriever
    # qdrant_client = AsyncQdrantClient(
    #     url=Config.QDRANT_URL,
    #     api_key=Config.QDRANT_API_KEY,
    #     timeout=30,  # TƒÉng timeout
    #     # Th√™m config pool
    #     grpc_options={
    #         'grpc.max_connection_idle_ms': 60000,  # 60s
    #         'grpc.keepalive_time_ms': 30000,       # 30s
    #         'grpc.http2.max_pings_without_data': 0,
    #     }
    # )

    retriever = HybridRetriever(
        collection_name=Config.COLLECTION_NAME
    )

    stats = {'used_large': 0, 'used_small': 0}
    
    # 4. Run Sequential (V√≤ng l·∫∑p ƒë∆°n lu·ªìng - AN TO√ÄN NH·∫§T)
    # limit=1 ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ c√≥ 1 request t·∫°i 1 th·ªùi ƒëi·ªÉm
    sem = asyncio.Semaphore(Config.MAX_CONCURRENT_TASKS) 
    
    # Lock: ƒê·∫£m b·∫£o khi ghi file kh√¥ng b·ªã tranh ch·∫•p
    write_lock = asyncio.Lock()
    
    # Connection Pool l·ªõn h∆°n s·ªë task m·ªôt ch√∫t
    conn = aiohttp.TCPConnector(limit=0) # limit=0 ƒë·ªÉ Semaphore lo vi·ªác gi·ªõi h·∫°n

    # --- WORKER FUNCTION (Ch·∫°y song song) ---
    async def worker(session, row):
        async with sem: # Chi·∫øm 1 slot
            qid = str(row.get('qid', row.get('id')))
            
            try:
                # Jitter nh·∫π ƒë·ªÉ tr√°nh g·ª≠i request ƒë·ªìng lo·∫°t ƒë√∫ng 1 th·ªùi ƒëi·ªÉm
                await asyncio.sleep(random.uniform(0.1, 1.5))
                
                # G·ªçi x·ª≠ l√Ω ch√≠nh (ƒë√£ bao g·ªìm retry b√™n trong r·ªìi)
                result = await asyncio.wait_for(
                    process_row_logic(session, retriever, row, stats=None),
                    timeout=TIMEOUT_PER_QUESTION
                )
                
                # Chu·∫©n h√≥a output (H√†m ch·ªët ch·∫∑n t√¥i ƒë√£ g·ª≠i tr∆∞·ªõc ƒë√≥)
                # final_result = standardize_submission_output(result, row) 
                # (N·∫øu ch∆∞a c√≥ h√†m tr√™n th√¨ d√πng result tr·ª±c ti·∫øp nh∆∞ng r·ªßi ro h∆°n)
                final_result = result if result else {"qid": qid, "answer": "A"}

                # Ghi file an to√†n (Thread-safe write)
                async with write_lock:
                    df_res = pd.DataFrame([final_result])
                    need_header = not OUTPUT_FILE.exists()
                    df_res[['qid', 'answer']].to_csv(OUTPUT_FILE, mode='a', header=need_header, index=False)
                    logger.info(f"üíæ Saved Q:{qid}")

            except asyncio.TimeoutError:
                logger.error(f"‚è∞ TIMEOUT Q:{qid}")
                # Fallback ghi A ƒë·ªÉ kh√¥ng m·∫•t b√†i
                async with write_lock:
                    pd.DataFrame([{"qid": qid, "answer": "A"}]).to_csv(OUTPUT_FILE, mode='a', header=not OUTPUT_FILE.exists(), index=False)
            
            except Exception as e:
                logger.error(f"‚ùå ERROR Q:{qid}: {e}")
                async with write_lock:
                    pd.DataFrame([{"qid": qid, "answer": "A"}]).to_csv(OUTPUT_FILE, mode='a', header=not OUTPUT_FILE.exists(), index=False)

    # 5. EXECUTE BATCH
    async with aiohttp.ClientSession(connector=conn) as session:
        tasks = []
        for row in data_to_process:
            # T·∫°o task nh∆∞ng ch∆∞a await ngay -> N√≥ s·∫Ω ch·∫°y n·ªÅn
            task = asyncio.create_task(worker(session, row))
            tasks.append(task)
        
        # Ch·ªù t·∫•t c·∫£ xong
        await asyncio.gather(*tasks)

    await retriever.client.close()
    logger.info("üéâ BATCH COMPLETED!")
    
    # 6. Verify Output
    if OUTPUT_FILE.exists():
        logger.info(f"üìÅ Output saved to: {OUTPUT_FILE}")

if __name__ == "__main__":
    if sys.platform == 'win32': asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())