import requests
from bs4 import BeautifulSoup
import trafilatura
import pandas as pd
import time
import random
import re
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urljoin

# --- C·∫§U H√åNH ---
OUTPUT_FILE = "output/vietjack_history_geo.parquet"

# Danh s√°ch trang M·ª•c l·ª•c (Index) c√°c m√¥n
# L∆∞u √Ω: L·ªõp 10, 11 ch∆∞∆°ng tr√¨nh m·ªõi th∆∞·ªùng chia 3 b·ªô s√°ch (KNTT, CD, CTST). 
# ·ªû ƒë√¢y t√¥i ch·ªçn b·ªô "K·∫øt N·ªëi Tri Th·ª©c" (ph·ªï bi·∫øn nh·∫•t) l√†m m·∫´u. B·∫°n c√≥ th·ªÉ th√™m link b·ªô kh√°c n·∫øu c·∫ßn.
SUBJECT_INDEXES = [
    # --- L·ªöP 12 (Ch∆∞∆°ng tr√¨nh c≈© - V·∫´n thi THPTQG 2025 theo form n√†y nhi·ªÅu) ---
    {"url": "https://vietjack.com/lich-su-12/index.jsp", "category": "L·ªãch s·ª≠ 12", "match": "lich-su-12"},
    {"url": "https://vietjack.com/dia-li-12/index.jsp", "category": "ƒê·ªãa l√Ω 12", "match": "dia-li-12"},
    
    # --- L·ªöP 11 (K·∫øt n·ªëi tri th·ª©c) ---
    {"url": "https://vietjack.com/lich-su-11-kn/index.jsp", "category": "L·ªãch s·ª≠ 11", "match": "lich-su-11"},
    {"url": "https://vietjack.com/dia-li-11-kn/index.jsp", "category": "ƒê·ªãa l√Ω 11", "match": "dia-li-11"},

    # --- L·ªöP 10 (K·∫øt n·ªëi tri th·ª©c) ---
    {"url": "https://vietjack.com/lich-su-10-kn/index.jsp", "category": "L·ªãch s·ª≠ 10", "match": "lich-su-10"},
    {"url": "https://vietjack.com/dia-li-10-kn/index.jsp", "category": "ƒê·ªãa l√Ω 10", "match": "dia-li-10"},
]

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def get_lesson_links(index_info):
    """B∆∞·ªõc 1: L·∫•y danh s√°ch link b√†i h·ªçc t·ª´ trang m·ª•c l·ª•c"""
    url = index_info['url']
    category = index_info['category']
    match_pattern = index_info['match']
    
    print(f"üîç Scanning Index: {category} ({url})...")
    links = []
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # T√¨m t·∫•t c·∫£ th·∫ª a
        # Vietjack th∆∞·ªùng ƒë·ªÉ b√†i h·ªçc trong th·∫ª <a> c√≥ href ch·ª©a t√™n m√¥n
        all_links = soup.find_all('a', href=True)
        
        seen_links = set()
        
        for a in all_links:
            href = a['href']
            # 1. L·ªçc link ch·ª©a pattern m√¥n (v√≠ d·ª• 'lich-su-12')
            # 2. L·ªçc b·ªè link b√†i t·∫≠p/tr·∫Øc nghi·ªám (ch·ªâ l·∫•y b√†i h·ªçc/l√Ω thuy·∫øt/gi·∫£i b√†i t·∫≠p sgk c√≥ n·ªôi dung)
            # 3. Tr√°nh link tr√πng
            if match_pattern in href and href not in seen_links:
                # Lo·∫°i b·ªè c√°c link kh√¥ng ph·∫£i b√†i h·ªçc ch√≠nh (v√≠ d·ª• link v·ªÅ t√°c gi·∫£, qu·∫£ng c√°o)
                if any(x in href for x in ['facebook', 'youtube', '#']): continue
                
                full_url = urljoin("https://vietjack.com/", href)
                title = a.get_text().strip()
                
                # ∆Øu ti√™n c√°c link c√≥ ti√™u ƒë·ªÅ b·∫Øt ƒë·∫ßu b·∫±ng "B√†i", "Ch∆∞∆°ng", "L√Ω thuy·∫øt"
                if len(title) > 5: 
                    links.append({"url": full_url, "title": title, "category": category})
                    seen_links.add(href)
                    
        print(f"   -> Found {len(links)} lessons for {category}.")
        return links
        
    except Exception as e:
        print(f"‚ùå Error scanning index {url}: {e}")
        return []

def clean_text(text):
    if not text: return ""
    # X√≥a r√°c Vietjack
    garbage = ["Qu·∫£ng c√°o", "Xem th√™m", "T·∫£i v·ªÅ", "M·ª•c l·ª•c", "B·∫£n in", "Trang ch·ªß", "VietJack", "B√¨nh lu·∫≠n", "Theo d√µi ch√∫ng t√¥i"]
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    clean_lines = [line for line in lines if not any(g.lower() in line.lower() for g in garbage)]
    # B·ªè d√≤ng qu√° ng·∫Øn (th∆∞·ªùng l√† menu)
    clean_lines = [line for line in clean_lines if len(line) > 5]
    return "\n".join(clean_lines)

def scrape_content(target):
    """B∆∞·ªõc 2: C√†o n·ªôi dung chi ti·∫øt"""
    url = target['url']
    title = target['title']
    category = target['category']
    
    # print(f"üï∑Ô∏è Crawling: {title[:30]}...") # Uncomment n·∫øu mu·ªën log chi ti·∫øt
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        
        # 1. Trafilatura Extract (Nhanh & S·∫°ch)
        content = ""
        try:
            content = trafilatura.extract(response.text, include_comments=False, include_tables=True)
        except: pass
        
        # 2. Fallback BS4 (N·∫øu Trafilatura fail)
        if not content or len(content) < 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            # Vietjack content hay n·∫±m trong div class 'content' ho·∫∑c 'middle-col'
            main_div = soup.find('div', class_='content') or soup.find('div', class_='middle-col') or soup.body
            if main_div:
                content = main_div.get_text(separator='\n')
        
        content = clean_text(content)
        
        if len(content) < 100: # N·ªôi dung qu√° ng·∫Øn -> B·ªè qua (c√≥ th·ªÉ l√† trang l·ªói)
            return None

        # Format chu·∫©n RAG
        # Inject Title v√† Category v√†o ƒë·∫ßu text
        full_vector_text = f"S√°ch gi√°o khoa: {category}. B√†i: {title}.\n{content}"
        
        return {
            "title": title,
            "url": url,
            "text": full_vector_text, # D√πng text n√†y ƒë·ªÉ embed
            "display_text": content, # D√πng text n√†y ƒë·ªÉ hi·ªÉn th·ªã (n·∫øu c·∫ßn t√°ch)
            "categories": [category],
            "doc_type": "textbook_lesson"
        }

    except Exception as e:
        # print(f"‚ùå Error {url}: {e}")
        return None

def main():
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    
    # B∆Ø·ªöC 1: L·∫§Y DANH S√ÅCH LINK
    all_targets = []
    print("üöÄ B·∫Øt ƒë·∫ßu qu√©t M·ª•c l·ª•c...")
    for index_info in SUBJECT_INDEXES:
        links = get_lesson_links(index_info)
        all_targets.extend(links)
    
    print(f"\nüî• T·ªïng c·ªông t√¨m th·∫•y {len(all_targets)} b√†i h·ªçc. B·∫Øt ƒë·∫ßu c√†o n·ªôi dung...")
    
    # B∆Ø·ªöC 2: C√ÄO N·ªòI DUNG (ƒêA LU·ªíNG)
    final_data = []
    # D√πng 10 workers cho nhanh
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(scrape_content, target) for target in all_targets]
        
        for i, future in enumerate(futures):
            result = future.result()
            if result:
                final_data.append(result)
            
            # Log ti·∫øn ƒë·ªô m·ªói 50 b√†i
            if (i + 1) % 50 == 0:
                print(f"   ‚úÖ Progress: {i + 1}/{len(all_targets)}...")

    print(f"\n‚úÖ Ho√†n t·∫•t! Thu th·∫≠p ƒë∆∞·ª£c {len(final_data)} b√†i h·ªçc.")
    
    if final_data:
        df = pd.DataFrame(final_data)
        df['crawled_at'] = datetime.now().isoformat()
        
        # Mapping c·ªôt cho chu·∫©n pipeline c≈©
        # N·∫øu pipeline c≈© c·ªßa b·∫°n d√πng c·ªôt 'text' ƒë·ªÉ embed th√¨ code n√†y ƒë√£ chu·∫©n.
        # N·∫øu pipeline c≈© d√πng 'vector_text', h√£y rename:
        # df = df.rename(columns={'text': 'vector_text'}) 
        
        df.to_parquet(OUTPUT_FILE, index=False)
        print(f"üíæ ƒê√£ l∆∞u file Parquet t·∫°i: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()