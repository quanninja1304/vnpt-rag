import pandas as pd
import re
import json
import os
import logging
from tqdm import tqdm
from pathlib import Path
from typing import List, Dict, Any
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Import Config tá»« file config cá»§a báº¡n
from config import Config

logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. TEXT CLEANING & STRUCTURE PRESERVATION (Core Logic tá»« chunking_stem)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def clean_wiki_text_master(text: str) -> str:
    """
    LÃ m sáº¡ch vÄƒn báº£n nhÆ°ng giá»¯ nguyÃªn cáº¥u trÃºc Báº£ng, List, CÃ´ng thá»©c.
    """
    if not text: return ""
    
    # 1. Footer Detection (Cáº¯t bá» pháº§n tham kháº£o rÃ¡c)
    stop_phrases = ['tham kháº£o', 'liÃªn káº¿t ngoÃ i', 'chÃº thÃ­ch', 'Ä‘á»c thÃªm', 
                    'nguá»“n', 'xem thÃªm', 'thÆ° má»¥c', 'bÃ i liÃªn quan']
    
    lines = text.split('\n')
    cut_index = len(lines)
    
    for i, line in enumerate(lines):
        line_clean = line.strip().lower()
        # Header ngáº¯n + Chá»©a tá»« khÃ³a dá»«ng = Footer
        is_heading = (line_clean.startswith('=') or line_clean.startswith('#') or line == line.upper())
        
        # Bá» cÃ¡c kÃ½ tá»± trang trÃ­ Ä‘á»ƒ check
        line_core = line_clean.strip('=#-=:. ')
        
        if len(line_clean) < 50 and is_heading and line_core in stop_phrases:
            cut_index = i
            break
    
    content_lines = lines[:cut_index]
    
    # 2. Structure Detection (State Machine)
    processed_blocks = []
    current_block = []
    in_structure = None  # None | 'table' | 'list' | 'formula'
    
    for line in content_lines:
        line = line.strip()
        if not line:
            if in_structure: current_block.append("")
            continue
        
        # XÃ³a citation [1], [2]
        line = re.sub(r'\[(?:\d+|cáº§n dáº«n nguá»“n|citation needed)\]', '', line)
        
        # Detect types
        is_table = line.startswith('|')
        is_list = bool(re.match(r'^[\-\*â€¢]\s+|^\d+\.\s+', line))
        is_formula = bool(re.search(r'\$|\\[a-z]+\{|[âˆ‘âˆ«âˆšÂ±â‰ â‰¤â‰¥]', line))
        
        # State Machine Logic
        if is_table:
            if in_structure != 'table':
                if current_block: processed_blocks.append(_join_block(current_block, in_structure))
                current_block = []
                in_structure = 'table'
            current_block.append(line)
        elif is_list:
            if in_structure != 'list':
                if current_block: processed_blocks.append(_join_block(current_block, in_structure))
                current_block = []
                in_structure = 'list'
            current_block.append(line)
        elif is_formula:
            if in_structure != 'formula':
                if current_block: processed_blocks.append(_join_block(current_block, in_structure))
                current_block = []
                in_structure = 'formula'
            current_block.append(line)
        else:
            # Regular text
            if in_structure:
                if current_block: processed_blocks.append(_join_block(current_block, in_structure))
                current_block = []
                in_structure = None
            current_block.append(line)
    
    if current_block:
        processed_blocks.append(_join_block(current_block, in_structure))
    
    return '\n\n'.join(processed_blocks)

def _join_block(lines: List[str], structure_type: str) -> str:
    # Cáº¥u trÃºc Ä‘áº·c biá»‡t thÃ¬ giá»¯ nguyÃªn xuá»‘ng dÃ²ng
    if structure_type in ['table', 'list', 'formula']:
        return '\n'.join(lines)
    # VÄƒn báº£n thÆ°á»ng thÃ¬ ná»‘i láº¡i thÃ nh Ä‘oáº¡n vÄƒn báº±ng \n (Ä‘á»ƒ splitter dá»… cáº¯t hÆ¡n lÃ  space)
    return '\n'.join(lines)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. DOMAIN-SPECIFIC SPLITTERS (Káº¿t há»£p cáº£ 2 logic)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_domain_splitter(domain: str = "general") -> RecursiveCharacterTextSplitter:
    """Táº¡o Splitter tá»‘i Æ°u cho tá»«ng lÄ©nh vá»±c"""
    chunk_size = 1024
    chunk_overlap = 200
    
    if domain == "legal":
        # Æ¯u tiÃªn cáº¥u trÃºc Luáº­t
        separators = ["\n\nÄiá»u ", "\n\nKhoáº£n ", "\n\nChÆ°Æ¡ng ", "\n\n", "\n", "; ", ". ", " "]
    elif domain == "stem":
        # Æ¯u tiÃªn cáº¥u trÃºc ToÃ¡n/LÃ½
        separators = ["\n\n", "\n### ", "\n- ", "\n1. ", "\n", "; ", ". ", " "]
    else: 
        # General (Wiki thÆ°á»ng): Logic cá»§a chunking_wiki.py nhÆ°ng tá»‘i Æ°u hÆ¡n
        separators = ["\n\n", "\n", ". ", "; ", ", ", " ", ""]
    
    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=separators,
        length_function=len, # DÃ¹ng Ä‘á»™ dÃ i kÃ½ tá»± chuáº©n (tá»‘t hÆ¡n count_tokens cho regex)
        strip_whitespace=True
    )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. CONTENT DETECTION & FILTERING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def detect_content_type(text: str) -> Dict[str, bool]:
    result = {'has_math': False, 'has_legal': False, 'is_substantial': False}
    
    # Math Detection
    math_pattern = r'[+\-*/=<>^]{1,2}\s*\d|\d\s*[+\-*/=<>^]|\d+/\d+|âˆš\d+|\$|\\frac'
    result['has_math'] = bool(re.search(math_pattern, text))
    
    # Legal Detection
    legal_pattern = r'(?:Äiá»u|Khoáº£n|ChÆ°Æ¡ng)\s+\d+|Bá»™\s+luáº­t|Nghá»‹\s+Ä‘á»‹nh'
    result['has_legal'] = bool(re.search(legal_pattern, text, re.IGNORECASE))
    
    # Substantial (Äá»™ dÃ i ná»™i dung thá»±c)
    result['is_substantial'] = len(re.findall(r'\w+', text)) >= 15
    return result

def should_keep_chunk(content: str) -> bool:
    # Náº¿u dÃ i > 50 kÃ½ tá»± -> Giá»¯
    if len(content) >= 50: return True
    
    # Náº¿u ngáº¯n nhÆ°ng lÃ  cÃ´ng thá»©c hoáº·c Ä‘iá»u luáº­t -> Giá»¯
    info = detect_content_type(content)
    if info['has_math'] or info['has_legal']: return True
    
    # Giá»¯ láº¡i cÃ¡c má»‘c thá»i gian (Logic cá»§a chunking_wiki.py)
    is_timeline = any(kw in content for kw in ["NiÃªn biá»ƒu", "Sá»± kiá»‡n", "nÄƒm"])
    has_number = any(char.isdigit() for char in content)
    if is_timeline and has_number: return True
    
    return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. METADATA ENRICHMENT & ROUTING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def auto_detect_domain(category: str) -> str:
    """Tá»± Ä‘á»™ng xÃ¡c Ä‘á»‹nh loáº¡i bÃ i viáº¿t dá»±a trÃªn Category"""
    cat_lower = str(category).lower()
    
    stem_keywords = ['toÃ¡n', 'lÃ½', 'hÃ³a', 'sinh', 'tin', 'cÃ´ng nghá»‡', 'ká»¹ thuáº­t', 'khoa há»c']
    if any(kw in cat_lower for kw in stem_keywords):
        return 'stem'
        
    legal_keywords = ['luáº­t', 'nghá»‹ Ä‘á»‹nh', 'phÃ¡p luáº­t', 'hiáº¿n phÃ¡p', 'thÃ´ng tÆ°']
    if any(kw in cat_lower for kw in legal_keywords):
        return 'legal'
        
    return 'general'

def shorten_category(category: str) -> str:
    # RÃºt gá»n category Ä‘á»ƒ tiáº¿t kiá»‡m token embedding
    parts = [p.strip() for p in str(category).split('>')] if category else ["Tá»•ng há»£p"]
    if len(parts) > 1:
        return f"{parts[0].split('_')[0]}-{parts[-1]}".replace('_', ' ')
    return parts[0].replace('_', ' ')

def create_enriched_chunk(content, title, category, idx, url, domain):
    cat_short = shorten_category(category)
    
    # --- CONTEXT INJECTION ---
    # Format chuáº©n cho cáº£ STEM vÃ  Wiki thÆ°á»ng: [Category] Title \n Content
    # ÄÃ¢y lÃ  format tá»‘i Æ°u nháº¥t cho Vector Search
    vector_text = f"[{cat_short}] {title}\n{content}"
    
    info = detect_content_type(content)
    
    return {
        "chunk_id": f"{title}_{idx}",
        "doc_title": title,
        "doc_category": category,
        "vector_text": vector_text,   # DÃ¹ng Ä‘á»ƒ Embed
        "display_text": content,      # DÃ¹ng Ä‘á»ƒ hiá»ƒn thá»‹ cho LLM
        "doc_url": url,
        "metadata": {
            "has_math": info['has_math'],
            "has_legal": info['has_legal'],
            "domain": domain,
            "chunk_index": idx
        }
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. MAIN PROCESS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_chunking():
    # 1. Load Data
    if not Config.CRAWL_OUTPUT_PARQUET.exists():
        print(f"âŒ Missing file: {Config.CRAWL_OUTPUT_PARQUET}")
        return
    
    print("â³ Loading Parquet Data...")
    df = pd.read_parquet(Config.CRAWL_OUTPUT_PARQUET)
    
    # 2. Load State (Incremental Processing)
    processed_titles = set()
    if Config.CHUNKING_STATE_FILE.exists():
        try:
            with open(Config.CHUNKING_STATE_FILE, 'r', encoding='utf-8') as f:
                processed_titles = set(json.load(f))
        except: pass
            
    df_new = df[~df['title'].isin(processed_titles)]
    print(f"ğŸ“¦ Total: {len(df)} | ğŸ”„ New: {len(df_new)}")
    
    if len(df_new) == 0:
        print("âœ… No new articles to process.")
        return

    all_chunks = []
    
    print("ğŸš€ Starting Master Chunking Pipeline...")
    
    for idx, row in tqdm(df_new.iterrows(), total=len(df_new)):
        try:
            raw_text = row.get('text', '')
            title = row.get('title', 'Unknown')
            url = row.get('url', '')
            
            # Handle Category
            cats = row.get('categories', [])
            if isinstance(cats, list):
                cat_full = " > ".join(cats)
            else:
                cat_full = str(cats)
            
            # A. Detect Domain (Quyáº¿t Ä‘á»‹nh cÃ¡ch xá»­ lÃ½)
            domain = auto_detect_domain(cat_full)
            
            # B. Clean Text (DÃ¹ng báº£n xá»‹n nháº¥t)
            cleaned_text = clean_wiki_text_master(raw_text)
            if len(cleaned_text) < 50:
                processed_titles.add(title)
                continue
            
            # C. Split (DÃ¹ng splitter tÆ°Æ¡ng á»©ng vá»›i Domain)
            splitter = get_domain_splitter(domain)
            
            # ThÃªm title vÃ o Ä‘áº§u Ä‘á»ƒ chunk 0 luÃ´n cÃ³ ngá»¯ cáº£nh
            text_with_header = f"# {title}\n\n{cleaned_text}" 
            raw_chunks = splitter.create_documents([text_with_header])
            
            # D. Enrich & Filter
            for i, chunk in enumerate(raw_chunks):
                content = chunk.page_content.strip()
                if not should_keep_chunk(content): continue
                
                chunk_data = create_enriched_chunk(content, title, cat_full, i, url, domain)
                all_chunks.append(chunk_data)
            
            processed_titles.add(title)

        except Exception as e:
            logger.error(f"Error processing {title}: {e}")
            continue

    # 3. Save Results
    if all_chunks:
        # Append mode logic
        if Config.LATEST_CHUNKS_FILE.exists():
            try:
                df_old = pd.read_parquet(Config.LATEST_CHUNKS_FILE)
                df_new_chunks = pd.DataFrame(all_chunks)
                # Äáº£m báº£o cá»™t khá»›p nhau
                df_final = pd.concat([df_old, df_new_chunks], ignore_index=True)
            except:
                df_final = pd.DataFrame(all_chunks)
        else:
            df_final = pd.DataFrame(all_chunks)
            
        # Ensure string types
        for col in ['chunk_id', 'vector_text', 'display_text', 'doc_title']:
            if col in df_final.columns:
                df_final[col] = df_final[col].astype(str)

        df_final.to_parquet(Config.LATEST_CHUNKS_FILE, index=False)
        print(f"ğŸ’¾ Saved total {len(df_final)} chunks to {Config.LATEST_CHUNKS_FILE}")
        
        with open(Config.CHUNKING_STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(list(processed_titles), f, ensure_ascii=False)
            
    print("âœ… Chunking pipeline finished.")

if __name__ == "__main__":
    process_chunking()