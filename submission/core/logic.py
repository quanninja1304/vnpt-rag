import logging
import asyncio
from typing import Dict, Any

import aiohttp

# Import t·ª´ c√°c module ƒë√£ t√°ch
from config import Config
from utils.logger import logger, write_debug_log
from utils.text_utils import (
    get_dynamic_options,
    find_true_refusal_key,
    find_no_info_key,
    extract_answer_strict,
    heuristic_answer_math,
    heuristic_answer_overlap
)

from core.router import unified_router_v3
from core.retriever import HybridRetriever
from core.llm_client import call_llm_generic
from core.prompts import (
    build_cot_prompt,
    build_simple_prompt,
    build_rag_instruction_fixed
)


async def process_row_logic(session, retriever, row, stats=None):
    qid = row.get('qid', row.get('id', 'unknown'))
    question = row.get('question', '')
    true_label = row.get('answer', None) # C√≥ th·ªÉ None n·∫øu l√† file test
    opts = get_dynamic_options(row)
    opt_text = "\n".join([f"{k}. {v}" for k, v in opts.items()])
    
    # ==========================================================================
    # B∆Ø·ªöC 0: PH√ÇN LO·∫†I C√ÇU H·ªéI (ROUTING)
    # ==========================================================================
    # G·ªçi Router V3 (C√≥ AI + Regex + Check ƒê√°p √°n)
    route = await unified_router_v3(session, question, opts)
    
    # CASE 1: B·ªä CH·∫∂N (SAFETY / TRAP DETECTED)
    if route["is_unsafe"]:
        ans = route["refusal_key"]
        # Log r√µ l√Ω do b·ªã ch·∫∑n
        logger.info(f"üö´ Q:{qid} {route['tag']} -> Ans:{ans}")
        write_debug_log(qid, question, route['tag'], "BLOCKED", ans, true_label, "Safety Block")
        return {"qid": qid, "answer": ans}

    # ==========================================================================
    # B∆Ø·ªöC 1: RETRIEVAL
    # ==========================================================================
    top_k = 8 if route["is_stem"] else 12
    docs = await retriever.search(session, question, top_k=top_k)
    context_text = " ".join([d['text'].lower() for d in docs])
    ctx_len = len(context_text)
    
    # ==========================================================================
    # B∆Ø·ªöC 2: MODEL & PROMPT SELECTION
    # ==========================================================================
    SAFE_LIMIT_LARGE = 37500
    
    # M·∫∑c ƒë·ªãnh theo Router
    use_large = route["use_large"]
    limit_note = ""
    
    # ƒêi·ªÅu ch·ªânh l·∫°i d·ª±a tr√™n Context Length (N·∫øu d√†i qu√° b·∫Øt bu·ªôc d√πng Small)
    if ctx_len > SAFE_LIMIT_LARGE:
        docs = docs[:5]
        # C·∫Øt m·ªói doc xu·ªëng 2000 k√Ω t·ª±
        docs = [{**d, 'text': d['text'][:2000]} for d in docs]
        context_text = " ".join([d['text'].lower() for d in docs])
        limit_note = f"(Trimmed context: {len(context_text)} docs)"
    
    # Ch·ªçn Model
    model = Config.LLM_MODEL_LARGE if use_large else Config.LLM_MODEL_SMALL
    
    # Ch·ªçn Prompt 
    if route["is_stem"]:
        msgs = build_cot_prompt(question, opt_text, docs, is_stem=True)
    elif model == Config.LLM_MODEL_LARGE:
        msgs = build_cot_prompt(question, opt_text, docs, is_stem=False)
    else:
        msgs = build_simple_prompt(question, opt_text, docs)

    # ==========================================================================
    # B∆Ø·ªöC 3: INFERENCE (G·ªåI API)
    # ==========================================================================
    raw = await call_llm_generic(session, msgs, model, stats)
    
    # Fallback n·∫øu model ch√≠nh l·ªói
    if not raw:
        fallback_model = Config.LLM_MODEL_SMALL if model == Config.LLM_MODEL_LARGE else Config.LLM_MODEL_LARGE
        raw = await call_llm_generic(session, msgs, fallback_model, stats)
        limit_note += f" -> Fallback {fallback_model}"

    # ==========================================================================
    # B∆Ø·ªöC 4: X·ª¨ L√ù REFUSAL (MODEL B·∫¢O KH√îNG BI·∫æT)
    # ==========================================================================
    refusal_phrases = ["kh√¥ng c√≥ th√¥ng tin", "kh√¥ng t√¨m th·∫•y", "kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p", "kh√¥ng ƒë·ªß c∆° s·ªü"]
    
    # N·∫øu model tr·∫£ l·ªùi c√≥ ch·ª©a c·ª•m t·ª´ t·ª´ ch·ªëi
    if raw and any(p in raw.lower() for p in refusal_phrases):
        # T√¨m ƒë√°p √°n "Kh√¥ng c√≥ th√¥ng tin" trong options (D√πng h√†m m·ªõi)
        no_info_opt = find_no_info_key(opts)
        
        if no_info_opt:
            logger.info(f"‚ÑπÔ∏è Q:{qid} Model Refusal -> Found NO_INFO Option {no_info_opt}")
            write_debug_log(qid, question, route['tag'], model, no_info_opt, true_label, "Model Refusal -> No Info")
            return {"qid": qid, "answer": no_info_opt}
        
        # N·∫øu kh√¥ng c√≥ ƒë√°p √°n "Kh√¥ng c√≥ th√¥ng tin" -> C√≥ th·ªÉ do RAG fail
        # √âp d√πng ki·∫øn th·ª©c n·ªôi t·∫°i (Force Knowledge)
        force_msgs = [
            {"role": "system", "content": "D√πng ki·∫øn th·ª©c c·ªßa b·∫°n ƒë·ªÉ ch·ªçn ƒë√°p √°n ƒë√∫ng nh·∫•t A/B/C/D. Kh√¥ng gi·∫£i th√≠ch."},
            {"role": "user", "content": f"C√¢u h·ªèi: {question}\nL·ª±a ch·ªçn:\n{opt_text}"}
        ]
        raw = await call_llm_generic(session, force_msgs, model, stats)
        limit_note += " -> Force Know"

    # ==========================================================================
    # B∆Ø·ªöC 5: TR√çCH XU·∫§T ƒê√ÅP √ÅN & FINAL CHECK
    # ==========================================================================
    ans = extract_answer_strict(raw, opts)

    trap_key, trap_type = find_true_refusal_key(opts)

    # [CHECK 1] NO_INFO HINT FALLBACK
    if route["refusal_key"] and "NO_INFO" in route["tag"]:
        model_uncertain = not ans or (raw and "kh√¥ng" in raw.lower() and "th√¥ng tin" in raw.lower())
        if model_uncertain:
            logger.info(f"‚ÑπÔ∏è Q:{qid} Model uncertain -> Fallback to NO_INFO Hint")
            ans = route["refusal_key"]
            limit_note += " -> Hint NO_INFO"

    if ans:
        if ans == trap_key:
            # N·∫øu l√† TRAP ("T√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y" - chung chung) -> H·ª¶Y
            if trap_type == "TRAP":
                logger.warning(f"‚ö†Ô∏è Q:{qid} Generic Trap Detected ({ans}). Discarding.")
                ans = None
            
            # N·∫øu l√† SAFETY ("Vi ph·∫°m ph√°p lu·∫≠t" - c·ª• th·ªÉ) -> GI·ªÆ NGUY√äN (Tin Model)
            elif trap_type == "SAFETY":
                logger.info(f"üõ°Ô∏è Q:{qid} Model detected Safety Issue -> Keeping Refusal Ans: {ans}")
                # KH√îNG set ans = None

    # [CHECK 2.5] ANTI-LOGIC ("All of the above" Fallacy)
    # N·∫øu ch·ªçn "T·∫•t c·∫£" nh∆∞ng trong ƒë√≥ c√≥ 1 c√¢u l√† SAFETY/TRAP -> V√¥ l√Ω -> H·ªßy
    if ans and trap_key and trap_type in ["SAFETY", "TRAP"]:
        ans_text = opts.get(ans, "")
        is_all_above = any(p in ans_text.lower() for p in ["t·∫•t c·∫£", "c·∫£ ba", "c·∫£ 3", "m·ªçi ƒë√°p √°n", "c√°c √Ω tr√™n"])
        
        # Ch·ªâ h·ªßy n·∫øu ƒë√°p √°n "T·∫•t c·∫£" kh√°c v·ªõi ƒë√°p √°n Trap
        if is_all_above and ans != trap_key:
            logger.warning(f"‚ö†Ô∏è Q:{qid} Logical Fallacy! Picked 'All Above' ({ans}) but '{trap_key}' is a Trap. Discarding.")
            ans = None

    # [CHECK 3] HEURISTIC FALLBACK (Cleaned)
    heuristic_used = False
    if not ans:
        # T·∫°o danh s√°ch options "s·∫°ch" (lo·∫°i b·ªè c√¢u Trap ƒë·ªÉ Heuristic kh√¥ng ch·ªçn nh·∫ßm v√†o n√≥)
        clean_opts = opts.copy()
        
        # Ch·ªâ lo·∫°i b·ªè n·∫øu n√≥ l√† TRAP v√¥ nghƒ©a. N·∫øu l√† Safety/NoInfo th√¨ c·ª© ƒë·ªÉ ƒë√≥.
        if trap_key and trap_type == "TRAP":
            clean_opts.pop(trap_key, None)
        
        # N·∫øu l·ª° x√≥a h·∫øt (hi·∫øm) th√¨ d√πng l·∫°i c√°i c≈©
        target_opts = clean_opts if clean_opts else opts

        if route["is_stem"]:
            ans = heuristic_answer_math(question, target_opts)
        else:
            ans = heuristic_answer_overlap(question, target_opts)
        heuristic_used = True

    # ==========================================================================
    # LOGGING
    # ==========================================================================
    mod_name = model.split('_')[-1].upper()
    logger.info(f"Q:{qid} | Tag:{route['tag']} | Mod:{mod_name} | Ans:{ans}")

    write_debug_log(
        qid=qid,
        question=question,
        route_tag=route['tag'],
        model_used=f"{mod_name} {limit_note}",
        answer=ans,
        true_label=true_label,
        note="HEURISTIC" if heuristic_used else "EXTRACTED"
    )

    return {"qid": qid, "answer": ans}