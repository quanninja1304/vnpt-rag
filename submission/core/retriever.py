import asyncio
import pickle
import logging
import string
import re
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Set, Any
import uuid
import pickle
import asyncio
import logging
import string
import bm25s
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from underthesea import word_tokenize

from qdrant_client import AsyncQdrantClient
from qdrant_client.http import models

# Import n·ªôi b·ªô
from config import Config
from utils.logger import logger
from utils.text_utils import generate_uuid5
from core.llm_client import get_embedding_async, call_llm_generic

from underthesea import word_tokenize

TRANSLATOR = str.maketrans(string.punctuation, ' ' * len(string.punctuation))

class HybridRetriever:
    """
    Hybrid Retriever V2.1 - Network Stability Optimized
    - Added Qdrant Semaphore (Throttling)
    - Added Exponential Backoff
    - Reduced Batch Size
    """
    
    VECTOR_TIMEOUT = 20.0  # TƒÉng l√™n ch√∫t cho ch·∫Øc
    RETRIEVE_TIMEOUT = 10.0
    BATCH_SIZE = 20        # [FIX] Gi·∫£m batch size xu·ªëng 20 theo ƒë·ªÅ xu·∫•t
    RRF_K = 60
    
    def __init__(self, collection_name: str, top_k: int = 5, alpha_vector: float = 0.5):
        # [FIX] C·∫•u h√¨nh Client HTTP ·ªïn ƒë·ªãnh (T·∫Øt HTTP2, T·∫Øt gRPC)
        self.client = AsyncQdrantClient(
            url=Config.QDRANT_URL, 
            api_key=Config.QDRANT_API_KEY,
            timeout=30.0,
            prefer_grpc=False, 
            http2=False
        )
        
        self.collection_name = collection_name
        self.top_k = top_k
        self.alpha_vector = alpha_vector
        
        # [FIX] SEMAPHORE RI√äNG CHO QDRANT
        # D√π b√™n ngo√†i ch·∫°y 8 lu·ªìng, nh∆∞ng ch·ªâ cho ph√©p 3 lu·ªìng chui v√†o Qdrant c√πng l√∫c
        self.qdrant_sem = asyncio.Semaphore(2) 
        
        self.bm25_retriever = None
        self.chunk_ids = [] 
        self.bm25_loaded = False
        
        self._load_bm25s()
    
    def _load_bm25s(self) -> None:
        index_dir = Config.BASE_DIR / "resources" / "bm25s_index"
        id_map_file = Config.BASE_DIR / "resources" / "bm25s_ids.pkl"
        
        if not index_dir.exists():
            logger.warning(f"‚ùå BM25s not found at {index_dir}")
            return

        try:
            self.bm25_retriever = bm25s.BM25.load(str(index_dir), load_corpus=False, mmap=True)
            with open(id_map_file, "rb") as f:
                self.chunk_ids = pickle.load(f)
            self.bm25_loaded = True
            logger.info(f"‚úì BM25s Loaded: {len(self.chunk_ids)} docs")
        except Exception as e:
            logger.error(f"Failed to load BM25s: {e}")

    async def search(self, session, query: str, top_k: Optional[int] = None) -> List[Dict]:
        top_k = top_k or self.top_k
        
        vec_task = self._vector_search(session, query, top_k)
        bm25_task = self._bm25_search(query, top_k)
        
        (vec_hits_map, vec_scores), bm25_scores = await asyncio.gather(
            vec_task, bm25_task, return_exceptions=True
        )
        
        if isinstance(vec_hits_map, Exception):
            logger.error(f"Vector crash: {vec_hits_map}")
            vec_hits_map, vec_scores = {}, {}
        
        if isinstance(bm25_scores, Exception):
            bm25_scores = {}
        
        vec_hits_map, vec_scores = await self._fetch_missing_text_optimized(
            vec_hits_map, vec_scores, bm25_scores
        )
        
        return self._fuse_scores_rrf(vec_hits_map, vec_scores, bm25_scores)[:top_k]
    
    async def _vector_search(self, session, query: str, top_k: int) -> Tuple[Dict, Dict]:
        vec_hits_map = {}
        vec_scores = {}
        
        try:
            # 1. L·∫•y embedding (Th∆∞·ªùng l√† g·ªçi API kh√°c ho·∫∑c local, gi·ªØ nguy√™n)
            query_vec = await get_embedding_async(session, query)
            if not query_vec: 
                return {}, {}
            
            # [QUAN TR·ªåNG] S·ª≠ d·ª•ng Semaphore ƒë·ªÉ tr√°nh spam k·∫øt n·ªëi khi m·∫°ng ƒëang ngh·∫Ωn
            async with self.qdrant_sem:
                for attempt in range(3):
                    try:
                        # [T·ªêI ∆ØU 1] Gi·∫£m timeout m·ªói l·∫ßn th·ª≠ ƒë·ªÉ fail-fast v√† retry s·ªõm
                        # [T·ªêI ∆ØU 2] Ch·ªâ l·∫•y nh·ªØng field payload th·ª±c s·ª± c·∫ßn thi·∫øt ƒë·ªÉ gi·∫£m dung l∆∞·ª£ng g√≥i tin (r·∫•t quan tr·ªçng)
                        res = await asyncio.wait_for(
                            self.client.query_points(
                                collection_name=self.collection_name,
                                query=query_vec,
                                limit=top_k, # Kh√¥ng l·∫•y d∆∞ th·ª´a top_k * 2
                                with_payload=["text", "chunk_id", "title"], # [FIX] Kh√¥ng d√πng True, ch·ªâ l·∫•y field c·∫ßn
                                search_params={"hnsw_ef": 64, "exact": False} # [FIX] TƒÉng t·ªëc search, ch·∫•p nh·∫≠n ƒë·ªô ch√≠nh x√°c gi·∫£m nh·∫π
                            ),
                            timeout=self.VECTOR_TIMEOUT
                        )
                        
                        for point in res.points:
                            if not point.payload: continue
                            # D√πng .get ƒë·ªÉ an to√†n h∆°n
                            cid = str(point.payload.get('chunk_id', ''))
                            if cid:
                                vec_hits_map[cid] = point.payload
                                vec_scores[cid] = float(point.score)
                        
                        # N·∫øu c√≥ k·∫øt qu·∫£ th√¨ tho√°t v√≤ng l·∫∑p retry ngay
                        if vec_hits_map:
                            return vec_hits_map, vec_scores
                        
                        # N·∫øu th√†nh c√¥ng nh∆∞ng r·ªóng (do filter ho·∫∑c d·ªØ li·ªáu), kh√¥ng c·∫ßn retry
                        break

                    except (asyncio.TimeoutError, Exception) as e:
                        # [T·ªêI ∆ØU 3] Gi·∫£m Backoff ƒë·ªÉ kh√¥ng b·ªã t√≠ch t·ª• th·ªùi gian ch·ªù qu√° l√¢u
                        wait_time = 1.5 ** attempt 
                        if attempt < 2:
                            logger.warning(f"‚ö†Ô∏è Vector search retry {attempt+1}/3 (Latency: {wait_time}s)")
                            await asyncio.sleep(wait_time)
                        else:
                            logger.error(f"‚ùå Vector search GAVE UP sau 3 l·∫ßn th·ª≠.")

        except Exception as e:
            logger.error(f"üî• Vector search fatal error: {e}")
        
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£ (c√≥ th·ªÉ r·ªóng) ƒë·ªÉ BM25 g√°nh ph√≠a sau, kh√¥ng l√†m crash to√†n b·ªô pipeline
        return vec_hits_map, vec_scores
    
    async def _bm25_search(self, query: str, top_k: int) -> Dict[str, float]:
        bm25_scores = {}
        if not self.bm25_loaded: return bm25_scores
        
        try:
            text = str(query).lower().translate(TRANSLATOR)
            tokens = word_tokenize(text)
            query_tokens = [t for t in tokens if len(t.strip()) > 0]
            
            if not query_tokens: return bm25_scores
            
            doc_indices, doc_scores = self.bm25_retriever.retrieve([query_tokens], k=top_k * 2)
            
            for i, doc_idx in enumerate(doc_indices[0]):
                score = float(doc_scores[0][i])
                if score > 0:
                    bm25_scores[self.chunk_ids[doc_idx]] = score
                    
        except Exception:
            pass
        return bm25_scores
    
    async def _fetch_missing_text_optimized(self, vec_hits_map, vec_scores, bm25_scores):
        """
        [FIX] Optimized Batch Fetching:
        - Sequential Batches (Tu·∫ßn t·ª±)
        - Smaller Batch Size (20)
        - Retry with Backoff
        """
        missing_ids = [cid for cid in bm25_scores.keys() if cid not in vec_hits_map]
        if not missing_ids: return vec_hits_map, vec_scores
        
        # Chia batch nh·ªè (20 items)
        batches = [missing_ids[i:i + self.BATCH_SIZE] for i in range(0, len(missing_ids), self.BATCH_SIZE)]
        
        for batch_ids in batches:
            point_ids = [generate_uuid5(cid) for cid in batch_ids]
            
            # D√πng Semaphore ƒë·ªÉ gi·ªõi h·∫°n c·∫£ vi·ªác fetch
            async with self.qdrant_sem:
                for attempt in range(3):
                    try:
                        points = await asyncio.wait_for(
                            self.client.retrieve(
                                collection_name=self.collection_name,
                                ids=point_ids,
                                with_payload=True
                            ),
                            timeout=self.RETRIEVE_TIMEOUT
                        )
                        
                        for point in points:
                            if point.payload:
                                cid = str(point.payload['chunk_id'])
                                vec_hits_map[cid] = point.payload
                                if cid not in vec_scores: vec_scores[cid] = 0.0
                        
                        break # Th√†nh c√¥ng -> Tho√°t
                    
                    except Exception as e:
                        wait_time = 1 * (attempt + 1)
                        # logger.warning(f"Batch fetch retry {attempt+1} in {wait_time}s")
                        await asyncio.sleep(wait_time)
                        
                        if attempt == 2:
                            logger.error(f"Batch fetch GAVE UP: {e}")

        return vec_hits_map, vec_scores

    def _fuse_scores_rrf(self, vec_hits_map, vec_scores, bm25_scores) -> List[Dict]:
        def get_ranks(scores_dict):
            sorted_keys = sorted(scores_dict.keys(), key=lambda x: scores_dict[x], reverse=True)
            return {k: i for i, k in enumerate(sorted_keys)}
            
        vec_ranks = get_ranks(vec_scores)
        bm25_ranks = get_ranks(bm25_scores)
        
        final_results = []
        all_ids = set(vec_scores.keys()) | set(bm25_scores.keys())
        
        for cid in all_ids:
            if cid not in vec_hits_map: continue
            
            r_vec = vec_ranks.get(cid, float('inf'))
            r_bm25 = bm25_ranks.get(cid, float('inf'))
            
            score_v = 1.0 / (self.RRF_K + r_vec) if r_vec != float('inf') else 0.0
            score_b = 1.0 / (self.RRF_K + r_bm25) if r_bm25 != float('inf') else 0.0
            
            final_score = score_v * self.alpha_vector + score_b * (1 - self.alpha_vector)
            
            final_results.append({
                "chunk_id": cid,
                "text": vec_hits_map[cid].get('text', ''),
                "title": vec_hits_map[cid].get('title', ''),
                "score": final_score
            })
            
        final_results.sort(key=lambda x: x['score'], reverse=True)
        return final_results
    
    
async def rerank_with_small(session, question, initial_docs, top_n=8, stats=None):
    if not initial_docs: return []
    if len(initial_docs) <= top_n: return initial_docs

    # [T·ªêI ∆ØU] Ch·ªâ l·∫•y top 15 ƒë·ªÉ rerank, tr√°nh qu√° t·∫£i Model Small
    candidates = initial_docs[:15]

    # [T·ªêI ∆ØU] C·∫Øt ng·∫Øn text preview xu·ªëng 300 k√Ω t·ª± (ƒë·ªß ƒë·ªÉ AI bi·∫øt n·ªôi dung)
    docs_text = ""
    for i, doc in enumerate(candidates):
        clean_body = str(doc.get('text', '')).strip().replace("\n", " ")
        preview_text = " ".join(clean_body.split())[:300] 
        docs_text += f"ID [{i}]: {preview_text}...\n\n"

    system_prompt = """B·∫°n l√† chuy√™n gia l·ªçc tin.
NHI·ªÜM V·ª§: Ch·ªçn c√°c ID t√†i li·ªáu li√™n quan nh·∫•t ƒë·∫øn c√¢u h·ªèi.
OUTPUT JSON: {"ids": [0, 2, ...]}"""

    user_prompt = f"""C√ÇU H·ªéI: "{question}"

DANH S√ÅCH:
{docs_text}

CH·ªåN ID LI√äN QUAN NH·∫§T:"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    try:
        # [FIX 1] Th√™m timeout=30s ƒë·ªÉ fail fast n·∫øu model treo
        response = await call_llm_generic(
            session, messages, 
            Config.LLM_MODEL_SMALL, 
            stats, 
            max_tokens=100, 
            timeout=30 # Th√™m tham s·ªë timeout
        )
        
        # [FIX LOGIC] Check response
        if response:
            # T√¨m t·∫•t c·∫£ s·ªë trong response (ch·∫•p nh·∫≠n c·∫£ format [1, 2] ho·∫∑c 1, 2)
            found_indices = [int(s) for s in re.findall(r'\d+', response)]
            
            valid_docs = []
            seen = set()
            
            # L·∫•y docs theo th·ª© t·ª± AI ch·ªçn
            for idx in found_indices:
                if 0 <= idx < len(candidates) and idx not in seen:
                    valid_docs.append(candidates[idx])
                    seen.add(idx)
            
            # Backfill (N·∫øu AI ch·ªçn √≠t h∆°n top_n, l·∫•y th√™m t·ª´ danh s√°ch g·ªëc b√π v√†o)
            if len(valid_docs) < top_n:
                for i, doc in enumerate(candidates):
                    if i not in seen:
                        valid_docs.append(doc)
                        if len(valid_docs) >= top_n: break
            
            return valid_docs[:top_n]
        else:
            logger.warning("‚ö†Ô∏è Rerank Empty Response. Using original order.")

    except Exception as e:
        logger.warning(f"Rerank Error: {e}")
    
    # Fallback: Tr·∫£ v·ªÅ top_n ƒë·∫ßu ti√™n c·ªßa danh s√°ch g·ªëc
    return initial_docs[:top_n]