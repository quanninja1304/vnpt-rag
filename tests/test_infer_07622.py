import asyncio
import aiohttp
import pandas as pd
import json
import re
import pickle
import sys
import os
import time
import logging
import random
from pathlib import Path
from aiolimiter import AsyncLimiter
from qdrant_client import AsyncQdrantClient
from underthesea import word_tokenize
from config import Config

# ==============================================================================
# 0. C·∫§U H√åNH CHI·∫æN THU·∫¨T (Tactical Config)
# ==============================================================================
Config.LOGS_DIR.mkdir(parents=True, exist_ok=True)
Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# File l∆∞u k·∫øt qu·∫£ (D√πng ƒë·ªÉ Resume)
OUTPUT_FILE = Config.BASE_DIR / "output" / "submission.csv"

# C·∫•u h√¨nh ch·∫°y an to√†n tuy·ªát ƒë·ªëi
MAX_CONCURRENT_TASKS = 1      # Ch·∫°y t·ª´ng c√¢u m·ªôt (Ch·∫≠m nh∆∞ng ch·∫Øc 100%)
TIMEOUT_PER_QUESTION = 120    # 2 ph√∫t/c√¢u (ƒê·ªß ƒë·ªÉ retry n·∫øu m·∫°ng lag)

# Rate Limit (T·ªëc ƒë·ªô)
LIMITER_LARGE = AsyncLimiter(20, 60)   # 20 req/ph√∫t
LIMITER_SMALL = AsyncLimiter(50, 60)   # 50 req/ph√∫t
LIMITER_EMBED = AsyncLimiter(300, 60)  # 300 req/ph√∫t

# Ng√¢n s√°ch (Quota) - ƒê·ªÉ theo d√µi
QUOTA_LARGE = 500
QUOTA_SMALL = 1000

# Constants
THRESHOLD_SMALL_CONTEXT = 15000 
TOP_K = 18
ALPHA_VECTOR = 0.5
BM25_FILE = Config.OUTPUT_DIR / "bm25_index.pkl"

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler(Config.LOGS_DIR / 'inference_resume.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("VNPT_BOT")

# ==============================================================================
# 1. C√ÅC H√ÄM X·ª¨ L√ù (UTILS)
# ==============================================================================
def is_sensitive_topic(question):
    q_lower = question.lower()
    blacklist = [
        "sex", "khi√™u d√¢m", "ƒë·ªìi tr·ª•y", "l√†m t√¨nh", "·∫•u d√¢m", "k√≠ch d·ª•c",
        "ph·∫£n ƒë·ªông", "kh·ªßng b·ªë", "gi·∫øt ng∆∞·ªùi", "ma t√∫y", "bu√¥n l·∫≠u", "v≈© kh√≠", "b·∫°o l·ª±c",
        "x√∫c ph·∫°m", "lƒÉng m·∫°", "ƒë·∫£ng c·ªông s·∫£n", "xuy√™n t·∫°c", "c·ªù b·∫°c", "c√° ƒë·ªô"
    ]
    whitelist = [
        "lu·∫≠t", "ngh·ªã ƒë·ªãnh", "quy ƒë·ªãnh", "th√¥ng t∆∞", "ph√°p lu·∫≠t", "hi·∫øn ph√°p",
        "l·ªãch s·ª≠", "chi·∫øn tranh", "kh√°ng chi·∫øn", "v·ª• √°n", "t√≤a √°n", "x√©t x·ª≠",
        "t√°c h·∫°i", "ph√≤ng ch·ªëng", "ngƒÉn ch·∫∑n", "kh√°i ni·ªám", "ƒë·ªãnh nghƒ©a"
    ]
    has_bad = any(w in q_lower for w in blacklist)
    has_good = any(w in q_lower for w in whitelist)
    return has_bad and not has_good

# --- THAY TH·∫æ ƒêO·∫†N is_sensitive_topic C≈® B·∫∞NG ƒêO·∫†N N√ÄY ---
def check_keywords_sensitive(question):
    """L·ªçc th√¥ b·∫±ng t·ª´ kh√≥a - T·∫ßng 1"""
    q_lower = question.lower()
    # T·ª´ kh√≥a c·∫•m tuy·ªát ƒë·ªëi
    hard_ban = ["khi√™u d√¢m", "l√†m t√¨nh", "·∫•u d√¢m", "k√≠ch d·ª•c", "c·ªù b·∫°c", "c√° ƒë·ªô", "ƒë·∫£ng c·ªông s·∫£n", "ph·∫£n ƒë·ªông", "l·∫≠t ƒë·ªï", "sex"]
    if any(w in q_lower for w in hard_ban): return True
    
    # T·ª´ kh√≥a nghi ng·ªù (C·∫ßn AI check l·∫°i)
    soft_ban = ["gi·∫øt", "ma t√∫y", "v≈© kh√≠", "b·∫°o l·ª±c", "ch·∫øt", "t·ª± t·ª≠", "ch√≠nh tr·ªã", "bi·ªÉu t√¨nh"]
    whitelist = ["lu·∫≠t", "ngh·ªã ƒë·ªãnh", "l·ªãch s·ª≠", "kh√°ng chi·∫øn", "t√°c h·∫°i", "ph√≤ng ch·ªëng", "x√©t x·ª≠", "t√≤a √°n", "b·ªô lu·∫≠t"]
    
    has_bad = any(w in q_lower for w in soft_ban)
    has_good = any(w in q_lower for w in whitelist)
    
    if has_bad and not has_good: return "SUSPICIOUS" # Tr·∫£ v·ªÅ tr·∫°ng th√°i nghi ng·ªù
    return False

async def confirm_safety_with_llm(session, question):
    """D√πng Model Small ƒë·ªÉ check l·∫°i - T·∫ßng 2"""
    # Prompt chuy√™n d·ª•ng ƒë·ªÉ b·∫Øt c√°c c√¢u h·ªèi g√†i b·∫´y
    prompt = [
        {"role": "system", "content": "B·∫°n l√† b·ªô l·ªçc an to√†n. Nhi·ªám v·ª•: X√°c ƒë·ªãnh c√¢u h·ªèi c√≥ vi ph·∫°m ph√°p lu·∫≠t, thu·∫ßn phong m·ªπ t·ª•c Vi·ªát Nam ho·∫∑c nh·∫°y c·∫£m ch√≠nh tr·ªã kh√¥ng. Ch·ªâ tr·∫£ l·ªùi 'UNSAFE' ho·∫∑c 'SAFE'."},
        {"role": "user", "content": f"C√¢u h·ªèi: {question}"}
    ]
    try:
        # G·ªçi model small, max token th·∫•p ƒë·ªÉ ti·∫øt ki·ªám
        res = await call_llm_generic(session, prompt, Config.LLM_MODEL_SMALL, {'used_large':0, 'used_small':0}, max_tokens=10)
        if res and "UNSAFE" in res.upper():
            return True
    except: pass
    return False

# --- THAY TH·∫æ heuristic_answer C≈® ---
def heuristic_answer_overlap(question, options_map):
    """Ch·ªçn ƒë√°p √°n c√≥ nhi·ªÅu t·ª´ chung nh·∫•t v·ªõi c√¢u h·ªèi"""
    try:
        q_tokens = set(word_tokenize(question.lower()))
        best_opt = list(options_map.keys())[0]
        max_score = -1
        
        for key, text in options_map.items():
            opt_tokens = set(word_tokenize(str(text).lower()))
            # ƒê·∫øm s·ªë t·ª´ tr√πng l·∫∑p gi·ªØa c√¢u h·ªèi v√† ƒë√°p √°n
            score = len(q_tokens.intersection(opt_tokens))
            if score > max_score:
                max_score = score
                best_opt = key
        return best_opt
    except:
        return list(options_map.keys())[0] # Fallback cu·ªëi c√πng

# --- THAY TH·∫æ build_prompt C≈® ---
def build_cot_prompt(question, options_text, docs):
    context = ""
    for i, doc in enumerate(docs):
        # C·∫Øt ng·∫Øn b·ªõt m·ªói doc ƒë·ªÉ tr√°nh b·ªã tr√¥i context n·∫øu qu√° d√†i
        clean_text = " ".join(doc['text'].split()[:350]) 
        context += f"[T√†i li·ªáu {i+1}]: {clean_text}...\n\n"

    system_prompt = """B·∫°n l√† chuy√™n gia gi·∫£i quy·∫øt c√¢u h·ªèi tr·∫Øc nghi·ªám.
QUY TR√åNH T∆Ø DUY (B·∫ÆT BU·ªòC):
1. ƒê·ªçc k·ªπ C√ÇU H·ªéI v√† D·ªÆ LI·ªÜU.
2. Ph√¢n t√≠ch t·ª´ng ƒë√°p √°n (A, B, C, D):
   - T√¨m b·∫±ng ch·ª©ng trong D·ªÆ LI·ªÜU ƒë·ªÉ x√°c nh·∫≠n ho·∫∑c b√°c b·ªè.
   - Ch√∫ √Ω: Con s·ªë (ng√†y th√°ng, s·ªë l∆∞·ª£ng) ph·∫£i ch√≠nh x√°c 100%.
   - Ch√∫ √Ω: Logic nh√¢n qu·∫£ (T·∫°i sao, Nguy√™n nh√¢n).
3. Vi·∫øt ph·∫ßn 'PH√ÇN T√çCH' tr∆∞·ªõc, sau ƒë√≥ m·ªõi ch·ªët 'ƒê√ÅP √ÅN'.

ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI:
### PH√ÇN T√çCH:
[L√Ω lu·∫≠n ng·∫Øn g·ªçn c·ªßa b·∫°n]
### ƒê√ÅP √ÅN: [Ch·ªâ ghi 1 k√Ω t·ª± in hoa: A, B, C ho·∫∑c D]"""

    user_prompt = f"""D·ªÆ LI·ªÜU THAM KH·∫¢O:
{context}

C√ÇU H·ªéI: {question}

L·ª∞A CH·ªåN:
{options_text}"""
    
    return [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

def find_refusal_key(options_map):
    keywords = ["kh√¥ng th·ªÉ tr·∫£ l·ªùi", "t·ª´ ch·ªëi", "vi ph·∫°m", "nh·∫°y c·∫£m", "kh√¥ng ph√π h·ª£p"]
    for label, text in options_map.items():
        if any(kw in str(text).lower() for kw in keywords): return label
    return None

def get_dynamic_options(row):
    options = []
    if 'choices' in row and isinstance(row['choices'], list): options = row['choices']
    elif 'options' in row and isinstance(row['options'], list): options = row['options']
    else:
        i = 1
        while True:
            val = row.get(f"option_{i}")
            if not val or str(val).lower() == 'nan': break
            options.append(str(val))
            i += 1
    return {chr(65 + i): str(text) for i, text in enumerate(options)}

def extract_answer_two_step(text, options_map):
    valid_keys = list(options_map.keys())
    fallback = valid_keys[0]
    if not text: return fallback
    text = text.strip()
    
    # ∆Øu ti√™n 1: Format chu·∫©n
    match = re.search(r'###\s*ƒê√ÅP √ÅN[:\s\n]*([A-Z])', text, re.IGNORECASE)
    if match and match.group(1).upper() in valid_keys: return match.group(1).upper()
    
    # ∆Øu ti√™n 2: Markdown
    match = re.search(r'\*\*([A-Z])\*\*', text)
    if match and match.group(1).upper() in valid_keys: return match.group(1).upper()

    # Fallback: T√¨m k√Ω t·ª± cu·ªëi c√πng
    matches = re.findall(r'\b([A-Z])\b', text)
    for m in reversed(matches):
        if m.upper() in valid_keys: return m.upper()
    return fallback

def heuristic_answer(options_map):
    # Ch·ªçn ƒë√°p √°n d√†i nh·∫•t
    return max(options_map.items(), key=lambda x: len(str(x[1])))[0]

def build_prompt(question, options_text, docs):
    context = ""
    for i, doc in enumerate(docs):
        context += f"--- T√ÄI LI·ªÜU #{i+1} ---\n{doc['text']}\n\n"

    system_prompt = """B·∫°n l√† chuy√™n gia t∆∞ v·∫•n v√† gi·∫£i quy·∫øt c√°c c√¢u h·ªèi tr·∫Øc nghi·ªám d·ª±a tr√™n b·∫±ng ch·ª©ng th·ª±c t·∫ø.
QUY TR√åNH SUY LU·∫¨N:
1. ƒê·ªçc k·ªπ c√¢u h·ªèi v√† t·ª´ng l·ª±a ch·ªçn (A, B, C, D).
2. T√¨m ki·∫øm th√¥ng tin ch√≠nh x√°c trong ph·∫ßn D·ªÆ LI·ªÜU kh·ªõp v·ªõi c√°c t·ª´ kh√≥a trong c√¢u h·ªèi.
3. So s√°nh t·ª´ng l·ª±a ch·ªçn v·ªõi D·ªÆ LI·ªÜU:
   - N·∫øu d·ªØ li·ªáu ·ªßng h·ªô l·ª±a ch·ªçn n√†o, h√£y tr√≠ch d·∫´n ng·∫Øn g·ªçn √Ω ƒë√≥.
   - Ch√∫ √Ω c√°c b·∫´y v·ªÅ th·ªùi gian, ƒë·ªãa ƒëi·ªÉm, con s·ªë (v√≠ d·ª•: 1 b·∫£n vs 2 b·∫£n).
   - V·ªõi c√¢u h·ªèi "nguy√™n nh√¢n/ngu·ªìn g·ªëc", h√£y t√¨m c√¢u vƒÉn ch·ª©a quan h·ªá nh√¢n qu·∫£ (v√¨, do, t·ª´ ƒë√≥...).
4. ƒê∆∞a ra k·∫øt lu·∫≠n cu·ªëi c√πng.

L∆ØU √ù ƒê·∫∂C BI·ªÜT:
- N·∫øu c√¢u h·ªèi d·∫°ng "T·∫•t c·∫£ c√°c √Ω tr√™n" ho·∫∑c "C·∫£ A, B, C", h√£y ki·ªÉm tra xem c√°c √Ω l·∫ª c√≥ ƒë√∫ng kh√¥ng. N·∫øu 2 √Ω ƒë√∫ng tr·ªü l√™n -> Ch·ªçn ƒë√°p √°n t·ªïng h·ª£p.
- ∆Øu ti√™n th√¥ng tin trong D·ªÆ LI·ªÜU h∆°n ki·∫øn th·ª©c b√™n ngo√†i.
"""

    user_prompt = f"""D·ªÆ LI·ªÜU THAM KH·∫¢O:
{context}

C√ÇU H·ªéI: {question}

C√ÅC L·ª∞A CH·ªåN:
{options_text}

H√ÉY TR·∫¢ L·ªúI THEO ƒê√öNG ƒê·ªäNH D·∫†NG SAU:
### SUY LU·∫¨N:
[Ph√¢n t√≠ch chi ti·∫øt c·ªßa b·∫°n t·∫°i ƒë√¢y, ch·ªâ ra b·∫±ng ch·ª©ng trong vƒÉn b·∫£n]
### ƒê√ÅP √ÅN:
[Ch·ªâ vi·∫øt 1 k√Ω t·ª± in hoa ƒë·∫°i di·ªán ƒë√°p √°n ƒë√∫ng: A, B, C ho·∫∑c D]"""
    
    return [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

# ==============================================================================
# 2. RETRIEVER & API CLIENTS
# ==============================================================================
class HybridRetriever:
    def __init__(self, qdrant_client):
        self.client = qdrant_client
        self.bm25 = None
        if BM25_FILE.exists():
            try:
                with open(BM25_FILE, "rb") as f: self.bm25 = pickle.load(f)
                logger.info(f"BM25 loaded: {len(self.bm25.get('chunk_ids', []))} chunks")
            except: pass

    async def search(self, session, query, top_k=TOP_K):
        # 1. Embed
        query_vec = await get_embedding_async(session, query)
        vec_hits = []
        if query_vec:
            for _ in range(3):
                try:
                    res = await self.client.query_points(Config.COLLECTION_NAME, query=query_vec, limit=top_k, with_payload=True)
                    vec_hits = res.points
                    break
                except: await asyncio.sleep(1)

        # 2. BM25
        bm25_hits = []
        if self.bm25:
            try:
                tokens = word_tokenize(query.lower())
                scores = self.bm25['bm25_obj'].get_scores(tokens)
                top_idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k*2]
                thresh = max(1.0, scores[top_idxs[int(len(top_idxs)*0.3)]]) if top_idxs else 1.0
                for idx in top_idxs:
                    if scores[idx] >= thresh:
                        bm25_hits.append({"id": self.bm25['chunk_ids'][idx], "score": scores[idx], "text": self.bm25['texts'][idx], "title": self.bm25['titles'][idx]})
            except: pass

        # 3. Fusion
        fused = {}
        max_v = max([h.score for h in vec_hits]) if vec_hits else 1.0
        for h in vec_hits:
            fused[h.payload['chunk_id']] = {"text": h.payload['text'], "title": h.payload['title'], "score": (h.score/max_v)*ALPHA_VECTOR}
        
        max_b = max([h['score'] for h in bm25_hits]) if bm25_hits else 1.0
        for h in bm25_hits:
            norm = (h['score']/max_b)*(1-ALPHA_VECTOR)
            cid = h['id']
            if cid in fused: fused[cid]['score'] += norm
            else: fused[cid] = {"text": h['text'], "title": h['title'], "score": norm}
        
        return sorted(fused.values(), key=lambda x: x['score'], reverse=True)[:top_k]

async def get_embedding_async(session, text):
    await LIMITER_EMBED.acquire()
    creds = Config.VNPT_CREDENTIALS.get(Config.MODEL_EMBEDDING_API)
    headers = {'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}', 'Token-id': creds['token_id'], 'Token-key': creds['token_key'], 'Content-Type': 'application/json'}
    payload = {"model": Config.MODEL_EMBEDDING_API, "input": text, "encoding_format": "float"}
    for i in range(2):
        try:
            async with session.post(Config.VNPT_EMBEDDING_URL, json=payload, headers=headers, timeout=30) as r:
                if r.status == 200:
                    d = await r.json()
                    if 'data' in d: return d['data'][0]['embedding']
                elif r.status in [429, 500]: await asyncio.sleep(2)
        except: await asyncio.sleep(1)
    return None

async def call_llm_generic(session, messages, model_name, stats, max_tokens=1024):
    limiter = LIMITER_LARGE if "large" in model_name.lower() else LIMITER_SMALL
    await limiter.acquire()
    
    # Ghi nh·∫≠n d√πng quota
    if "large" in model_name.lower(): stats['used_large'] += 1
    else: stats['used_small'] += 1

    try:
        creds = Config.VNPT_CREDENTIALS.get(model_name)
        url = f"{Config.VNPT_API_URL}/{model_name.replace('_', '-')}"
        headers = {'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}', 'Token-id': creds['token_id'], 'Token-key': creds['token_key'], 'Content-Type': 'application/json'}
        payload = {"model": model_name, "messages": messages, "temperature": 0.1, "top_p": 0.95, "max_completion_tokens": max_tokens}
        
        # Jitter ƒë·ªÉ tr√°nh d·ªìn toa
        await asyncio.sleep(random.uniform(0.5, 1.5))

        async with session.post(url, json=payload, headers=headers, timeout=90) as resp:
            if resp.status == 200:
                d = await resp.json()
                if 'choices' in d: return d['choices'][0]['message']['content']
            elif resp.status >= 400:
                logger.warning(f"‚ö†Ô∏è API {model_name} Error {resp.status}")
    except Exception as e:
        logger.warning(f"üîå Net Error {model_name}: {str(e)[:30]}")
    return None

# ==============================================================================
# 3. CORE LOGIC (PROCESS SINGLE ROW)
# ==============================================================================
# async def process_row_logic(session, retriever, row, stats):
#     """X·ª≠ l√Ω 1 d√≤ng, tr·∫£ v·ªÅ k·∫øt qu·∫£"""
#     qid = row.get('qid', row.get('id', 'unknown'))
#     question = row.get('question', '')
#     true_label = row.get('answer', None)
    
#     opts = get_dynamic_options(row)
#     opt_text = "\n".join([f"{k}. {v}" for k, v in opts.items()])

#     # 1. Safety
#     if is_sensitive_topic(question):
#         ans = find_refusal_key(opts) or "A"
#         logger.info(f"üö´ Q:{qid} Sensitive")
#         return {"qid": qid, "answer": ans, "is_correct": ans == true_label if true_label else None}

#     # 2. Retrieval
#     docs = await retriever.search(session, question, top_k=TOP_K)
#     msgs = build_prompt(question, opt_text, docs)
#     ctx_len = sum([len(d['text']) for d in docs])

#     # 3. Model
#     model = Config.LLM_MODEL_LARGE
#     if ctx_len < THRESHOLD_SMALL_CONTEXT: model = Config.LLM_MODEL_SMALL

#     # 4. Infer
#     raw = await call_llm_generic(session, msgs, model, stats)
    
#     if not raw:
#         # Fallback
#         fallback_model = Config.LLM_MODEL_SMALL if model == Config.LLM_MODEL_LARGE else Config.LLM_MODEL_LARGE
#         logger.warning(f"‚ö†Ô∏è Q:{qid} Fallback -> {fallback_model}")
#         raw = await call_llm_generic(session, msgs, fallback_model, stats)

#     # 5. Extract
#     if not raw:
#         ans = heuristic_answer(opts)
#         logger.error(f"0 Q:{qid} Failed all models -> Heuristic")
#     else:
#         ans = extract_answer_two_step(raw, opts)

#     is_correct = (ans == true_label) if true_label else None
#     status = "1" if is_correct else ("0" if is_correct is False else "")
#     logger.info(f"Q:{qid} | Ans:{ans} {status}")
    
#     return {"qid": qid, "answer": ans, "is_correct": is_correct}
async def process_row_logic(session, retriever, row, stats):
    qid = row.get('qid', row.get('id', 'unknown'))
    question = row.get('question', '')
    true_label = row.get('answer', None)
    opts = get_dynamic_options(row)
    opt_text = "\n".join([f"{k}. {v}" for k, v in opts.items()])

    # --- B∆Ø·ªöC 1: KI·ªÇM TRA NH·∫†Y C·∫¢M N√ÇNG CAO ---
    is_unsafe = check_keywords_sensitive(question)
    if is_unsafe == "SUSPICIOUS":
        # N·∫øu nghi ng·ªù, h·ªèi LLM x√°c nh·∫≠n
        is_unsafe = await confirm_safety_with_llm(session, question)
            
    if is_unsafe is True:
        ans = find_refusal_key(opts) or "A"
        logger.info(f"üö´ Q:{qid} Blocked by Safety Filter")
        return {"qid": qid, "answer": ans}

    # --- B∆Ø·ªöC 2: T√åM KI·∫æM T√ÄI LI·ªÜU ---
    docs = await retriever.search(session, question, top_k=15) # Gi·∫£m TopK xu·ªëng 15 ƒë·ªÉ b·ªõt nhi·ªÖu
    
    # --- B∆Ø·ªöC 3: X√ÇY D·ª∞NG PROMPT M·ªöI ---
    msgs = build_cot_prompt(question, opt_text, docs) # D√πng h√†m Prompt m·ªõi
    ctx_len = sum([len(d['text']) for d in docs])

    # --- B∆Ø·ªöC 4: CH·ªåN MODEL ---
    model = Config.LLM_MODEL_LARGE
    if ctx_len < 12000: model = Config.LLM_MODEL_SMALL # H·∫° ng∆∞·ª°ng xu·ªëng 12k

    # --- B∆Ø·ªöC 5: G·ªåI API ---
    # Gi·∫£m temperature xu·ªëng 0.1 ƒë·ªÉ model tr·∫£ l·ªùi ki√™n ƒë·ªãnh h∆°n
    raw = await call_llm_generic(session, msgs, model, stats)
    
    if not raw:
        # Retry model kh√°c n·∫øu l·ªói
        fallback = Config.LLM_MODEL_SMALL if model == Config.LLM_MODEL_LARGE else Config.LLM_MODEL_LARGE
        raw = await call_llm_generic(session, msgs, fallback, stats)

    # --- B∆Ø·ªöC 6: TR√çCH XU·∫§T ƒê√ÅP √ÅN ---
    # D√πng h√†m extract c≈© c·ªßa b·∫°n ho·∫∑c logic regex m·ªõi
    match = re.search(r'###\s*ƒê√ÅP √ÅN[:\s\n]*([A-Z])', str(raw), re.IGNORECASE)
    if match:
        ans = match.group(1).upper()
    else:
        # N·∫øu kh√¥ng t√¨m th·∫•y ƒë√°p √°n trong text -> D√πng Heuristic m·ªõi
        ans = heuristic_answer_overlap(question, opts)
        logger.warning(f"Q:{qid} Fallback Heuristic -> {ans}")

    # Log k·∫øt qu·∫£
    logger.info(f"Q:{qid} | Ans:{ans}")
    return {"qid": qid, "answer": ans}

# ==============================================================================
# 4. MAIN LOOP WITH RESUME
# ==============================================================================
async def main():
    # 1. Load Data
    # files = [Config.BASE_DIR / "data" / "val.json", Config.BASE_DIR / "data" / "test.json"]
    files = [Config.BASE_DIR / "data" / "test.json"]
    input_file = next((f for f in files if f.exists()), None)
    if not input_file: return
    with open(input_file, 'r', encoding='utf-8') as f: data = json.load(f)

    # 2. Check Resume (ƒê·ªçc file ƒë√£ l∆∞u)
    processed_ids = set()
    if OUTPUT_FILE.exists():
        try:
            df_done = pd.read_csv(OUTPUT_FILE)
            processed_ids = set(df_done['qid'].astype(str))
            logger.info(f"RESUMING... Found {len(processed_ids)} processed questions.")
        except: pass
    
    # L·ªçc c√¢u ch∆∞a l√†m
    data_to_process = [r for r in data if str(r.get('qid', r.get('id'))) not in processed_ids]
    
    if not data_to_process:
        logger.info("ALL DONE! Nothing to process.")
        return

    logger.info(f"REMAINING: {len(data_to_process)}/{len(data)} questions")

    # 3. Setup
    qdrant_client = AsyncQdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY, timeout=30)
    retriever = HybridRetriever(qdrant_client)
    stats = {'used_large': 0, 'used_small': 0}
    
    # 4. Run Sequential (V√≤ng l·∫∑p ƒë∆°n lu·ªìng)
    conn = aiohttp.TCPConnector(limit=1, force_close=True, enable_cleanup_closed=True)
    async with aiohttp.ClientSession(connector=conn) as session:
        
        for i, row in enumerate(data_to_process):
            qid = row.get('qid', row.get('id'))
            
            # Retry loop cho t·ª´ng c√¢u
            for attempt in range(3):
                try:
                    # Timeout c·ª©ng
                    result = await asyncio.wait_for(
                        process_row_logic(session, retriever, row, stats),
                        timeout=TIMEOUT_PER_QUESTION
                    )
                    
                    # --- WRITE TO DISK IMMEDIATELY ---
                    df_res = pd.DataFrame([result])
                    # N·∫øu file ch∆∞a c√≥ th√¨ ghi header, c√≥ r·ªìi th√¨ append kh√¥ng header
                    need_header = not OUTPUT_FILE.exists()
                    df_res[['qid', 'answer']].to_csv(OUTPUT_FILE, mode='a', header=need_header, index=False)
                    
                    break # Success -> Next question
                    
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout Q:{qid} (Attempt {attempt+1})")
                    if attempt == 2:
                        # Fail h·∫≥n -> Ghi 'A' ƒë·ªÉ l·∫ßn sau kh√¥ng b·ªã k·∫πt
                        pd.DataFrame([{"qid": qid, "answer": "A"}]).to_csv(OUTPUT_FILE, mode='a', header=not OUTPUT_FILE.exists(), index=False)
                except Exception as e:
                    logger.error(f"Error Q:{qid}: {e}")
                    await asyncio.sleep(5)

            # Ngh·ªâ ng∆°i gi·ªØa c√°c c√¢u ƒë·ªÉ server th·ªü
            await asyncio.sleep(1)

    await qdrant_client.close()
    logger.info("BATCH COMPLETED!")

    if OUTPUT_FILE.exists():
        print("\n" + "="*40)
        print("T·ªîNG K·∫æT TO√ÄN B·ªò (CUMULATIVE STATS)")
        print("="*40)
        
        try:
            # 1. ƒê·ªçc to√†n b·ªô k·∫øt qu·∫£ ƒë√£ l∆∞u trong CSV
            df_results = pd.read_csv(OUTPUT_FILE)
            
            # 2. T·∫°o t·ª´ ƒëi·ªÉn ƒë√°p √°n ƒë√∫ng (Ground Truth) t·ª´ file input g·ªëc
            # L∆∞u √Ω: Ch·ªâ l·∫•y nh·ªØng c√¢u c√≥ tr∆∞·ªùng 'answer' (ƒë·ªÅ ph√≤ng file Test kh√¥ng c√≥)
            ground_truth = {
                str(r.get('qid', r.get('id'))): str(r.get('answer')).strip() 
                for r in data if r.get('answer')
            }
            
            if not ground_truth:
                print("ƒê√¢y l√† t·∫≠p Test (kh√¥ng c√≥ ƒë√°p √°n) -> B·ªè qua t√≠nh ƒëi·ªÉm.")
            else:
                correct_count = 0
                total_checked = 0
                
                # 3. So kh·ªõp t·ª´ng c√¢u trong CSV v·ªõi ƒë√°p √°n g·ªëc
                for _, row in df_results.iterrows():
                    qid = str(row['qid'])
                    # Chuy·ªÉn v·ªÅ string v√† strip ƒë·ªÉ so s√°nh ch√≠nh x√°c
                    pred = str(row['answer']).strip()
                    
                    if qid in ground_truth:
                        total_checked += 1
                        true_label = ground_truth[qid]
                        
                        # So s√°nh
                        if pred == true_label:
                            correct_count += 1
                
                # 4. In k·∫øt qu·∫£
                if total_checked > 0:
                    acc = (correct_count / total_checked) * 100
                    print(f"ƒê√£ l√†m: {total_checked}/{len(ground_truth)} c√¢u")
                    print(f"ƒê√∫ng  : {correct_count} c√¢u")
                    print(f"T·ª∑ l·ªá : {acc:.2f}%")
                else:
                    print("‚ö†Ô∏è Ch∆∞a c√≥ c√¢u n√†o kh·ªõp ID v·ªõi t·∫≠p d·ªØ li·ªáu g·ªëc.")
                    
        except Exception as e:
            print(f"L·ªói t√≠nh ƒëi·ªÉm: {e}")

        print(f"File k·∫øt qu·∫£: {OUTPUT_FILE}")

if __name__ == "__main__":
    if sys.platform == 'win32': asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())