#build_bm25.py:

import bm25s
import pandas as pd
import pickle
import string
import os
import sys
import json
import gc
import pyarrow.parquet as pq
from underthesea import word_tokenize
from tqdm import tqdm
from config import Config
from multiprocessing import Pool, cpu_count
from pathlib import Path
from datetime import datetime

# --- CONFIG ---
INDEX_VERSION = "4.1"  # Major.Minor (Minor change: Optimization)
TOKENIZER_NAME = "underthesea_word_tokenize"

# --- 1. PREPROCESSOR (WORKER FUNCTION) ---
# H√†m n√†y ph·∫£i ƒë·∫∑t ·ªü top-level ƒë·ªÉ Multiprocessing pickle ƒë∆∞·ª£c
translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))

def preprocess_text_worker(text):
    """
    H√†m x·ª≠ l√Ω ƒë∆°n l·∫ª cho t·ª´ng text.
    T·ªëi ∆∞u h√≥a string manipulation tr∆∞·ªõc khi g·ªçi tokenizer n·∫∑ng.
    """
    if not isinstance(text, str) or not text:
        return []
    
    # 1. Lowercase & Remove Punctuation (Nhanh, Python C-optimized)
    # D√πng translate nhanh h∆°n replace/regex nhi·ªÅu l·∫ßn
    clean_text = text.lower().translate(translator)
    
    # 2. Tokenize (Ch·∫≠m, CPU bound)
    # Ch·ªâ g·ªçi underthesea khi text ƒë√£ s·∫°ch
    tokens = word_tokenize(clean_text)
    
    # 3. Filter empty (Nhanh)
    return [t for t in tokens if len(t.strip()) > 0]

# --- 2. BUILDER CLASS ---
class BM25Builder:
    def __init__(self):
        self.output_dir = Config.BASE_DIR / "bm25s_index"
        self.id_map_file = Config.BASE_DIR / "bm25s_ids.pkl"
        self.metadata_file = Config.BASE_DIR / "bm25_metadata.json"
        
        self.all_ids = []
        self.all_tokens = [] # List[List[str]] - Nh·∫π h∆°n raw text nhi·ªÅu
        
        Config.BASE_DIR.mkdir(parents=True, exist_ok=True)

    def process_files_streaming(self):
        """
        ƒê·ªçc v√† x·ª≠ l√Ω t·ª´ng file m·ªôt (Streaming) ƒë·ªÉ ti·∫øt ki·ªám RAM.
        """
        files = list((Config.BASE_DIR / "output_batch_chunking").glob("*.parquet"))
        files.sort()
        
        print(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(files)} file theo c∆° ch·∫ø Streaming...")
        
        # T·∫≠n d·ª•ng t·ªëi ƒëa CPU (tr·ª´ 1 core cho OS)
        num_cores = max(1, cpu_count() - 1)
        
        # T·∫°o Pool m·ªôt l·∫ßn d√πng cho to√†n b·ªô qu√° tr√¨nh
        with Pool(processes=num_cores) as pool:
            
            for file_path in tqdm(files, desc="Processing Files"):
                # 1. ƒê·ªçc 1 file v√†o RAM (Pyarrow nhanh h∆°n Pandas)
                try:
                    table = pq.read_table(file_path, columns=['chunk_id', 'vector_text'])
                    
                    # Convert sang list python (nhanh h∆°n x·ª≠ l√Ω vector pandas cho text)
                    batch_ids = table['chunk_id'].to_pylist()
                    batch_texts = table['vector_text'].to_pylist()
                    
                    # Gi·∫£i ph√≥ng table pyarrow ngay
                    del table
                except Exception as e:
                    print(f"‚ùå L·ªói ƒë·ªçc file {file_path.name}: {e}")
                    continue

                # 2. Tokenize song song cho batch n√†y
                # chunksize l·ªõn ƒë·ªÉ gi·∫£m overhead IPC (Inter-Process Communication)
                batch_tokens = pool.map(preprocess_text_worker, batch_texts, chunksize=2000)
                
                # 3. L∆∞u k·∫øt qu·∫£ v√†o list t·ªïng
                self.all_ids.extend([str(i) for i in batch_ids])
                self.all_tokens.extend(batch_tokens)
                
                # 4. D·ªåN D·∫∏P RAM NGAY L·∫¨P T·ª®C
                del batch_ids
                del batch_texts
                del batch_tokens
                gc.collect() # √âp Python tr·∫£ RAM cho OS
        
        print(f"‚úÖ Tokenization ho√†n t·∫•t. T·ªïng chunks: {len(self.all_ids)}")

    def build_and_save(self):
        if not self.all_tokens:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ build index.")
            return

        print(f"\nüèóÔ∏è ƒêang Build Index BM25s (Method: Luceta)...")
        # Kh·ªüi t·∫°o kh√¥ng truy·ªÅn corpus ƒë·ªÉ ti·∫øt ki·ªám RAM l√∫c init
        retriever = bm25s.BM25(method='lucene')
        
        # Indexing
        retriever.index(self.all_tokens)
        
        # Gi·∫£i ph√≥ng list tokens kh·ªïng l·ªì ngay sau khi index xong
        print("üßπ Gi·∫£i ph√≥ng RAM token list...")
        del self.all_tokens
        gc.collect()

        print(f"\nüíæ ƒêang l∆∞u xu·ªëng ƒëƒ©a...")
        
        # 1. L∆∞u Index
        retriever.save(self.output_dir)
        
        # 2. L∆∞u ID Map
        with open(self.id_map_file, "wb") as f:
            pickle.dump(self.all_ids, f, protocol=pickle.HIGHEST_PROTOCOL)
            
        # 3. L∆∞u Metadata (Version Control)
        metadata = {
            "version": INDEX_VERSION,
            "created_at": datetime.now().isoformat(),
            "num_chunks": len(self.all_ids),
            "tokenizer": TOKENIZER_NAME,
            "library": "bm25s"
        }
        with open(self.metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2)
            
        print("üéâ HO√ÄN T·∫§T TO√ÄN B·ªò QU√Å TR√åNH!")
        
        # T√≠nh k√≠ch th∆∞·ªõc th∆∞ m·ª•c index
        total_size = sum(f.stat().st_size for f in self.output_dir.glob('**/*') if f.is_file())
        print(f"üì¶ Index Size: {total_size / (1024*1024):.2f} MB")
        print(f"üîñ Metadata: {self.metadata_file}")

if __name__ == "__main__":
    if sys.platform == 'win32':
        import multiprocessing
        multiprocessing.freeze_support()
        
    builder = BM25Builder()
    builder.process_files_streaming()
    builder.build_and_save()