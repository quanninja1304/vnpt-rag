import pandas as pd
from langchain_text_splitters import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer
from tqdm import tqdm
import re
import json
import os
from config import Config

# --- 1. SETUP TOKENIZER ---
print("‚è≥ Loading Tokenizer...")
try:
    tokenizer = AutoTokenizer.from_pretrained(Config.TOKENIZER_NAME)
    def count_tokens(text):
        return len(tokenizer.encode(text))
    print("‚úÖ Tokenizer loaded successfully.")
except Exception as e:
    print(f"‚ö†Ô∏è Warning: Tokenizer error ({e}). Using Vietnamese-optimized fallback.")
    def count_tokens(text):
        # Fallback: ƒê·∫øm t·ª´ (split space) * 1.3
        if not text: return 0
        return int(len(text.split()) * 1.3)

# --- 2. CLEAN TEXT ---
def clean_wiki_text(text):
    if not text: return ""
    
    # C·∫Øt footer
    stop_phrases = ['tham kh·∫£o', 'li√™n k·∫øt ngo√†i', 'ch√∫ th√≠ch', 'ƒë·ªçc th√™m']
    lines = text.split('\n')
    cut_index = len(lines)
    for i, line in enumerate(lines):
        line_clean = line.strip().lower()
        if len(line_clean) < 40 and any(p == line_clean.strip('.:-=') for p in stop_phrases):
            cut_index = i
            break
    text = '\n'.join(lines[:cut_index])
    
    # Clean artifacts & format
    text = re.sub(r'\[\d+\]', '', text) # Remove citation [1]
    text = re.sub(r'(?<!\n)\n(?!\n)', '. ', text) # Fix broken lines
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# --- 3. MAIN CHUNKING ---
def process_chunking():
    # Load Data
    if not Config.CRAWL_OUTPUT_PARQUET.exists():
        print(f"‚ùå Missing file: {Config.CRAWL_OUTPUT_PARQUET}")
        return
    
    df = pd.read_parquet(Config.CRAWL_OUTPUT_PARQUET)
    
    # Load State (Incremental check)
    processed_titles = set()
    if Config.CHUNKING_STATE_FILE.exists():
        with open(Config.CHUNKING_STATE_FILE, 'r', encoding='utf-8') as f:
            processed_titles = set(json.load(f))
            
    df_new = df[~df['title'].isin(processed_titles)]
    print(f"üì¶ Total Articles: {len(df)} | üîÑ New to Process: {len(df_new)}")
    
    if len(df_new) == 0:
        print("‚úÖ No new articles.")
        return

    # Splitter config
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=Config.CHUNK_SIZE_TOKENS,
        chunk_overlap=Config.CHUNK_OVERLAP_TOKENS,
        length_function=count_tokens,
        separators=["\n\n", "\n", ". ", ".", ";", " ", ""]
    )

    new_chunks = []
    
    for idx, row in tqdm(df_new.iterrows(), total=len(df_new)):
        text = clean_wiki_text(row.get('text', ''))
        title = row.get('title', '')
        url = row.get('url', '')
        
        # L·∫•y category ƒë·∫ßu ti√™n l√†m metadata
        cats = row.get('categories', [])
        cat_str = cats[0] if isinstance(cats, list) and cats else "T·ªïng h·ª£p"
        cat_str = cat_str.replace('_', ' ')

        # Filter b√†i qu√° ng·∫Øn
        if len(text) < 50: 
            processed_titles.add(title)
            continue

        chunks = splitter.create_documents([text])
        
        for i, chunk in enumerate(chunks):
            content = chunk.page_content.strip()
            
            # --- FILTERS (Relaxed) ---
            # Gi·ªØ l·∫°i c√°c chunk "Ni√™n bi·ªÉu", "S·ª± ki·ªán" k·ªÉ c·∫£ khi ng·∫Øn
            is_timeline = any(kw in content for kw in ["Ni√™n bi·ªÉu", "S·ª± ki·ªán", "nƒÉm"])
            has_number = any(char.isdigit() for char in content)
            
            if len(content) < 30 and not (is_timeline and has_number):
                continue
                
            if "M·ª•c l·ª•c" in content and len(content) < 50:
                continue

            # --- CONTEXT INJECTION ---
            # Th√™m Title v√† Category v√†o ƒë·∫ßu ƒëo·∫°n vƒÉn ƒë·ªÉ model hi·ªÉu ng·ªØ c·∫£nh
            vector_text = f"Lƒ©nh v·ª±c: {cat_str}. Ch·ªß ƒë·ªÅ: {title}.\nN·ªôi dung: {content}"
            
            new_chunks.append({
                "chunk_id": f"{title}_{i}",
                "doc_title": title,
                "doc_category": cat_str,
                "vector_text": vector_text, # D√πng ƒë·ªÉ Embed
                "display_text": content,    # D√πng ƒë·ªÉ hi·ªÉn th·ªã
                "doc_url": url
            })
        
        processed_titles.add(title)

    # Save
    if new_chunks:
        df_delta = pd.DataFrame(new_chunks)
        df_delta.to_parquet(Config.LATEST_CHUNKS_FILE, index=False)
        print(f"üíæ Saved {len(df_delta)} chunks to {Config.LATEST_CHUNKS_FILE}")
        
        # Update State
        with open(Config.CHUNKING_STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(list(processed_titles), f, ensure_ascii=False)
    else:
        print("‚ö†Ô∏è Processed articles but generated no chunks.")

if __name__ == "__main__":
    process_chunking()