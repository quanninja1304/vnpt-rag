import pandas as pd
import pickle
import string
import os
import shutil
import tempfile
import pyarrow.parquet as pq
from rank_bm25 import BM25Okapi
from underthesea import word_tokenize
from tqdm import tqdm
from config import Config
from multiprocessing import Pool, cpu_count

# --- VERSION CONTROL ---
# TÄƒng sá»‘ nÃ y lÃªn náº¿u báº¡n thay Ä‘á»•i cáº¥u trÃºc dá»¯ liá»‡u lÆ°u trong pickle
INDEX_VERSION = 2 

# --- HÃ€M Xá»¬ LÃ TEXT ---
def preprocess_text(text):
    if not text: return []
    # Chuyá»ƒn vá» string Ä‘á»ƒ trÃ¡nh lá»—i náº¿u dá»¯ liá»‡u lÃ  sá»‘/None
    text = str(text).lower()
    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))
    tokens = word_tokenize(text)
    return [t for t in tokens if len(t.strip()) > 0]

def save_atomic(data, filepath):
    """Ghi file an toÃ n: Ghi vÃ o temp -> Move Ä‘Ã¨ lÃªn file cÅ©"""
    dirname = os.path.dirname(filepath)
    os.makedirs(dirname, exist_ok=True)
    
    # Táº¡o file táº¡m
    tmp_f = tempfile.NamedTemporaryFile(delete=False, dir=dirname, suffix=".tmp")
    try:
        pickle.dump(data, tmp_f)
        tmp_f.close() # ÄÃ³ng file Ä‘á»ƒ flush buffer
        # Move atomic (Ghi Ä‘Ã¨ an toÃ n)
        shutil.move(tmp_f.name, filepath)
        print(f"âœ… Saved atomically to: {filepath}")
    except Exception as e:
        os.unlink(tmp_f.name) # XÃ³a file táº¡m náº¿u lá»—i
        raise e

def build_bm25_incremental():
    output_path = Config.OUTPUT_DIR / "bm25_index.pkl"
    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # 1. State Variables
    master_corpus = []
    master_ids = []
    master_texts = []
    master_titles = []
    existing_ids = set() # Set Ä‘á»ƒ lookup O(1)

    # 2. Load Old Index (Backward Compatibility check)
    if output_path.exists():
        print(f"ğŸ”„ Kiá»ƒm tra file index cÅ©: {output_path}")
        try:
            with open(output_path, "rb") as f:
                data = pickle.load(f)
            
            # Check Version
            if data.get("version", 0) < INDEX_VERSION:
                print(f"âš ï¸ Index cÅ© (v{data.get('version')}) khÃ´ng tÆ°Æ¡ng thÃ­ch v{INDEX_VERSION}. Rebuild toÃ n bá»™.")
            elif "tokenized_corpus" in data:
                print("âœ… Load thÃ nh cÃ´ng dá»¯ liá»‡u cÅ©.")
                master_corpus = data["tokenized_corpus"]
                master_ids = data["chunk_ids"]
                master_texts = data["texts"]
                master_titles = data["titles"]
                existing_ids = set(map(str, master_ids)) # Äáº£m báº£o ID lÃ  string Ä‘á»ƒ so sÃ¡nh chuáº©n
                print(f"ğŸ“Š Dá»¯ liá»‡u hiá»‡n cÃ³: {len(master_ids)} chunks.")
            else:
                print("âš ï¸ File cÅ© thiáº¿u dá»¯ liá»‡u corpus. Rebuild toÃ n bá»™.")
        except Exception as e:
            print(f"âš ï¸ Lá»—i Ä‘á»c file cÅ© ({e}). Sáº½ build má»›i.")

    # 3. Files to Process
    files_to_index = [
        Config.LATEST_CHUNKS_FILE,
        Config.BASE_DIR / "data" / "law_chunks_ready.parquet"
    ]

    new_documents = [] # Buffer chá»©a dá»¯ liá»‡u má»›i cáº§n tokenize

    print("\nğŸ” Äang quÃ©t dá»¯ liá»‡u má»›i (Batch Processing)...")
    
    for file_path in files_to_index:
        if not file_path.exists():
            continue
            
        print(f"   ğŸ“‚ Äang quÃ©t: {file_path.name}")
        
        try:
            # [MEMORY SAFETY] Äá»c file Parquet theo tá»«ng batch (trÃ¡nh trÃ n RAM vá»›i file lá»›n)
            parquet_file = pq.ParquetFile(file_path)
            
            # Kiá»ƒm tra Schema
            required_cols = {'chunk_id', 'vector_text'}
            file_schema = set(parquet_file.schema.names)
            if not required_cols.issubset(file_schema):
                print(f"   âš ï¸ Bá» qua {file_path.name}: Thiáº¿u cá»™t {required_cols - file_schema}")
                continue

            # Batch size 10k dÃ²ng Ä‘á»ƒ cÃ¢n báº±ng tá»‘c Ä‘á»™/RAM
            for batch in parquet_file.iter_batches(batch_size=10000, columns=['chunk_id', 'vector_text', 'display_text', 'doc_title']):
                df_batch = batch.to_pandas()
                
                # [PERFORMANCE] DÃ¹ng itertuples nhanh gáº¥p nhiá»u láº§n iterrows
                for row in df_batch.itertuples(index=False):
                    cid = str(row.chunk_id)
                    
                    # [CORRECT LOGIC] Check duplicate TRÆ¯á»šC khi xá»­ lÃ½
                    if cid in existing_ids:
                        continue
                    
                    # Add ngay vÃ o set Ä‘á»ƒ cháº·n cÃ¡c dÃ²ng trÃ¹ng láº·p tiáº¿p theo ngay trong vÃ²ng láº·p nÃ y
                    existing_ids.add(cid)
                    
                    new_documents.append({
                        "text_to_process": row.vector_text,
                        "chunk_id": cid,
                        # Fallback an toÃ n náº¿u display_text null
                        "display_text": getattr(row, 'display_text', row.vector_text),
                        "doc_title": getattr(row, 'doc_title', '')
                    })
                    
        except Exception as e:
            print(f"   âŒ Lá»—i Ä‘á»c file {file_path.name}: {e}")

    # 4. Check if update needed
    count_new = len(new_documents)
    print(f"   => Tá»•ng cá»™ng tÃ¬m tháº¥y {count_new} chunks Má»šI.")
    
    if count_new == 0:
        print("\nğŸ‰ Index Ä‘Ã£ cáº­p nháº­t nháº¥t. KhÃ´ng cáº§n lÃ m gÃ¬ thÃªm.")
        return

    # 5. Tokenize (Parallel)
    print(f"\nâš¡ Äang tÃ¡ch tá»« (Multiprocessing) cho {count_new} chunks...")
    texts_to_process = [d['text_to_process'] for d in new_documents]
    
    num_processes = max(1, cpu_count() - 1)
    with Pool(processes=num_processes) as pool:
        new_tokenized = list(tqdm(
            pool.imap(preprocess_text, texts_to_process, chunksize=100),
            total=count_new,
            desc="Tokenizing"
        ))

    # 6. Merge Data
    print("ğŸ“¥ Äang gá»™p dá»¯ liá»‡u...")
    master_corpus.extend(new_tokenized)
    master_ids.extend([d['chunk_id'] for d in new_documents])
    master_texts.extend([d['display_text'] for d in new_documents])
    master_titles.extend([d['doc_title'] for d in new_documents])

    # 7. Re-calculate BM25 (Fast)
    print(f"ğŸ—ï¸ Äang tÃ­nh toÃ¡n láº¡i trá»ng sá»‘ BM25 cho {len(master_corpus)} chunks...")
    bm25 = BM25Okapi(master_corpus)

    # 8. Save (Atomic)
    output_data = {
        "version": INDEX_VERSION, # ÄÃ¡nh dáº¥u version
        "bm25_obj": bm25,
        "tokenized_corpus": master_corpus,
        "chunk_ids": master_ids,
        "texts": master_texts,
        "titles": master_titles
    }

    print(f"ğŸ’¾ Äang lÆ°u file (Atomic)...")
    save_atomic(output_data, output_path)

    # Stats
    size_mb = os.path.getsize(output_path) / (1024 * 1024)
    print(f"âœ… HOÃ€N Táº¤T! Tá»•ng DB: {len(master_ids)} chunks. Size: {size_mb:.2f} MB")

if __name__ == "__main__":
    build_bm25_incremental()