import asyncio
import aiohttp
import pandas as pd
import json
import re
import pickle
import sys
import os
import time
import logging
import random
from pathlib import Path
from aiolimiter import AsyncLimiter
from qdrant_client import AsyncQdrantClient
from underthesea import word_tokenize
from config import Config

# ==============================================================================
# 0. C·∫§U H√åNH CHI·∫æN THU·∫¨T (Tactical Config)
# ==============================================================================
Config.LOGS_DIR.mkdir(parents=True, exist_ok=True)
Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# File l∆∞u k·∫øt qu·∫£ (D√πng ƒë·ªÉ Resume)
OUTPUT_FILE = Config.BASE_DIR / "output" / "submission.csv"

# C·∫•u h√¨nh ch·∫°y an to√†n tuy·ªát ƒë·ªëi
MAX_CONCURRENT_TASKS = 1      # Ch·∫°y t·ª´ng c√¢u m·ªôt (Ch·∫≠m nh∆∞ng ch·∫Øc 100%)
TIMEOUT_PER_QUESTION = 120    # 2 ph√∫t/c√¢u (ƒê·ªß ƒë·ªÉ retry n·∫øu m·∫°ng lag)

# Rate Limit (T·ªëc ƒë·ªô)
LIMITER_LARGE = AsyncLimiter(20, 60)   # 20 req/ph√∫t
LIMITER_SMALL = AsyncLimiter(50, 60)   # 50 req/ph√∫t
LIMITER_EMBED = AsyncLimiter(300, 60)  # 300 req/ph√∫t

# Ng√¢n s√°ch (Quota) - ƒê·ªÉ theo d√µi
QUOTA_LARGE = 500
QUOTA_SMALL = 1000

# Constants
THRESHOLD_SMALL_CONTEXT = 20000 
TOP_K = 12
ALPHA_VECTOR = 0.7
BM25_FILE = Config.OUTPUT_DIR / "bm25_index.pkl"

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler(Config.LOGS_DIR / 'inference_resume.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("VNPT_BOT")

# ==============================================================================
# 1. C√ÅC H√ÄM X·ª¨ L√ù (UTILS)
# ==============================================================================
def is_sensitive_topic(question):
    q_lower = question.lower()
    blacklist = [
        "sex", "khi√™u d√¢m", "ƒë·ªìi tr·ª•y", "l√†m t√¨nh", "·∫•u d√¢m", "k√≠ch d·ª•c",
        "ph·∫£n ƒë·ªông", "kh·ªßng b·ªë", "gi·∫øt ng∆∞·ªùi", "ma t√∫y", "bu√¥n l·∫≠u", "v≈© kh√≠", "b·∫°o l·ª±c",
        "x√∫c ph·∫°m", "lƒÉng m·∫°", "ƒë·∫£ng c·ªông s·∫£n", "xuy√™n t·∫°c", "c·ªù b·∫°c", "c√° ƒë·ªô"
    ]
    whitelist = [
        "lu·∫≠t", "ngh·ªã ƒë·ªãnh", "quy ƒë·ªãnh", "th√¥ng t∆∞", "ph√°p lu·∫≠t", "hi·∫øn ph√°p",
        "l·ªãch s·ª≠", "chi·∫øn tranh", "kh√°ng chi·∫øn", "v·ª• √°n", "t√≤a √°n", "x√©t x·ª≠",
        "t√°c h·∫°i", "ph√≤ng ch·ªëng", "ngƒÉn ch·∫∑n", "kh√°i ni·ªám", "ƒë·ªãnh nghƒ©a"
    ]
    has_bad = any(w in q_lower for w in blacklist)
    has_good = any(w in q_lower for w in whitelist)
    return has_bad and not has_good

def find_refusal_key(options_map):
    keywords = ["kh√¥ng th·ªÉ tr·∫£ l·ªùi", "t·ª´ ch·ªëi", "vi ph·∫°m", "nh·∫°y c·∫£m", "kh√¥ng ph√π h·ª£p"]
    for label, text in options_map.items():
        if any(kw in str(text).lower() for kw in keywords): return label
    return None

def get_dynamic_options(row):
    options = []
    if 'choices' in row and isinstance(row['choices'], list): options = row['choices']
    elif 'options' in row and isinstance(row['options'], list): options = row['options']
    else:
        i = 1
        while True:
            val = row.get(f"option_{i}")
            if not val or str(val).lower() == 'nan': break
            options.append(str(val))
            i += 1
    return {chr(65 + i): str(text) for i, text in enumerate(options)}

def extract_answer_two_step(text, options_map):
    valid_keys = list(options_map.keys())
    fallback = valid_keys[0]
    if not text: return fallback
    text = text.strip()
    
    # ∆Øu ti√™n 1: Format chu·∫©n
    match = re.search(r'###\s*ƒê√ÅP √ÅN[:\s\n]*([A-Z])', text, re.IGNORECASE)
    if match and match.group(1).upper() in valid_keys: return match.group(1).upper()
    
    # ∆Øu ti√™n 2: Markdown
    match = re.search(r'\*\*([A-Z])\*\*', text)
    if match and match.group(1).upper() in valid_keys: return match.group(1).upper()

    # Fallback: T√¨m k√Ω t·ª± cu·ªëi c√πng
    matches = re.findall(r'\b([A-Z])\b', text)
    for m in reversed(matches):
        if m.upper() in valid_keys: return m.upper()
    return fallback

def heuristic_answer(options_map):
    # Ch·ªçn ƒë√°p √°n d√†i nh·∫•t
    return max(options_map.items(), key=lambda x: len(str(x[1])))[0]

def build_prompt(question, options_text, docs):
    context = "".join([f"--- T√ÄI LI·ªÜU #{i+1} ({d['title']}) ---\n{d['text']}\n\n" for i, d in enumerate(docs)])
    sys_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n gia (STEM & X√£ h·ªôi).
NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi tr·∫Øc nghi·ªám d·ª±a tr√™n d·ªØ li·ªáu.
ƒê·ªäNH D·∫†NG:
### SUY LU·∫¨N:
[Ph√¢n t√≠ch ng·∫Øn]
### ƒê√ÅP √ÅN:
[Ch·ªâ vi·∫øt 1 k√Ω t·ª± A, B, C...]"""
    user_prompt = f"D·ªÆ LI·ªÜU:\n{context}\n\nC√ÇU H·ªéI: {question}\nL·ª∞A CH·ªåN:\n{options_text}\n\nTR·∫¢ L·ªúI THEO ƒê√öNG ƒê·ªäNH D·∫†NG:"
    return [{"role": "system", "content": sys_prompt}, {"role": "user", "content": user_prompt}]

# ==============================================================================
# 2. RETRIEVER & API CLIENTS
# ==============================================================================
class HybridRetriever:
    def __init__(self, qdrant_client):
        self.client = qdrant_client
        self.bm25 = None
        if BM25_FILE.exists():
            try:
                with open(BM25_FILE, "rb") as f: self.bm25 = pickle.load(f)
                logger.info(f"BM25 loaded: {len(self.bm25.get('chunk_ids', []))} chunks")
            except: pass

    async def search(self, session, query, top_k=TOP_K):
        # 1. Embed
        query_vec = await get_embedding_async(session, query)
        vec_hits = []
        if query_vec:
            for _ in range(3):
                try:
                    res = await self.client.query_points(Config.COLLECTION_NAME, query=query_vec, limit=top_k, with_payload=True)
                    vec_hits = res.points
                    break
                except: await asyncio.sleep(1)

        # 2. BM25
        bm25_hits = []
        if self.bm25:
            try:
                tokens = word_tokenize(query.lower())
                scores = self.bm25['bm25_obj'].get_scores(tokens)
                top_idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k*2]
                thresh = max(1.0, scores[top_idxs[int(len(top_idxs)*0.3)]]) if top_idxs else 1.0
                for idx in top_idxs:
                    if scores[idx] >= thresh:
                        bm25_hits.append({"id": self.bm25['chunk_ids'][idx], "score": scores[idx], "text": self.bm25['texts'][idx], "title": self.bm25['titles'][idx]})
            except: pass

        # 3. Fusion
        fused = {}
        max_v = max([h.score for h in vec_hits]) if vec_hits else 1.0
        for h in vec_hits:
            fused[h.payload['chunk_id']] = {"text": h.payload['text'], "title": h.payload['title'], "score": (h.score/max_v)*ALPHA_VECTOR}
        
        max_b = max([h['score'] for h in bm25_hits]) if bm25_hits else 1.0
        for h in bm25_hits:
            norm = (h['score']/max_b)*(1-ALPHA_VECTOR)
            cid = h['id']
            if cid in fused: fused[cid]['score'] += norm
            else: fused[cid] = {"text": h['text'], "title": h['title'], "score": norm}
        
        return sorted(fused.values(), key=lambda x: x['score'], reverse=True)[:top_k]

async def get_embedding_async(session, text):
    await LIMITER_EMBED.acquire()
    creds = Config.VNPT_CREDENTIALS.get(Config.MODEL_EMBEDDING_API)
    headers = {'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}', 'Token-id': creds['token_id'], 'Token-key': creds['token_key'], 'Content-Type': 'application/json'}
    payload = {"model": Config.MODEL_EMBEDDING_API, "input": text, "encoding_format": "float"}
    for i in range(2):
        try:
            async with session.post(Config.VNPT_EMBEDDING_URL, json=payload, headers=headers, timeout=30) as r:
                if r.status == 200:
                    d = await r.json()
                    if 'data' in d: return d['data'][0]['embedding']
                elif r.status in [429, 500]: await asyncio.sleep(2)
        except: await asyncio.sleep(1)
    return None

async def call_llm_generic(session, messages, model_name, stats, max_tokens=1024):
    limiter = LIMITER_LARGE if "large" in model_name.lower() else LIMITER_SMALL
    await limiter.acquire()
    
    # Ghi nh·∫≠n d√πng quota
    if "large" in model_name.lower(): stats['used_large'] += 1
    else: stats['used_small'] += 1

    try:
        creds = Config.VNPT_CREDENTIALS.get(model_name)
        url = f"{Config.VNPT_API_URL}/{model_name.replace('_', '-')}"
        headers = {'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}', 'Token-id': creds['token_id'], 'Token-key': creds['token_key'], 'Content-Type': 'application/json'}
        payload = {"model": model_name, "messages": messages, "temperature": 0.1, "top_p": 0.95, "max_completion_tokens": max_tokens}
        
        # Jitter ƒë·ªÉ tr√°nh d·ªìn toa
        await asyncio.sleep(random.uniform(0.5, 1.5))

        async with session.post(url, json=payload, headers=headers, timeout=90) as resp:
            if resp.status == 200:
                d = await resp.json()
                if 'choices' in d: return d['choices'][0]['message']['content']
            elif resp.status >= 400:
                logger.warning(f"‚ö†Ô∏è API {model_name} Error {resp.status}")
    except Exception as e:
        logger.warning(f"üîå Net Error {model_name}: {str(e)[:30]}")
    return None

# ==============================================================================
# 3. CORE LOGIC (PROCESS SINGLE ROW)
# ==============================================================================
async def process_row_logic(session, retriever, row, stats):
    """X·ª≠ l√Ω 1 d√≤ng, tr·∫£ v·ªÅ k·∫øt qu·∫£"""
    qid = row.get('qid', row.get('id', 'unknown'))
    question = row.get('question', '')
    true_label = row.get('answer', None)
    
    opts = get_dynamic_options(row)
    opt_text = "\n".join([f"{k}. {v}" for k, v in opts.items()])

    # 1. Safety
    if is_sensitive_topic(question):
        ans = find_refusal_key(opts) or "A"
        logger.info(f"üö´ Q:{qid} Sensitive")
        return {"qid": qid, "answer": ans, "is_correct": ans == true_label if true_label else None}

    # 2. Retrieval
    docs = await retriever.search(session, question, top_k=TOP_K)
    msgs = build_prompt(question, opt_text, docs)
    ctx_len = sum([len(d['text']) for d in docs])

    # 3. Model
    model = Config.LLM_MODEL_LARGE
    if ctx_len < THRESHOLD_SMALL_CONTEXT: model = Config.LLM_MODEL_SMALL

    # 4. Infer
    raw = await call_llm_generic(session, msgs, model, stats)
    
    if not raw:
        # Fallback
        fallback_model = Config.LLM_MODEL_SMALL if model == Config.LLM_MODEL_LARGE else Config.LLM_MODEL_LARGE
        logger.warning(f"‚ö†Ô∏è Q:{qid} Fallback -> {fallback_model}")
        raw = await call_llm_generic(session, msgs, fallback_model, stats)

    # 5. Extract
    if not raw:
        ans = heuristic_answer(opts)
        logger.error(f"0 Q:{qid} Failed all models -> Heuristic")
    else:
        ans = extract_answer_two_step(raw, opts)

    is_correct = (ans == true_label) if true_label else None
    status = "1" if is_correct else ("0" if is_correct is False else "")
    logger.info(f"Q:{qid} | Ans:{ans} {status}")
    
    return {"qid": qid, "answer": ans, "is_correct": is_correct}

# ==============================================================================
# 4. MAIN LOOP WITH RESUME
# ==============================================================================
async def main():
    # 1. Load Data
    files = [Config.BASE_DIR / "data" / "val.json", Config.BASE_DIR / "data" / "test.json"]
    input_file = next((f for f in files if f.exists()), None)
    if not input_file: return
    with open(input_file, 'r', encoding='utf-8') as f: data = json.load(f)

    # 2. Check Resume (ƒê·ªçc file ƒë√£ l∆∞u)
    processed_ids = set()
    if OUTPUT_FILE.exists():
        try:
            df_done = pd.read_csv(OUTPUT_FILE)
            processed_ids = set(df_done['qid'].astype(str))
            logger.info(f"RESUMING... Found {len(processed_ids)} processed questions.")
        except: pass
    
    # L·ªçc c√¢u ch∆∞a l√†m
    data_to_process = [r for r in data if str(r.get('qid', r.get('id'))) not in processed_ids]
    
    if not data_to_process:
        logger.info("ALL DONE! Nothing to process.")
        return

    logger.info(f"REMAINING: {len(data_to_process)}/{len(data)} questions")

    # 3. Setup
    qdrant_client = AsyncQdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY, timeout=30)
    retriever = HybridRetriever(qdrant_client)
    stats = {'used_large': 0, 'used_small': 0}
    
    # 4. Run Sequential (V√≤ng l·∫∑p ƒë∆°n lu·ªìng)
    conn = aiohttp.TCPConnector(limit=1, force_close=True, enable_cleanup_closed=True)
    async with aiohttp.ClientSession(connector=conn) as session:
        
        for i, row in enumerate(data_to_process):
            qid = row.get('qid', row.get('id'))
            
            # Retry loop cho t·ª´ng c√¢u
            for attempt in range(3):
                try:
                    # Timeout c·ª©ng
                    result = await asyncio.wait_for(
                        process_row_logic(session, retriever, row, stats),
                        timeout=TIMEOUT_PER_QUESTION
                    )
                    
                    # --- WRITE TO DISK IMMEDIATELY ---
                    df_res = pd.DataFrame([result])
                    # N·∫øu file ch∆∞a c√≥ th√¨ ghi header, c√≥ r·ªìi th√¨ append kh√¥ng header
                    need_header = not OUTPUT_FILE.exists()
                    df_res[['qid', 'answer']].to_csv(OUTPUT_FILE, mode='a', header=need_header, index=False)
                    
                    break # Success -> Next question
                    
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout Q:{qid} (Attempt {attempt+1})")
                    if attempt == 2:
                        # Fail h·∫≥n -> Ghi 'A' ƒë·ªÉ l·∫ßn sau kh√¥ng b·ªã k·∫πt
                        pd.DataFrame([{"qid": qid, "answer": "A"}]).to_csv(OUTPUT_FILE, mode='a', header=not OUTPUT_FILE.exists(), index=False)
                except Exception as e:
                    logger.error(f"Error Q:{qid}: {e}")
                    await asyncio.sleep(5)

            # Ngh·ªâ ng∆°i gi·ªØa c√°c c√¢u ƒë·ªÉ server th·ªü
            await asyncio.sleep(1)

    await qdrant_client.close()
    logger.info("BATCH COMPLETED!")

    if OUTPUT_FILE.exists():
        print("\n" + "="*40)
        print("T·ªîNG K·∫æT TO√ÄN B·ªò (CUMULATIVE STATS)")
        print("="*40)
        
        try:
            # 1. ƒê·ªçc to√†n b·ªô k·∫øt qu·∫£ ƒë√£ l∆∞u trong CSV
            df_results = pd.read_csv(OUTPUT_FILE)
            
            # 2. T·∫°o t·ª´ ƒëi·ªÉn ƒë√°p √°n ƒë√∫ng (Ground Truth) t·ª´ file input g·ªëc
            # L∆∞u √Ω: Ch·ªâ l·∫•y nh·ªØng c√¢u c√≥ tr∆∞·ªùng 'answer' (ƒë·ªÅ ph√≤ng file Test kh√¥ng c√≥)
            ground_truth = {
                str(r.get('qid', r.get('id'))): str(r.get('answer')).strip() 
                for r in data if r.get('answer')
            }
            
            if not ground_truth:
                print("ƒê√¢y l√† t·∫≠p Test (kh√¥ng c√≥ ƒë√°p √°n) -> B·ªè qua t√≠nh ƒëi·ªÉm.")
            else:
                correct_count = 0
                total_checked = 0
                
                # 3. So kh·ªõp t·ª´ng c√¢u trong CSV v·ªõi ƒë√°p √°n g·ªëc
                for _, row in df_results.iterrows():
                    qid = str(row['qid'])
                    # Chuy·ªÉn v·ªÅ string v√† strip ƒë·ªÉ so s√°nh ch√≠nh x√°c
                    pred = str(row['answer']).strip()
                    
                    if qid in ground_truth:
                        total_checked += 1
                        true_label = ground_truth[qid]
                        
                        # So s√°nh
                        if pred == true_label:
                            correct_count += 1
                
                # 4. In k·∫øt qu·∫£
                if total_checked > 0:
                    acc = (correct_count / total_checked) * 100
                    print(f"ƒê√£ l√†m: {total_checked}/{len(ground_truth)} c√¢u")
                    print(f"ƒê√∫ng  : {correct_count} c√¢u")
                    print(f"T·ª∑ l·ªá : {acc:.2f}%")
                else:
                    print("‚ö†Ô∏è Ch∆∞a c√≥ c√¢u n√†o kh·ªõp ID v·ªõi t·∫≠p d·ªØ li·ªáu g·ªëc.")
                    
        except Exception as e:
            print(f"L·ªói t√≠nh ƒëi·ªÉm: {e}")

        print(f"File k·∫øt qu·∫£: {OUTPUT_FILE}")

if __name__ == "__main__":
    if sys.platform == 'win32': asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())