"""
RAG Inference Engine (Hybrid Search Version)
============================================
Engine n√†y th·ª±c hi·ªán t√¨m ki·∫øm "Lai" (Hybrid):
1. T√¨m b·∫±ng Dense Vector (Hi·ªÉu ng·ªØ nghƒ©a).
2. T√¨m b·∫±ng Sparse Vector (B·∫Øt t·ª´ kh√≥a ch√≠nh x√°c - BM25).
3. D√πng thu·∫≠t to√°n RRF (Reciprocal Rank Fusion) ƒë·ªÉ tr·ªôn k·∫øt qu·∫£ l·∫°i.
"""

import time
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http import models
import torch

# Import Config
from config import Config

# Import FastEmbed cho Sparse
try:
    from fastembed import SparseTextEmbedding
    SPARSE_AVAILABLE = True
except ImportError:
    SPARSE_AVAILABLE = False
    print("‚ö†Ô∏è Ch∆∞a c√†i 'fastembed'. Ch·∫ø ƒë·ªô Hybrid s·∫Ω b·ªã t·∫Øt.")

class RAGPipeline:
    def __init__(self):
        print("‚è≥ ƒêang kh·ªüi t·∫°o Search Engine...")
        
        # --- 1. LOAD DENSE MODEL (Ng·ªØ nghƒ©a) ---
        # Model n√†y B·∫ÆT BU·ªòC ph·∫£i kh·ªõp v·ªõi model l√∫c Indexing
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   - Device: {device.upper()}")
        
        try:
            self.dense_model = SentenceTransformer(Config.MODEL_PATH, device=device)
            print(f"   - Dense Model: {Config.MODEL_PATH}")
        except:
            print(f"   - ‚ö†Ô∏è Fallback Dense: {Config.DUMMY_MODEL_NAME}")
            self.dense_model = SentenceTransformer(Config.DUMMY_MODEL_NAME, device=device)

        # --- 2. LOAD SPARSE MODEL (T·ª´ kh√≥a) ---
        self.sparse_model = None
        if SPARSE_AVAILABLE and Config.SPARSE_AVAILABLE:
            print("   - Loading Sparse Model (BM25)...")
            # L∆∞u √Ω: Ph·∫£i d√πng ƒë√∫ng t√™n model ƒë√£ d√πng l√∫c Indexing ("Qdrant/bm25")
            self.sparse_model = SparseTextEmbedding(model_name="Qdrant/bm25")

        # --- 3. K·∫æT N·ªêI QDRANT ---
        try:
            if Config.USE_QDRANT_CLOUD:
                self.qdrant = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)
            else:
                self.qdrant = QdrantClient(url=Config.QDRANT_URL)
            print("‚úÖ K·∫øt n·ªëi Database th√†nh c√¥ng!")
        except Exception as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi Qdrant: {e}")

    def retrieve_hybrid(self, query: str, top_k: int = 5) -> List[Dict]:
        t0 = time.time()
        
        # A. T·∫°o Dense Vector
        dense_vector = self.dense_model.encode(query, normalize_embeddings=True).tolist()
        
        # B. T·∫°o Sparse Vector
        sparse_vector = None
        if self.sparse_model:
            sparse_output = list(self.sparse_model.embed(query))[0]
            sparse_vector = models.SparseVector(
                indices=sparse_output.indices.tolist(),
                values=sparse_output.values.tolist()
            )

        # C. Prefetch
        prefetch_requests = [
            models.Prefetch(
                query=dense_vector,
                using="dense",
                limit=top_k * 2
            )
        ]
        
        if sparse_vector:
            prefetch_requests.append(
                models.Prefetch(
                    query=sparse_vector,
                    using="sparse",
                    limit=top_k * 2
                )
            )

        # D. Execute Search (ƒê√É S·ª¨A L·ªñI ·ªû ƒê√ÇY)
        search_result = self.qdrant.query_points(
            collection_name=Config.COLLECTION_NAME,
            prefetch=prefetch_requests,
            # S·ª≠a tham s·ªë 'method' th√†nh 'fusion'
            query=models.FusionQuery(fusion=models.Fusion.RRF), 
            limit=top_k,
            with_payload=True
        )
        
        # E. Format
        results = []
        for point in search_result.points:
            results.append({
                "score": point.score,
                "title": point.payload.get("title", "No Title"),
                "text": point.payload.get("text", ""),
                "url": point.payload.get("url", "")
            })
            
        print(f"üîç T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ trong {time.time()-t0:.3f}s")
        return results

    def run_test(self, query: str):
        """H√†m test nhanh ƒë·ªÉ xem k·∫øt qu·∫£"""
        print(f"\n‚ùì C√¢u h·ªèi: {query}")
        docs = self.retrieve_hybrid(query, top_k=3)
        
        print("--- K·∫æT QU·∫¢ T√åM KI·∫æM (Top 3) ---")
        for i, d in enumerate(docs):
            print(f"[{i+1}] (Score: {d['score']:.4f}) {d['title']}")
            print(f"    N·ªôi dung: {d['text']}") # In 150 k√Ω t·ª± ƒë·∫ßu
            print("-" * 30)

# ===========================
# CH·∫†Y TH·ª¨
# ===========================
if __name__ == "__main__":
    engine = RAGPipeline()
    
    # Test 1: C√¢u h·ªèi c·∫ßn ng·ªØ nghƒ©a (Dense gi·ªèi)
    engine.run_test("√ù nghƒ©a c·ªßa chi·∫øn th·∫Øng ƒêi·ªán Bi√™n Ph·ªß?")
    
    # Test 2: C√¢u h·ªèi c·∫ßn t·ª´ kh√≥a ch√≠nh x√°c (Sparse gi·ªèi)
    # Th·ª≠ h·ªèi v·ªÅ m·ªôt t√™n ri√™ng ho·∫∑c s·ªë li·ªáu c·ª• th·ªÉ trong d·ªØ li·ªáu c·ªßa b·∫°n
    engine.run_test("Tri·ªÅu ƒë·∫°i nh√† Nguy·ªÖn t·ª´ nƒÉm n√†o?")
