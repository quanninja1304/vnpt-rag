import asyncio
import aiohttp
import pandas as pd
import json
import re
import pickle
import sys
import os
import time
import logging
from pathlib import Path
from datetime import datetime, timedelta
from collections import deque
from qdrant_client import AsyncQdrantClient, models
from underthesea import word_tokenize
from config import Config

# ==============================================================================
# 0. CONFIG & LOGGING
# ==============================================================================
Config.LOGS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler(Config.LOGS_DIR / 'inference.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("VNPT_BOT")

# ==============================================================================
# 1. SMART RATE LIMITER (Daily Quota Management)
# ==============================================================================
# class SmartRateLimiter:
#     """
#     Rate limiter theo ng√†y v·ªõi auto-recovery.
#     T·ª± ƒë·ªông ƒë·ª£i khi h·∫øt quota v√† reset sau 24h.
#     """
#     def __init__(self, daily_limit, buffer=0.9):
#         self.daily_limit = int(daily_limit * buffer)  # ƒê·ªÉ buffer 10% an to√†n
#         self.requests_log = deque()
#         self.lock = asyncio.Lock()
    
#     async def acquire(self):
#         async with self.lock:
#             now = datetime.now()
#             cutoff = now - timedelta(days=1)
            
#             # X√≥a c√°c request c≈© h∆°n 24h
#             while self.requests_log and self.requests_log[0] < cutoff:
#                 self.requests_log.popleft()
            
#             # Ki·ªÉm tra quota
#             if len(self.requests_log) >= self.daily_limit:
#                 oldest = self.requests_log[0]
#                 sleep_time = (oldest - cutoff).total_seconds()
#                 logger.warning(f"‚è≥ Daily limit reached ({self.daily_limit}). Sleeping {sleep_time:.0f}s")
#                 await asyncio.sleep(sleep_time + 1)
#                 return await self.acquire()  # Recursive check sau khi sleep
            
#             # Ghi nh·∫≠n request
#             self.requests_log.append(now)
#             logger.debug(f"‚úÖ Token acquired. Used: {len(self.requests_log)}/{self.daily_limit}")

class SimpleRateLimiter:
    """
    Rate limiter ƒë∆°n gi·∫£n theo th·ªùi gian.
    ƒê·∫£m b·∫£o kho·∫£ng c√°ch t·ªëi thi·ªÉu gi·ªØa c√°c request.
    """
    def __init__(self, requests_per_minute=50):
        self.min_interval = 60.0 / requests_per_minute  # gi√¢y gi·ªØa m·ªói request
        self.last_request_time = 0
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        async with self.lock:
            now = time.time()
            time_since_last = now - self.last_request_time
            
            if time_since_last < self.min_interval:
                sleep_time = self.min_interval - time_since_last
                logger.debug(f"‚è≥ Rate limit: sleeping {sleep_time:.2f}s")
                await asyncio.sleep(sleep_time)
            
            self.last_request_time = time.time()


class EmbeddingRateLimiter:
    """Rate limiter theo ph√∫t cho embedding (300 req/ph√∫t)"""
    def __init__(self, per_minute=300):
        self.per_minute = per_minute
        self.requests_log = deque()
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        async with self.lock:
            now = time.time()
            cutoff = now - 60
            
            # X√≥a request c≈© h∆°n 1 ph√∫t
            while self.requests_log and self.requests_log[0] < cutoff:
                self.requests_log.popleft()
            
            # N·∫øu ƒë√£ ƒë·∫ßy -> ƒë·ª£i
            if len(self.requests_log) >= self.per_minute:
                sleep_time = self.requests_log[0] - cutoff + 0.1
                await asyncio.sleep(sleep_time)
                return await self.acquire()
            
            self.requests_log.append(now)

# Kh·ªüi t·∫°o limiters
# LIMITER_LARGE = SmartRateLimiter(daily_limit=500)
# LIMITER_SMALL = SmartRateLimiter(daily_limit=1000)
# LIMITER_EMBED = EmbeddingRateLimiter(per_minute=300)

LIMITER_LARGE = SimpleRateLimiter(requests_per_minute=0.3)  # 1 req/3s = 20/min max
LIMITER_SMALL = SimpleRateLimiter(requests_per_minute=0.6)  # 1 req/1.7s = 35/min max
LIMITER_EMBED = SimpleRateLimiter(requests_per_minute=300)  # 300/min nh∆∞ c≈©

# ==============================================================================
# 2. CONFIGURATION
# ==============================================================================
MAX_CONCURRENT_TASKS = 3  # TƒÉng t·ª´ 2 l√™n 3 (v√¨ ƒë√£ fix rate limiter)
TIMEOUT_PER_QUESTION = 120  # Gi·∫£m t·ª´ 300s xu·ªëng 120s (2 ph√∫t)
THRESHOLD_SMALL_CONTEXT = 20000  # Context < 20k chars -> d√πng Small model
TOP_K = 12
ALPHA_VECTOR = 0.7
BM25_FILE = Config.OUTPUT_DIR / "bm25_index.pkl"

# ==============================================================================
# 3. SMART SAFETY CHECK (OFFLINE)
# ==============================================================================
def is_sensitive_topic(question):
    """
    Ki·ªÉm tra nh·∫°y c·∫£m th√¥ng minh:
    - Block: C√°c t·ª´ kh√≥a ƒëen.
    - Allow: N·∫øu c√≥ t·ª´ kh√≥a h·ªçc thu·∫≠t/ph√°p lu·∫≠t ƒëi k√®m th√¨ CHO PH√âP.
    """
    q_lower = question.lower()
    
    # Danh s√°ch ƒëen
    blacklist = [
        "sex", "khi√™u d√¢m", "ƒë·ªìi tr·ª•y", "l√†m t√¨nh", "·∫•u d√¢m", "k√≠ch d·ª•c",
        "b·∫°o ƒë·ªông", "l·∫≠t ƒë·ªï", "ph·∫£n ƒë·ªông", "kh·ªßng b·ªë", "bi·ªÉu t√¨nh", "ch·ªëng ph√°",
        "gi·∫øt ng∆∞·ªùi", "t·ª± t·ª≠", "ma t√∫y", "bu√¥n l·∫≠u", "v≈© kh√≠", "b·∫°o l·ª±c",
        "x√∫c ph·∫°m", "lƒÉng m·∫°", "ƒë·∫£ng c·ªông s·∫£n", "xuy√™n t·∫°c", "c·ªù b·∫°c", "c√° ƒë·ªô"
    ]
    
    # Danh s√°ch tr·∫Øng (B·∫£o v·ªá c√°c c√¢u h·ªèi h·ªçc thu·∫≠t)
    whitelist = [
        "lu·∫≠t", "ngh·ªã ƒë·ªãnh", "quy ƒë·ªãnh", "th√¥ng t∆∞", "ph√°p lu·∫≠t", "hi·∫øn ph√°p",
        "l·ªãch s·ª≠", "chi·∫øn tranh", "kh√°ng chi·∫øn", "v·ª• √°n", "t√≤a √°n", "x√©t x·ª≠",
        "t√°c h·∫°i", "ph√≤ng ch·ªëng", "ngƒÉn ch·∫∑n", "kh√°i ni·ªám", "ƒë·ªãnh nghƒ©a"
    ]

    has_bad_word = any(w in q_lower for w in blacklist)
    has_good_word = any(w in q_lower for w in whitelist)

    # N·∫øu c√≥ t·ª´ x·∫•u NH∆ØNG c≈©ng c√≥ t·ª´ h·ªçc thu·∫≠t -> An to√†n
    if has_bad_word and has_good_word:
        return False
    
    return has_bad_word

def find_refusal_key(options_map):
    """T√¨m ƒë√°p √°n t·ª´ ch·ªëi trong options"""
    keywords = ["kh√¥ng th·ªÉ tr·∫£ l·ªùi", "t·ª´ ch·ªëi", "vi ph·∫°m", "nh·∫°y c·∫£m", 
                "kh√¥ng ph√π h·ª£p", "t√°c ƒë·ªông ti√™u c·ª±c"]
    for label, text in options_map.items():
        if any(kw in str(text).lower() for kw in keywords):
            return label
    return None

# ==============================================================================
# 4. RETRIEVER (DEPENDENCY INJECTION)
# ==============================================================================
class HybridRetriever:
    def __init__(self, qdrant_client):
        self.client = qdrant_client
        self.bm25 = None
        if BM25_FILE.exists():
            try:
                with open(BM25_FILE, "rb") as f:
                    self.bm25 = pickle.load(f)
                logger.info(f"‚úÖ BM25 loaded: {len(self.bm25.get('chunk_ids', []))} chunks")
            except Exception as e:
                logger.error(f"‚ùå BM25 Load Error: {e}")

    async def search_qdrant_retry(self, query_vec, top_k, max_retries=3):
        """Search Qdrant v·ªõi retry"""
        for i in range(max_retries):
            try:
                response = await self.client.query_points(
                    collection_name=Config.COLLECTION_NAME,
                    query=query_vec,
                    limit=top_k,
                    with_payload=True
                )
                return response.points
            except Exception as e:
                if i == max_retries - 1:
                    logger.error(f"‚ùå Qdrant Fail: {e}")
                    return []
                await asyncio.sleep(1)
        return []

    async def search(self, session, query, top_k=TOP_K):
        """Hybrid search: Vector + BM25"""
        # Vector Search
        query_vec = await get_embedding_async(session, query)
        vec_hits = []
        if query_vec:
            vec_hits = await self.search_qdrant_retry(query_vec, top_k)
        
        # BM25 Search
        bm25_hits = []
        if self.bm25:
            try:
                tokens = word_tokenize(query.lower())
                scores = self.bm25['bm25_obj'].get_scores(tokens)
                top_idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k*2]
                
                # Dynamic threshold
                batch_scores = [scores[i] for i in top_idxs]
                threshold = 1.0
                if batch_scores:
                    dynamic_thresh = batch_scores[int(len(batch_scores) * 0.3)]
                    threshold = max(1.0, dynamic_thresh)

                for idx in top_idxs:
                    if scores[idx] >= threshold:
                        bm25_hits.append({
                            "id": self.bm25['chunk_ids'][idx],
                            "score": scores[idx],
                            "text": self.bm25['texts'][idx],
                            "title": self.bm25['titles'][idx]
                        })
            except Exception as e:
                logger.error(f"‚ùå BM25 Error: {e}")

        # Fusion
        fused = {}
        max_v = max([h.score for h in vec_hits]) if vec_hits else 1.0
        for h in vec_hits:
            fused[h.payload['chunk_id']] = {
                "text": h.payload['text'],
                "title": h.payload['title'],
                "score": (h.score/max_v) * ALPHA_VECTOR
            }
        
        max_b = max([h['score'] for h in bm25_hits]) if bm25_hits else 1.0
        for h in bm25_hits:
            norm = (h['score']/max_b) * (1 - ALPHA_VECTOR)
            cid = h['id']
            if cid in fused:
                fused[cid]['score'] += norm
            else:
                fused[cid] = {"text": h['text'], "title": h['title'], "score": norm}
        
        return sorted(fused.values(), key=lambda x: x['score'], reverse=True)[:top_k]

# ==============================================================================
# 5. API CLIENTS
# ==============================================================================
async def call_llm_generic(session, messages, model_name, max_tokens=1024):
    """
    G·ªçi LLM v·ªõi smart rate limiting.
    KH√îNG retry - ch·ªâ g·ªçi 1 l·∫ßn duy nh·∫•t ƒë·ªÉ ti·∫øt ki·ªám quota.
    """
    # Ch·ªçn limiter theo model
    if "large" in model_name.lower():
        limiter = LIMITER_LARGE
    else:
        limiter = LIMITER_SMALL
    
    # Acquire token (t·ª± ƒë·ªông ƒë·ª£i n·∫øu h·∫øt quota)
    await limiter.acquire()
    
    try:
        creds = Config.VNPT_CREDENTIALS.get(model_name)
        url = f"{Config.VNPT_API_URL}/{model_name.replace('_', '-')}"
        headers = {
            'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}',
            'Token-id': creds['token_id'],
            'Token-key': creds['token_key'],
            'Content-Type': 'application/json'
        }
        payload = {
            "model": model_name,
            "messages": messages,
            "temperature": 0.1,
            "top_p": 0.95,
            "max_completion_tokens": max_tokens
        }
        
        async with session.post(url, json=payload, headers=headers, timeout=90) as resp:
            if resp.status == 200:
                d = await resp.json()
                if 'choices' in d:
                    return d['choices'][0]['message']['content']
            
            # Log l·ªói API
            if resp.status >= 400:
                error_text = await resp.text()
                logger.error(f"‚ùå API Error {resp.status} ({model_name}): {error_text[:100]}")
            
    except asyncio.TimeoutError:
        logger.warning(f"‚è∞ LLM Timeout ({model_name})")
    except Exception as e:
        logger.warning(f"üîå Network Error ({model_name}): {str(e)[:50]}")
    
    return None

async def get_embedding_async(session, text):
    """Get embedding v·ªõi rate limit"""
    await LIMITER_EMBED.acquire()
    
    model = Config.MODEL_EMBEDDING_API
    creds = Config.VNPT_CREDENTIALS.get(model)
    headers = {
        'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}',
        'Token-id': creds['token_id'],
        'Token-key': creds['token_key'],
        'Content-Type': 'application/json'
    }
    payload = {
        "model": model,
        "input": text,
        "encoding_format": "float"
    }
    
    for i in range(2):  # Ch·ªâ retry 1 l·∫ßn
        try:
            async with session.post(Config.VNPT_EMBEDDING_URL, json=payload, 
                                   headers=headers, timeout=30) as r:
                if r.status == 200:
                    d = await r.json()
                    if 'data' in d:
                        return d['data'][0]['embedding']
                elif r.status in [429, 500] and i == 0:
                    await asyncio.sleep(2)
        except Exception as e:
            if i == 0:
                await asyncio.sleep(1)
    
    return None

# ==============================================================================
# 6. PROMPT BUILDER
# ==============================================================================
def build_prompt(question, options_text, docs):
    """Build prompt cho LLM"""
    context_str = ""
    for i, doc in enumerate(docs):
        context_str += f"--- T√ÄI LI·ªÜU #{i+1} ({doc['title']}) ---\n{doc['text']}\n\n"

    system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n gia v·ªÅ Vi·ªát Nam (STEM & X√£ h·ªôi).
NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi tr·∫Øc nghi·ªám.
ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI B·∫ÆT BU·ªòC:
### SUY LU·∫¨N:
[Ph√¢n t√≠ch ng·∫Øn g·ªçn]
### ƒê√ÅP √ÅN:
[Ch·ªâ vi·∫øt 1 k√Ω t·ª±: A, B, C...]

QUY T·∫ÆC:
1. ∆Øu ti√™n th√¥ng tin 2024-2025.
2. N·∫øu l√† c√¢u h·ªèi To√°n/L√Ω/H√≥a -> T·ª± t√≠nh to√°n t·ª´ng b∆∞·ªõc.
3. N·∫øu vi ph·∫°m an to√†n -> Ch·ªçn ƒë√°p √°n T·ª™ CH·ªêI.
4. N·∫øu h·ªèi v·ªÅ Lu·∫≠t/L·ªãch s·ª≠ c√≥ ch·ª©a t·ª´ nh·∫°y c·∫£m -> V·∫´n tr·∫£ l·ªùi theo ki·∫øn th·ª©c ph√°p lu·∫≠t."""

    user_prompt = f"""D·ªÆ LI·ªÜU:\n{context_str}\n\nC√ÇU H·ªéI: {question}\nL·ª∞A CH·ªåN:\n{options_text}\n\nTR·∫¢ L·ªúI THEO ƒê√öNG ƒê·ªäNH D·∫†NG (### SUY LU·∫¨N... ### ƒê√ÅP √ÅN...):"""
    
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

# ==============================================================================
# 7. ANSWER EXTRACTION
# ==============================================================================
def get_dynamic_options(row):
    """Extract options t·ª´ row (h·ªó tr·ª£ nhi·ªÅu format)"""
    options = []
    if 'choices' in row and isinstance(row['choices'], list):
        options = row['choices']
    elif 'options' in row and isinstance(row['options'], list):
        options = row['options']
    else:
        i = 1
        while True:
            val = row.get(f"option_{i}")
            if not val or str(val).lower() == 'nan':
                break
            options.append(str(val))
            i += 1
    
    mapped = {}
    for idx, text in enumerate(options):
        mapped[chr(65 + idx)] = str(text)
    return mapped

def extract_answer_two_step(text, options_map):
    """Extract ƒë√°p √°n t·ª´ LLM response"""
    valid_keys = list(options_map.keys())
    fallback = valid_keys[0]
    
    if not text:
        return fallback
    
    text = text.strip()
    
    # Pattern 1: ### ƒê√ÅP √ÅN: X
    match_strict = re.search(r'###\s*ƒê√ÅP √ÅN[:\s\n]*([A-Z])', text, re.IGNORECASE)
    if match_strict and match_strict.group(1).upper() in valid_keys:
        return match_strict.group(1).upper()
    
    # Pattern 2: C√°c pattern kh√°c
    patterns = [
        r'(?:ƒë√°p √°n|ch·ªçn|l√†)[:\s\*\-\.\[\(]*([A-Z])[\]\)\*\.]*$',
        r'\*\*([A-Z])\*\*',
        r'^([A-Z])[\.\)]\s'
    ]
    for pat in patterns:
        match = re.search(pat, text, re.IGNORECASE | re.MULTILINE)
        if match and match.group(1).upper() in valid_keys:
            return match.group(1).upper()
    
    return fallback

def heuristic_answer(options_map):
    """Fallback: Ch·ªçn ƒë√°p √°n d√†i nh·∫•t (th∆∞·ªùng ƒë√∫ng trong tr·∫Øc nghi·ªám)"""
    return max(options_map.items(), key=lambda x: len(x[1]))[0]

# ==============================================================================
# 8. PROCESSOR
# ==============================================================================
async def process_row_safe(sem, session, retriever, row, stats):
    """Process 1 c√¢u h·ªèi v·ªõi smart retry strategy"""
    try:
        async with sem:
            qid = row.get('qid', row.get('id', 'unknown'))
            question = row.get('question', '')
            true_label = row.get('answer', None)

            options_map = get_dynamic_options(row)
            options_text = "\n".join([f"{k}. {v}" for k, v in options_map.items()])

            # 1. Smart Safety Check (Offline)
            if is_sensitive_topic(question):
                logger.info(f"üö´ Q:{qid} -> Sensitive (Offline Check).")
                refusal_key = find_refusal_key(options_map)
                pred = refusal_key if refusal_key else "A"
                stats['sensitive'] += 1
                return {
                    "qid": qid,
                    "answer": pred,
                    "is_correct": pred == true_label if true_label else None
                }

            # 2. Retrieval
            docs = await retriever.search(session, question, top_k=TOP_K)
            messages = build_prompt(question, options_text, docs)
            context_chars = sum([len(d['text']) for d in docs])

            # 3. Smart Model Selection
            # ∆Øu ti√™n SMALL n·∫øu context nh·ªè (ti·∫øt ki·ªám quota large)
            if context_chars < THRESHOLD_SMALL_CONTEXT:
                model = Config.LLM_MODEL_SMALL
                logger.debug(f"Q:{qid} -> Small (ctx={context_chars})")
                stats['used_small'] += 1
            else:
                model = Config.LLM_MODEL_LARGE
                logger.debug(f"Q:{qid} -> Large (ctx={context_chars})")
                stats['used_large'] += 1

            # 4. G·ªçi model ch√≠nh (CH·ªà 1 L·∫¶N)
            raw_ans = await call_llm_generic(session, messages, model)
            
            # 5. Fallback n·∫øu model ch√≠nh fail
            if not raw_ans:
                fallback_model = (Config.LLM_MODEL_SMALL if model == Config.LLM_MODEL_LARGE 
                                 else Config.LLM_MODEL_LARGE)
                logger.warning(f"Q:{qid} -> Fallback to {fallback_model.split('_')[-1]}")
                raw_ans = await call_llm_generic(session, messages, fallback_model)
                stats['fallback'] += 1
                
                if fallback_model == Config.LLM_MODEL_SMALL:
                    stats['used_small'] += 1
                else:
                    stats['used_large'] += 1

            # 6. Extract answer ho·∫∑c d√πng heuristic
            if not raw_ans:
                logger.error(f"Q:{qid} -> Both models failed. Use heuristic.")
                final_key = heuristic_answer(options_map)
                stats['heuristic'] += 1
            else:
                final_key = extract_answer_two_step(raw_ans, options_map)

            # 7. Log result
            status = f"| {'‚úÖ' if final_key == true_label else '‚ùå'}" if true_label else ""
            logger.info(f"Q:{qid} | Ans:{final_key} {status}")
            
            return {
                "qid": qid,
                "answer": final_key,
                "is_correct": final_key == true_label if true_label else None
            }

    except Exception as e:
        logger.error(f"‚ùå Crash Q:{qid}: {e}")
        stats['crashed'] += 1
        return {"qid": qid, "answer": "A", "is_correct": False}

async def process_with_timeout(sem, session, retriever, row, stats):
    """Wrapper v·ªõi timeout"""
    try:
        return await asyncio.wait_for(
            process_row_safe(sem, session, retriever, row, stats),
            timeout=TIMEOUT_PER_QUESTION
        )
    except asyncio.TimeoutError:
        qid = row.get('qid', 'unknown')
        logger.error(f"‚è∞ Timeout Q:{qid}")
        stats['timeout'] += 1
        return {"qid": qid, "answer": "A", "is_correct": False}

# ==============================================================================
# 9. MAIN
# ==============================================================================
async def main():
    # Load data
    files = [
        Config.BASE_DIR / "data" / "val.json",
        Config.BASE_DIR / "data" / "test.json"
    ]
    input_file = next((f for f in files if f.exists()), None)
    if not input_file:
        logger.error("‚ùå No input file found!")
        return
    
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Initialize clients
    qdrant_client = AsyncQdrantClient(
        url=Config.QDRANT_URL,
        api_key=Config.QDRANT_API_KEY,
        timeout=20
    )
    retriever = HybridRetriever(qdrant_client)

    # Stats tracking
    stats = {
        'used_large': 0,
        'used_small': 0,
        'fallback': 0,
        'sensitive': 0,
        'heuristic': 0,
        'timeout': 0,
        'crashed': 0
    }
    
    # Progress tracking
    start_time = time.time()
    completed = 0
    
    async def track_progress(task):
        nonlocal completed
        result = await task
        completed += 1
        elapsed = time.time() - start_time
        eta = (elapsed / completed) * (len(data) - completed) if completed > 0 else 0
        logger.info(f"üìä Progress: {completed}/{len(data)} | ETA: {eta/60:.1f}min")
        return result

    # Setup connection
    conn = aiohttp.TCPConnector(limit=10, force_close=True)
    sem = asyncio.Semaphore(MAX_CONCURRENT_TASKS)

    print(f"üî• STARTING: {input_file.name}")
    print(f"üìù Total: {len(data)} questions | Concurrent: {MAX_CONCURRENT_TASKS}")
    print(f"‚è±Ô∏è  Timeout: {TIMEOUT_PER_QUESTION}s/question")
    print(f"üéØ Strategy: Small first (ctx<{THRESHOLD_SMALL_CONTEXT})")
    print("-" * 60)
    
    async with aiohttp.ClientSession(connector=conn) as session:
        tasks = [
            track_progress(process_with_timeout(sem, session, retriever, row, stats))
            for row in data
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    # Close client
    await qdrant_client.close()

    # Process results
    clean_results = []
    correct = 0
    has_label = False
    
    for r in results:
        if isinstance(r, dict):
            clean_results.append(r)
            if r.get('is_correct') is not None:
                has_label = True
                if r['is_correct']:
                    correct += 1
        else:
            logger.error(f"‚ùå Invalid result: {r}")
            clean_results.append({"qid": "unknown", "answer": "A"})

    # Print statistics
    print("\n" + "=" * 60)
    print("üìä FINAL STATISTICS")
    print("=" * 60)
    if has_label and len(clean_results) > 0:
        print(f"‚úÖ Correct: {correct}/{len(clean_results)} ({(correct/len(clean_results))*100:.2f}%)")
    print(f"ü§ñ Model Usage:")
    print(f"   - Large: {stats['used_large']} calls")
    print(f"   - Small: {stats['used_small']} calls")
    print(f"   - Fallback: {stats['fallback']} times")
    print(f"üõ°Ô∏è  Safety: {stats['sensitive']} sensitive questions")
    print(f"üé≤ Heuristic: {stats['heuristic']} times")
    print(f"‚è∞ Timeout: {stats['timeout']} questions")
    print(f"üí• Crashed: {stats['crashed']} questions")
    print(f"‚è±Ô∏è  Total time: {(time.time() - start_time)/60:.1f} minutes")
    print("=" * 60)

    # Save results
    output_file = Config.BASE_DIR / "output" / "submission.csv"
    pd.DataFrame(clean_results)[['qid', 'answer']].to_csv(output_file, index=False)
    print(f"üíæ Saved to: {output_file}")
    print("‚úÖ DONE!")

if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())