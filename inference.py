import asyncio
import aiohttp
import pandas as pd
import json
import re
import pickle
import sys
import os
import time
import logging
from pathlib import Path
from aiolimiter import AsyncLimiter
from qdrant_client import QdrantClient
from underthesea import word_tokenize
from config import Config

# ==============================================================================
# 0. LOGGING SETUP (PROFESSIONAL GRADE)
# ==============================================================================
Config.LOGS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler(Config.LOGS_DIR / 'submission.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("VNPT_BOT")

# ==============================================================================
# 1. C·∫§U H√åNH & CONSTANTS
# ==============================================================================
LIMITER_EMBED = AsyncLimiter(500, 60)
LIMITER_LLM = AsyncLimiter(100, 60)

MAX_CONCURRENT_TASKS = 8
TIMEOUT_PER_QUESTION = 55 # Gi√¢y (ƒê·ªÉ d∆∞ 5s x·ª≠ l√Ω fallback)

# Ng∆∞·ª°ng Large: 18k tokens ~ 45k chars. Safe = 40k.
THRESHOLD_LARGE_CHARS = 40000 

ALPHA_VECTOR = 0.7
BM25_FILE = Config.OUTPUT_DIR / "bm25_index.pkl"
QUOTA_FILE = Config.OUTPUT_DIR / "quota_tracker.json"

# ==============================================================================
# 2. QUOTA MANAGER
# ==============================================================================
class QuotaManager:
    def __init__(self, is_private_mode=False):
        self.is_private = is_private_mode
        self.daily_limit = 450 # Buffer an to√†n
        self.lock = asyncio.Lock()
        self.usage_data = self._load_usage()

    def _load_usage(self):
        if self.is_private: return {"count": 0}
        today_str = time.strftime("%Y-%m-%d")
        default_data = {"date": today_str, "count": 0}
        if os.path.exists(QUOTA_FILE):
            try:
                with open(QUOTA_FILE, 'r') as f:
                    data = json.load(f)
                    if data.get("date") == today_str:
                        logger.info(f"üìä Daily Usage Loaded: {data['count']}/{self.daily_limit}")
                        return data
            except: pass
        return default_data

    async def _save_usage(self):
        if self.is_private: return
        with open(QUOTA_FILE, 'w') as f:
            json.dump(self.usage_data, f)

    async def can_use_large(self):
        if self.is_private: return True
        async with self.lock:
            if self.usage_data["count"] < self.daily_limit:
                self.usage_data["count"] += 1
                if self.usage_data["count"] % 10 == 0: await self._save_usage()
                return True
            return False

    async def refund_large(self):
        if self.is_private: return
        async with self.lock:
            if self.usage_data["count"] > 0:
                self.usage_data["count"] -= 1
                await self._save_usage()

# ==============================================================================
# 3. UTILS (ADAPTIVE LOGIC)
# ==============================================================================
def get_adaptive_top_k(question):
    """ƒêi·ªÅu ch·ªânh s·ªë l∆∞·ª£ng chunk d·ª±a tr√™n ƒë·ªô d√†i c√¢u h·ªèi"""
    q_len = len(question.split()) # ƒê·∫øm s·ªë t·ª´
    if q_len < 10: return 8   # C√¢u ng·∫Øn -> L·∫•y √≠t cho ƒë·ª° nhi·ªÖu
    if q_len < 30: return 10  # Trung b√¨nh
    return 12                 # C√¢u d√†i/ph·ª©c t·∫°p -> L·∫•y nhi·ªÅu context h∆°n

def get_dynamic_options(row):
    options = []
    # [FIX] ∆Øu ti√™n check key 'choices' theo format json th·ª±c t·∫ø c·ªßa BTC
    if 'choices' in row and isinstance(row['choices'], list):
        options = row['choices']
    # H·ªó tr·ª£ th√™m key 'options' (d·ª± ph√≤ng)
    elif 'options' in row and isinstance(row['options'], list):
        options = row['options']
    # Fallback cho CSV (option_1, option_2...)
    else:
        i = 1
        while True:
            val = row.get(f"option_{i}")
            if val is None: break
            val_str = str(val).strip()
            if not val_str or val_str.lower() == 'nan': break
            options.append(val_str)
            i += 1
    
    mapped = {}
    for idx, text in enumerate(options):
        mapped[chr(65 + idx)] = str(text) # A: text, B: text
    return mapped

async def verify_safety_with_model(session, question):
    """
    D√πng Model Small ƒë·ªÉ ki·ªÉm tra k·ªπ l·∫°i c√°c c√¢u h·ªèi d√≠nh keyword.
    Tr·∫£ v·ªÅ: True (An to√†n/Ki·∫øn th·ª©c) ho·∫∑c False (ƒê·ªôc h·∫°i/C·∫•m k·ªµ)
    """
    system_prompt = """B·∫°n l√† h·ªá th·ªëng ki·ªÉm duy·ªát n·ªôi dung an to√†n (Content Safety Moderator).
Nhi·ªám v·ª•: Ph√¢n lo·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng l√† AN TO√ÄN (Safe) hay ƒê·ªòC H·∫†I (Unsafe).

ƒê·ªäNH NGHƒ®A:
1. ƒê·ªòC H·∫†I (Unsafe):
   - Y√™u c·∫ßu h∆∞·ªõng d·∫´n th·ª±c hi·ªán h√†nh vi ph·∫°m ph√°p (ch·∫ø t·∫°o v≈© kh√≠, ma t√∫y, gi·∫øt ng∆∞·ªùi).
   - Ng√¥n t·ª´ th√π gh√©t, x√∫c ph·∫°m danh nh√¢n/l√£nh t·ª•, ch·ªëng ph√° nh√† n∆∞·ªõc.
   - N·ªôi dung khi√™u d√¢m, ƒë·ªìi tr·ª•y.

2. AN TO√ÄN (Safe):
   - C√¢u h·ªèi t√¨m ki·∫øm ki·∫øn th·ª©c, h·ªçc thu·∫≠t (L·ªãch s·ª≠, Ph√°p lu·∫≠t, Y h·ªçc).
   - V√≠ d·ª•: "Lu·∫≠t h√¨nh s·ª± ph·∫°t t·ªôi gi·∫øt ng∆∞·ªùi th·∫ø n√†o?" -> AN TO√ÄN (H·ªèi v·ªÅ lu·∫≠t).
   - V√≠ d·ª•: "V≈© kh√≠ qu√¢n ƒë·ªôi Vi·ªát Nam nƒÉm 1975?" -> AN TO√ÄN (H·ªèi v·ªÅ l·ªãch s·ª≠).

OUTPUT B·∫ÆT BU·ªòC: Ch·ªâ tr·∫£ l·ªùi m·ªôt t·ª´ duy nh·∫•t: SAFE ho·∫∑c UNSAFE."""

    user_prompt = f"C√ÇU H·ªéI: {question}\nPH√ÇN LO·∫†I:"
    
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
    
    # G·ªçi Model Small (Max speed)
    resp = await call_llm_generic(session, messages, Config.LLM_MODEL_SMALL, max_tokens=10)
    
    if resp:
        # N·∫øu model tr·∫£ l·ªùi SAFE -> An to√†n
        if "SAFE" in resp.upper() and "UN" not in resp.upper():
            return True
        # N·∫øu model tr·∫£ l·ªùi UNSAFE -> ƒê·ªôc h·∫°i
        if "UNSAFE" in resp.upper():
            return False
    
    # Fallback: N·∫øu model l·ªói, m·∫∑c ƒë·ªãnh l√† False (Ch·∫∑n nh·∫ßm h∆°n b·ªè s√≥t cho an to√†n)
    return False

def is_sensitive_topic(question):
    # Danh s√°ch t·ª´ kh√≥a nghi v·∫•n (Suspicious Keywords)
    blacklist = [
        "sex", "khi√™u d√¢m", "ƒë·ªìi tr·ª•y", "l√†m t√¨nh", "·∫•u d√¢m", "k√≠ch d·ª•c",
        "b·∫°o ƒë·ªông", "l·∫≠t ƒë·ªï", "ph·∫£n ƒë·ªông", "kh·ªßng b·ªë", "bi·ªÉu t√¨nh", "ch·ªëng ph√°",
        "gi·∫øt ng∆∞·ªùi", "t·ª± t·ª≠", "ma t√∫y", "bu√¥n l·∫≠u", "v≈© kh√≠", "b·∫°o l·ª±c", "bom m√¨n", "thu·ªëc n·ªï",
        "x√∫c ph·∫°m", "lƒÉng m·∫°", "ch√≠nh quy·ªÅn", "ƒë·∫£ng c·ªông s·∫£n", "xuy√™n t·∫°c",
        "c·ªù b·∫°c", "c√° ƒë·ªô", "m·∫°i d√¢m", "ƒë√°nh b·∫°c"
    ]
    q_lower = question.lower()
    return any(w in q_lower for w in blacklist)

def find_refusal_key(options_map):
    keywords = ["kh√¥ng th·ªÉ tr·∫£ l·ªùi", "t·ª´ ch·ªëi", "vi ph·∫°m", "nh·∫°y c·∫£m", "kh√¥ng ph√π h·ª£p", "t√°c ƒë·ªông ti√™u c·ª±c"]
    for label, text in options_map.items():
        if any(kw in str(text).lower() for kw in keywords):
            return label
    return None

def extract_answer_two_step(text, options_map):
    """Parser th√¥ng minh h·ªó tr·ª£ format '### ƒê√ÅP √ÅN:'"""
    valid_keys = list(options_map.keys())
    fallback = valid_keys[0]
    
    if not text: return fallback
    text = text.strip()

    # Priority 1: T√¨m theo format Prompt b·∫Øt bu·ªôc
    # B·∫Øt: "### ƒê√ÅP √ÅN: A" ho·∫∑c "### ƒê√ÅP √ÅN:\n[A]"
    match_strict = re.search(r'###\s*ƒê√ÅP √ÅN[:\s\n]*([A-Z])', text, re.IGNORECASE)
    if match_strict:
        key = match_strict.group(1).upper()
        if key in valid_keys: return key

    # Priority 2: Regex Markdown/Common
    patterns = [
        r'(?:ƒë√°p √°n|ch·ªçn|l√†)[:\s\*\-\.\[\(]*([A-Z])[\]\)\*\.]*$',
        r'\*\*([A-Z])\*\*',
        r'^([A-Z])[\.\)]\s'
    ]
    for pat in patterns:
        match = re.search(pat, text, re.IGNORECASE | re.MULTILINE)
        if match:
            key = match.group(1).upper()
            if key in valid_keys: return key

    # Priority 3: Fuzzy Matching (So s√°nh text)
    text_lower = text.lower()
    best_match = None
    max_len = 0
    for key, opt_text in options_map.items():
        opt_lower = opt_text.lower()
        # N·∫øu model rep nguy√™n c√¢u ƒë√°p √°n
        if opt_lower in text_lower:
            if len(opt_lower) > max_len:
                max_len = len(opt_lower)
                best_match = key
    if best_match: return best_match

    # Priority 4: T√¨m k√Ω t·ª± cu·ªëi c√πng
    matches = re.findall(r'\b([A-Z])\b', text)
    if matches:
        for cand in reversed(matches):
            if cand.upper() in valid_keys: return cand.upper()

    return fallback

# ==============================================================================
# 4. API CLIENT
# ==============================================================================
async def call_llm_generic(session, messages, model_name, max_tokens=1024, retry=3):
    creds = Config.VNPT_CREDENTIALS.get(model_name)
    url = f"{Config.VNPT_API_URL}/{model_name.replace('_', '-')}"
    headers = {'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}', 'Token-id': creds['token_id'], 'Token-key': creds['token_key'], 'Content-Type': 'application/json'}
    payload = {"model": model_name, "messages": messages, "temperature": 0.1, "top_p": 0.95, "max_completion_tokens": max_tokens}

    for attempt in range(retry):
        try:
            async with LIMITER_LLM:
                async with session.post(url, json=payload, headers=headers, timeout=50) as resp:
                    if resp.status == 200:
                        try:
                            d = await resp.json()
                            if 'choices' in d and d['choices']: return d['choices'][0]['message']['content']
                        except Exception as e:
                            logger.error(f"JSON Error {model_name}: {e}")
                    
                    # Retry logic
                    txt = await resp.text()
                    if resp.status in [429, 500, 502, 503] or (resp.status == 401 and "Rate limit" in txt):
                        await asyncio.sleep(2 * (attempt + 1))
                        continue
                    else:
                        logger.error(f"API Error {model_name} {resp.status}: {txt}")
                        return None
        except Exception as e:
            logger.warning(f"Net Error {model_name}: {str(e)[:50]}")
            await asyncio.sleep(1)
    return None

async def get_embedding_async(session, text):
    model = Config.MODEL_EMBEDDING_API
    creds = Config.VNPT_CREDENTIALS.get(model)
    headers = {'Authorization': f'Bearer {Config.VNPT_ACCESS_TOKEN}', 'Token-id': creds['token_id'], 'Token-key': creds['token_key'], 'Content-Type': 'application/json'}
    payload = {"model": model, "input": text, "encoding_format": "float"}
    for _ in range(4):
        try:
            async with LIMITER_EMBED:
                async with session.post(Config.VNPT_EMBEDDING_URL, json=payload, headers=headers, timeout=30) as r:
                    if r.status == 200: 
                        d = await r.json()
                        if 'data' in d: return d['data'][0]['embedding']
                    elif r.status in [429, 500, 401]: await asyncio.sleep(2)
        except: await asyncio.sleep(1)
    return None

# ==============================================================================
# 5. ROBUST HYBRID RETRIEVER
# ==============================================================================
class HybridRetriever:
    def __init__(self):
        self.client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)
        self.bm25 = None
        if BM25_FILE.exists():
            with open(BM25_FILE, "rb") as f: self.bm25 = pickle.load(f)

    async def search_qdrant_retry(self, query_vec, top_k, max_retries=3):
        """Qdrant v·ªõi c∆° ch·∫ø Retry"""
        for i in range(max_retries):
            try:
                return self.client.search(collection_name=Config.COLLECTION_NAME, query_vector=query_vec, limit=top_k, with_payload=True)
            except Exception as e:
                if i == max_retries - 1:
                    logger.error(f"Qdrant Fail: {e}")
                    return []
                await asyncio.sleep(0.5)
        return []

    async def search(self, session, query, top_k=10):
        # 1. Vector Search
        query_vec = await get_embedding_async(session, query)
        vec_hits = []
        if query_vec:
            vec_hits = await self.search_qdrant_retry(query_vec, top_k)
        
        # 2. BM25 Search (Dynamic Threshold)
        bm25_hits = []
        if self.bm25:
            tokens = word_tokenize(query.lower())
            scores = self.bm25['bm25_obj'].get_scores(tokens)
            
            # Ch·ªâ l·∫•y Top 2*k ƒë·ªÉ l·ªçc
            top_idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k*2]
            
            # Dynamic Threshold (Top 30% c·ªßa batch n√†y ho·∫∑c min 1.0)
            batch_scores = [scores[i] for i in top_idxs]
            if batch_scores:
                # L·∫•y gi√° tr·ªã ·ªü ph√¢n v·ªã 70% (Top 30%)
                dynamic_thresh = batch_scores[int(len(batch_scores) * 0.3)]
                threshold = max(1.0, dynamic_thresh) # Kh√¥ng th·∫•p h∆°n 1.0
            else:
                threshold = 1.0

            for idx in top_idxs:
                if scores[idx] >= threshold: 
                    bm25_hits.append({"id": self.bm25['chunk_ids'][idx], "score": scores[idx], "text": self.bm25['texts'][idx], "title": self.bm25['titles'][idx]})

        # 3. Fusion
        fused = {}
        max_v = max([h.score for h in vec_hits]) if vec_hits else 1.0
        for h in vec_hits:
            fused[h.payload['chunk_id']] = {"text": h.payload['text'], "title": h.payload['title'], "score": (h.score/max_v)*ALPHA_VECTOR}
        
        max_b = max([h['score'] for h in bm25_hits]) if bm25_hits else 1.0
        for h in bm25_hits:
            norm = (h['score']/max_b)*(1-ALPHA_VECTOR)
            cid = h['id']
            if cid in fused: fused[cid]['score'] += norm
            else: fused[cid] = {"text": h['text'], "title": h['title'], "score": norm}
            
        return sorted(fused.values(), key=lambda x: x['score'], reverse=True)[:top_k]

# ==============================================================================
# 6. PIPELINE CH√çNH (STRUCTURED PROMPT)
# ==============================================================================
def build_prompt(question, options_text, valid_keys_str, docs):
    context_str = ""
    for i, doc in enumerate(docs):
        context_str += f"--- T√ÄI LI·ªÜU #{i+1} ({doc['title']}) ---\n{doc['text']}\n\n"

    # System Prompt: ƒê√≥ng vai chuy√™n gia ƒëa nƒÉng
    system_prompt = """B·∫°n l√† chuy√™n gia gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ ƒëa lƒ©nh v·ª±c (STEM & X√£ h·ªôi).
NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi tr·∫Øc nghi·ªám ch√≠nh x√°c nh·∫•t.

H∆Ø·ªöNG D·∫™N SUY LU·∫¨N (CHAIN-OF-THOUGHT):
1. **X√°c ƒë·ªãnh lo·∫°i c√¢u h·ªèi:**
   - N·∫øu l√† L·ªãch s·ª≠/ƒê·ªãa l√Ω/Lu·∫≠t/VƒÉn h√≥a: T√¨m keyword trong t√†i li·ªáu, so kh·ªõp th·ªùi gian/ƒë·ªãa ƒëi·ªÉm. ∆Øu ti√™n th√¥ng tin 2024-2025.
   - N·∫øu l√† To√°n/L√Ω/H√≥a/Sinh/Tin: X√°c ƒë·ªãnh c√¥ng th·ª©c, ƒë·ªãnh l√Ω, bi·∫øn s·ªë v√† th·ª±c hi·ªán T√çNH TO√ÅN t·ª´ng b∆∞·ªõc.

2. **Quy t·∫Øc x·ª≠ l√Ω:**
   - ∆Øu ti√™n th√¥ng tin trong D·ªÆ LI·ªÜU THAM KH·∫¢O.
   - N·∫øu d·ªØ li·ªáu kh√¥ng ƒë·ªß ƒë·ªÉ t√≠nh to√°n/suy lu·∫≠n (ƒë·∫∑c bi·ªát l√† c√¢u h·ªèi STEM): H√£y d√πng KI·∫æN TH·ª®C KHOA H·ªåC CHU·∫®N X√ÅC c·ªßa b·∫°n ƒë·ªÉ gi·∫£i quy·∫øt.
   - Ki·ªÉm tra k·ªπ c√°c "b·∫´y" (ƒë∆°n v·ªã ƒëo, ph·ªß ƒë·ªãnh "kh√¥ng", ngo·∫°i l·ªá).
   - N·∫øu c√¢u h·ªèi vi ph·∫°m ti√™u chu·∫©n an to√†n/ƒë·∫°o ƒë·ª©c -> Ch·ªçn ƒë√°p √°n T·ª™ CH·ªêI.

ƒê·ªäNH D·∫†NG OUTPUT B·∫ÆT BU·ªòC:
### SUY LU·∫¨N:
[Vi·∫øt ng·∫Øn g·ªçn 2-3 d√≤ng ph√¢n t√≠ch, ph√©p t√≠nh ho·∫∑c d·∫´n ch·ª©ng]

### ƒê√ÅP √ÅN:
[Ch·ªâ vi·∫øt 1 k√Ω t·ª± ƒë·∫°i di·ªán ƒë√°p √°n ƒë√∫ng: A, B, C, D...]"""

    # User Prompt: √âp ƒë·ªãnh d·∫°ng tr·∫£ l·ªùi
    user_prompt = f"""D·ªÆ LI·ªÜU THAM KH·∫¢O:\n{context_str}\n\nC√ÇU H·ªéI: {question}\nL·ª∞A CH·ªåN:\n{options_text}\n\nH√ÉY SUY LU·∫¨N LOGIC V√Ä T√çNH TO√ÅN (N·∫æU C·∫¶N) ƒê·ªÇ CH·ªåN ƒê√ÅP √ÅN ƒê√öNG TRONG ({valid_keys_str}):"""
    
    return [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

async def select_model_strategy(context_chars, quota_mgr, qid):
    """
    Logic ch·ªçn model t∆∞·ªùng minh:
    1. ∆Øu ti√™n Large.
    2. B·ªã gi√°ng xu·ªëng Small n·∫øu Context qu√° d√†i.
    3. B·ªã gi√°ng xu·ªëng Small n·∫øu H·∫øt Quota.
    """
    # 1. Check k·ªπ thu·∫≠t: Context Window
    if context_chars > THRESHOLD_LARGE_CHARS:
        logger.warning(f"‚ö†Ô∏è Q:{qid} Context Too Long ({context_chars}). Forced SMALL.")
        return Config.LLM_MODEL_SMALL

    # 2. Check t√†i nguy√™n: Quota Manager
    # L∆∞u √Ω: H√†m can_use_large() s·∫Ω T·ª∞ ƒê·ªòNG tr·ª´ quota n·∫øu tr·∫£ v·ªÅ True
    if await quota_mgr.can_use_large():
        return Config.LLM_MODEL_LARGE
    
    # 3. Fallback: H·∫øt quota ho·∫∑c kh√¥ng ƒë∆∞·ª£c d√πng
    # logger.info(f"‚ÑπÔ∏è Q:{qid} Quota limit reached. Using SMALL.") # Uncomment n·∫øu mu·ªën log chi ti·∫øt
    return Config.LLM_MODEL_SMALL

# ==============================================================================
# C·∫¨P NH·∫¨T H√ÄM PROCESS_ROW_SAFE
# ==============================================================================
async def process_row_safe(sem, session, retriever, quota_mgr, row):
    try:
        async with sem:
            # L·∫•y QID v√† C√¢u h·ªèi
            qid = row.get('qid', row.get('id', 'unknown'))
            question = row.get('question', '')
            
            # [VAL MODE] L·∫•y ƒë√°p √°n ƒë√∫ng n·∫øu c√≥ (ƒë·ªÉ ch·∫•m ƒëi·ªÉm t·∫≠p Val)
            true_label = row.get('answer', None)

            # 1. Options & Mapping
            options_map = get_dynamic_options(row)
            options_text = "\n".join([f"{k}. {v}" for k, v in options_map.items()])
            valid_keys = list(options_map.keys())
            valid_keys_str = ", ".join(valid_keys)

            # 2. Safety Check (Offline)
            if is_sensitive_topic(question):
                logger.info(f"‚ö†Ô∏è Q:{qid} d√≠nh Keyword nh·∫°y c·∫£m. ƒêang th·∫©m ƒë·ªãnh l·∫°i b·∫±ng AI...")
                
                # B∆∞·ªõc 2: Th·∫©m ƒë·ªãnh b·∫±ng Model Small
                is_safe_context = await verify_safety_with_model(session, question)
                
                if not is_safe_context:
                    logger.info(f"üö´ Q:{qid} -> X√ÅC NH·∫¨N ƒê·ªòC H·∫†I. T·ª´ ch·ªëi tr·∫£ l·ªùi.")
                    refusal_key = find_refusal_key(options_map)
                    return {"qid": qid, "answer": refusal_key if refusal_key else "A"}
                else:
                    logger.info(f"‚úÖ Q:{qid} -> False Positive (H·ªèi ki·∫øn th·ª©c/Lu·∫≠t). Ti·∫øp t·ª•c x·ª≠ l√Ω.")

            # 3. Retrieval
            adaptive_k = get_adaptive_top_k(question)
            docs = await retriever.search(session, question, top_k=adaptive_k)
            
            # 4. Prompt & Model Selection (D√πng h√†m m·ªõi)
            messages = build_prompt(question, options_text, valid_keys_str, docs)
            context_chars = sum([len(d['text']) for d in docs])
            
            # [FIX LOGIC] G·ªçi h√†m ch·ªçn model t∆∞·ªùng minh
            model_to_use = await select_model_strategy(context_chars, quota_mgr, qid)

            # 5. Inference
            raw_ans = await call_llm_generic(session, messages, model_to_use)
            
            # Fallback: Large Fail -> Small
            if raw_ans is None and model_to_use == Config.LLM_MODEL_LARGE:
                logger.warning(f"üîÑ Q:{qid} Large Fail -> Retry Small.")
                await quota_mgr.refund_large() # Ho√†n quota
                raw_ans = await call_llm_generic(session, messages, Config.LLM_MODEL_SMALL)

            # 6. Extract Answer
            final_key = extract_answer_two_step(raw_ans, options_map)
            
            # [LOGGING] In k·∫øt qu·∫£ k√®m ch·∫•m ƒëi·ªÉm (n·∫øu c√≥ label)
            log_suffix = ""
            is_correct = None
            if true_label:
                is_correct = (final_key == true_label)
                icon = "‚úÖ" if is_correct else "‚ùå"
                log_suffix = f"| True: {true_label} {icon}"
            
            logger.info(f"Q:{qid} | Mod:{'L' if model_to_use==Config.LLM_MODEL_LARGE else 'S'} | Ans:{final_key} {log_suffix}")
            
            return {"qid": qid, "answer": final_key, "is_correct": is_correct}

    except asyncio.TimeoutError:
        logger.error(f"‚è∞ Timeout Q:{qid}")
        return {"qid": qid, "answer": "A", "is_correct": False if true_label else None}
    except Exception as e:
        logger.error(f"‚ùå Crash Q:{qid}: {e}")
        return {"qid": qid, "answer": "A", "is_correct": False if true_label else None}

async def process_row_with_timeout(sem, session, retriever, quota_mgr, row):
    # Wrapper ƒë·ªÉ b·∫Øt timeout
    return await asyncio.wait_for(
        process_row_safe(sem, session, retriever, quota_mgr, row),
        timeout=TIMEOUT_PER_QUESTION
    )

async def main():
    # Detect Input File
    # ∆Øu ti√™n theo th·ª© t·ª±: private -> public -> val -> test
    files_to_check = [
        Config.BASE_DIR / "data" / "private_test.json", 
        Config.BASE_DIR / "data" / "val.json",          # File b·∫°n mu·ªën ch·∫°y
        Config.BASE_DIR / "data" / "test.json",
        Config.BASE_DIR / "data" / "public_test.json"   
    ]
    
    input_file = None
    is_private = False
    
    for f in files_to_check:
        if f.exists():
            input_file = f
            # N·∫øu t√™n file l√† val.json ho·∫∑c private -> Ch·∫ø ƒë·ªô Private (Unlimited Quota)
            if "private" in f.name or f.name == "val.json":
                is_private = True
            break
    
    if not input_file:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file input data/")
        return

    logger.info(f"üöÄ STARTING | File: {input_file.name} | Mode: {'PRIVATE/VAL (Unlimited)' if is_private else 'PUBLIC (Quota)'}")

    # Load Data
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # Init
    quota_mgr = QuotaManager(is_private)
    conn = aiohttp.TCPConnector(limit=MAX_CONCURRENT_TASKS + 5, force_close=True)
    sem = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
    retriever = HybridRetriever()

    async with aiohttp.ClientSession(connector=conn) as session:
        tasks = [process_row_with_timeout(sem, session, retriever, quota_mgr, row) for row in data]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    # Clean results & Calculate Score
    clean_results = []
    correct_count = 0
    has_label = False

    for i, res in enumerate(results):
        if isinstance(res, dict):
            clean_results.append(res)
            if res.get('is_correct') is not None:
                has_label = True
                if res['is_correct']: correct_count += 1
        else:
            # Handle Exception
            qid = data[i].get('qid', 'unknown')
            logger.error(f"üî• Critical Failure Q:{qid}")
            clean_results.append({"qid": qid, "answer": "A"})

    # Print Validation Score
    if has_label and len(clean_results) > 0:
        acc = (correct_count / len(clean_results)) * 100
        logger.info("="*40)
        logger.info(f"üìä VALIDATION SCORE: {correct_count}/{len(clean_results)} ({acc:.2f}%)")
        logger.info("="*40)

    # Save output
    out_df = pd.DataFrame(clean_results)
    # Ch·ªâ gi·ªØ c·ªôt c·∫ßn thi·∫øt cho file n·ªôp
    final_df = out_df[['qid', 'answer']]
    
    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    final_df.to_csv(Config.BASE_DIR / "output" / "submission.csv", index=False)
    logger.info(f"üéâ DONE! Output saved to output/submission.csv")

if __name__ == "__main__":
    if sys.platform == 'win32': asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())